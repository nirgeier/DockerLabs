{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DockerLabs Hands-on","text":"<ul> <li>Welcome to the lab workspace that accompanies the DockerLabs project. </li> <li>Each folder below is a self-contained lab that you can complete independently to sharpen specific containerization skills. </li> <li>Follow the README file in each lab for detailed steps.</li> </ul>"},{"location":"#how-to-use-those-labs","title":"How to Use Those Labs","text":"<ul> <li>There are several ways to run the Docker Labs. </li> <li>Choose the method that works best for you.<ul> <li> Killercoda  (Recommended)</li> <li>\ud83d\udcdc From Source</li> <li> Using Google Cloud Shell</li> </ul> </li> </ul>  Killercoda  (Recommended)\ud83d\udcdc From Source Using Google Cloud Shell <ul> <li>The easiest way to get started with the labs</li> <li>Learn Docker in your browser without any local installation</li> </ul> <p>\ud83c\udf10 Launch on Killercoda</p> <p>Benefits:</p> <ul> <li>No installation required</li> <li>Pre-configured environment</li> <li>Works on any device with a web browser</li> <li>All tools pre-installed</li> </ul> <p>For those who prefer to run it directly on their machine:</p> <p><pre><code># Clone the repository\ngit clone https://github.com/nirgeier/DockerLabs.git\n# Change to the Labs directory\ncd DockerLabs/Labs\n# Start with the Docker CLI lab\ncd 001-DockerCli\n# Follow the instructions in the README of each lab\ncat README.md\n</code></pre> Prerequisites:</p> <ul> <li>Ansible installed on your system</li> <li>A Unix-like operating system (Linux, macOS, or Windows with WSL)</li> <li>Basic command-line tools</li> </ul> <ul> <li>Google Cloud Shell provides a free, browser-based environment with all necessary tools pre-installed.</li> <li>Click on the <code>Open in Google Cloud Shell</code> button below:</li> </ul> <p></p> <ul> <li>The repository will automatically be cloned into a free Cloud instance.</li> <li>Use CTRL + click to open it in a new window.</li> <li>Follow the instructions in the README of each lab.</li> </ul> <p>Benefits:</p> <ul> <li>No local installation required</li> <li>Pre-configured environment</li> <li>Works on any device with a web browser</li> <li>All tools pre-installed</li> <li>Free tier available</li> </ul>"},{"location":"#lab-index","title":"Lab Index","text":""},{"location":"#001-docker-cli","title":"001 - Docker CLI","text":"<p>Practice the core Docker CLI commands for running, inspecting, and managing containers.</p> <p> Get started</p>"},{"location":"#002-dockerfile-basics","title":"002 - Dockerfile Basics","text":"<p>Build your first Node.js container image from a Dockerfile and publish it to a registry.</p> <p> Get started</p>"},{"location":"#003-dockerfile-multi-stage","title":"003 - Dockerfile Multi-Stage","text":"<p>Learn how multi-stage Dockerfiles produce lean images across build targets.</p> <p> Get started</p>"},{"location":"#004-local-registry","title":"004 - Local Registry","text":"<p>Stand up a private registry, retag images, and push or pull them locally.</p> <p> Get started</p>"},{"location":"#005-docker-compose-stack","title":"005 - Docker Compose Stack","text":"<p>Orchestrate a WordPress and MariaDB stack with Docker Compose.</p> <p> Get started</p>"},{"location":"#006-compose-environments","title":"006 - Compose Environments","text":"<p>Structure Compose files and env vars for dev and prod workflows.</p> <p> Get started</p>"},{"location":"#007-docker-compose-fragments","title":"007 - Docker Compose Fragments","text":"<p>Learn advanced Docker Compose features with fragments and modular configurations.</p> <p> Get started</p>"},{"location":"#008-cri-crictl","title":"008 - CRI <code>crictl</code>","text":"<p>Learn about container runtime interface tooling using crictl.</p> <p> Get started</p>"},{"location":"#009-dive-layers","title":"009 - Dive Layers","text":"<p>Explore image layer creation and visualize them with the dive tool.</p> <p> Get started</p>"},{"location":"#010-docker-bake","title":"010 - Docker Bake","text":"<p>Use Docker Buildx Bake to coordinate complex, multi-target image builds.</p> <p> Get started</p>"},{"location":"#011-security-trust","title":"011 - Security &amp; Trust","text":"<p>Learn advanced Docker security features and best practices for container security.</p> <p> Get started</p>"},{"location":"#012-gvisor-seccomp","title":"012 - gVisor Seccomp","text":"<p>Apply a gVisor runtime profile to block privileged syscalls inside a container.</p> <p> Get started</p>"},{"location":"#013-onictl","title":"013 - onictl","text":"<p>Learn about container networking with onictl.</p> <p> Get started</p>"},{"location":"#100-hands-on-intro","title":"100 - Hands-On Intro","text":"<p>Guided Node.js exercise covering the full build, run, and publish workflow.</p> <p> Get started</p>"},{"location":"#tasks","title":"Tasks","text":"Task Description DockerCommit In-class exercise for capturing container changes with <code>docker commit</code>. DockerDebug Debugging challenge: troubleshoot a crashing Flask container and fix missing configurations. DockerfileAdvanced Advanced Dockerfile exercise covering BuildKit secrets, caching, and health checks. DockerLogs In-class exercise for running cowsay container, managing logs, and debugging. MultiStage In-class exercise for creating multi-stage Dockerfiles with alpine and node images. <p>Happy learning and hacking with Docker!</p>"},{"location":"001-DockerCli/","title":"001-DockerCli","text":""},{"location":"001-DockerCli/#lab-001-docker-cli","title":"Lab 001 - Docker CLI","text":"<ul> <li>This lab covers the basics of the Docker CLI.</li> <li>You will learn how to run, manage, and interact with Docker containers using various Docker commands.</li> <li>By the end of this lab, you will have a solid understanding of how to use the Docker CLI for container management.</li> </ul>"},{"location":"001-DockerCli/#docker-cli-commands","title":"Docker CLI Commands","text":"<ul> <li><code>docker attach</code></li> <li><code>docker build</code></li> <li><code>docker commit</code></li> <li><code>docker cp</code></li> <li><code>docker create</code></li> <li><code>docker exec</code></li> <li><code>docker images</code></li> <li><code>docker inspect</code></li> <li><code>docker kill</code></li> <li><code>docker logs</code></li> <li><code>docker pause</code></li> <li><code>docker ps</code></li> <li><code>docker pull</code></li> <li><code>docker push</code></li> <li><code>docker rename</code></li> <li><code>docker restart</code></li> <li><code>docker rm</code></li> <li><code>docker rmi</code></li> <li><code>docker run</code></li> <li><code>docker run -a</code></li> <li><code>docker run -d</code></li> <li><code>docker run -it</code></li> <li><code>docker run -name</code></li> <li><code>docker run -p</code></li> <li><code>docker run &lt;command&gt;</code></li> <li><code>docker start</code></li> <li><code>docker stats</code></li> <li><code>docker stop</code></li> <li><code>docker top</code></li> <li><code>docker unpause</code></li> <li><code>docker wait</code></li> </ul>"},{"location":"001-DockerCli/#docker-attach","title":"<code>docker attach</code>","text":"<ul> <li><code>docker attach</code> is used to attach your terminal to a running container.</li> <li>This is useful when you want to interact with a container that is already running.</li> <li>For example, if you have a container running a shell or an application that accepts input, you can use <code>docker attach</code> to connect to it.</li> </ul> <pre><code># Spin an alpine image and start it in the background\ndocker run -it -d --name alpine001 alpine sleep 10000\n\n# Attach to the container and start a shell inside it\ndocker attach alpine001\n</code></pre> <p>Detaching from a Container</p> <ul> <li>To detach from the container without stopping it, you can use the CTRL + P followed by CTRL + Q key combination.    </li> <li>This will leave the container running in the background while you return to your terminal.  <ul> <li>This detach sequence only works if the container was started with the <code>-it</code> flags (interactive with a TTY)</li> </ul> </li> </ul>"},{"location":"001-DockerCli/#docker-build","title":"<code>docker build</code>","text":"<ul> <li><code>docker build</code> creates a Docker image from a Dockerfile.</li> <li> <p>This is one of the most important commands for creating custom images.</p> <pre><code># Create a simple Dockerfile\nmkdir -p /tmp/docker-build-example\ncd /tmp/docker-build-example\n\ncat &lt;&lt;'EOF' &gt; Dockerfile\nFROM alpine:latest\nRUN apk add --no-cache curl\nCMD [\"echo\", \"Hello from custom image\"]\nEOF\n\n# Build the image with a tag\ndocker build -t my-custom-alpine:v1.0 .\n\n# Build with a different tag\ndocker build -t my-custom-alpine:latest .\n\n# Build without using cache\ndocker build --no-cache -t my-custom-alpine:v1.0 .\n\n# Test the built image\ndocker run --rm my-custom-alpine:v1.0\n\n# Clean up\ncd -\nrm -rf /tmp/docker-build-example\n</code></pre> </li> </ul>"},{"location":"001-DockerCli/#docker-commit","title":"<code>docker commit</code>","text":"<ul> <li> <p><code>docker commit</code> will create a new images out of an existing container.</p> <pre><code># Clean up and remove the container\ndocker stop nginx\ndocker rm   nginx\n\n# Spin the container\ndocker  run  -it -d -p 8888:80 --name nginx nginx\n\n# Wait for the container to start\nsleep 5\n\n# Prepare the desired welcome page\ndocker  exec -it nginx sh -c \"                  \\\n        echo 'This is a custom message ... ' &gt;  \\\n        /usr/share/nginx/html/index.html\"\n\n# Verify the changes\ncurl -s localhost:8888\n\n# Create the custom image\ndocker commit nginx nirgeier/custom-nginx-image\n\n# Clean up and remove the container\ndocker stop nginx\ndocker rm   nginx\n\n# Push to the registry\ndocker push nirgeier/custom-nginx-image\n\n# Clean up and remove the container\ndocker stop custom-nginx\ndocker rm   custom-nginx\n\n# Push to the registry\ndocker  run -it -d --name custom-nginx  \\\n        -p 8888:80                      \\\n        nirgeier/custom-nginx-image\n\n# Wait for the container to start\nsleep 5\n\n# Verify the changes\ncurl -s localhost:8888\n\n# Clean up and remove the container\ndocker stop custom-nginx\ndocker rm   custom-nginx\n</code></pre> </li> </ul>"},{"location":"001-DockerCli/#docker-cp","title":"<code>docker cp</code>","text":"<ul> <li><code>docker cp</code> is used to copy files between the container and the host</li> <li>Lets spin a container and then lets grab the logs of this container to our host</li> </ul> <p>Copying files from and to a Container</p> <p>Since container are \u201cfile system\u201d we can grab files even when the container is stopped.</p> <ul> <li> <p>Example 1 - Copy file from Container to Host</p> <pre><code># Spin a container\ndocker run -it -d --name nginx -p 8888:80 nginx\n\n# grab the nginx default configuration \ndocker cp nginx:/etc/nginx/nginx.conf nginx.conf \n\n# Verify that the file exists locally\ncat nginx.conf \n</code></pre> </li> <li> <p>Example 2 - Copy file from Host to Container</p> <ul> <li>In the second example we will upload file to our container</li> <li>We will change the default nginx welcome page with our own page</li> </ul> <pre><code># Prepare the desired welcome page\necho 'Welcome to the world of Docker' &gt; index.html\n\n# Copy the file to the container \ndocker cp index.html nginx:/usr/share/nginx/html\n\n# Test the changes to the container\ncurl -s localhost:8888\n\n# Clean up and remove the container\ndocker stop nginx\ndocker rm   nginx\n</code></pre> </li> </ul>"},{"location":"001-DockerCli/#docker-create","title":"<code>docker create</code>","text":"<ul> <li><code>docker create</code> creates a new container but does not start it.</li> <li>This is useful when you want to prepare a container and start it later.     <pre><code># Create a container without starting it\ndocker create --name my-nginx nginx\n\n# Verify the container is created but not running\ndocker ps -a | grep my-nginx\n\n# Create a container with port mapping\ndocker create --name web-server -p 8080:80 nginx\n\n# Create with environment variables\ndocker create --name db-container -e POSTGRES_PASSWORD=secret postgres\n\n# Create with volume mount\ndocker create --name data-container -v /data alpine\n\n# Start the created container\ndocker start my-nginx\n\n# Clean up\ndocker stop my-nginx\ndocker rm my-nginx web-server db-container data-container\n</code></pre></li> </ul>"},{"location":"001-DockerCli/#docker-exec","title":"<code>docker exec</code>","text":"<ul> <li>Execute a command in a running container     <pre><code># Remove old containers with the same name\ndocker stop alpine001\ndocker rm   alpine001\n\n# Spin an alpine image\ndocker run -it -d --name alpine001 alpine sleep 10000\n\n# Test that curl is not installed on the container\ndocker exec -it alpine001 curl\n\n# Install a new package on the container\ndocker exec -it alpine001 apk add curl\n\n# Test that curl is installed\ndocker exec -it alpine001 curl codewizard.co.il\n\n# Interact with the alpine image and open bash shell\ndocker exec -it alpine001 sh\n</code></pre></li> </ul>"},{"location":"001-DockerCli/#docker-images","title":"<code>docker images</code>","text":"<ul> <li><code>docker images</code> lists all Docker images on your system.</li> <li> <p>This command helps you see what images you have available locally.</p> <pre><code># List all images\ndocker images\n\n# List images with specific format\ndocker images --format \"table {{.Repository}}\\t{{.Tag}}\\t{{.Size}}\"\n\n# List all image IDs\ndocker images -q\n\n# List dangling images (images with no tag)\ndocker images -f \"dangling=true\"\n\n# Filter images by name\ndocker images alpine\n\n# Show all images including intermediate layers\ndocker images -a\n</code></pre> </li> </ul>"},{"location":"001-DockerCli/#docker-inspect","title":"<code>docker inspect</code>","text":"<ul> <li><code>docker inspect</code> provides detailed information about Docker objects (containers, images, volumes, networks).</li> <li>Returns a JSON array with all the metadata.     <pre><code># Create a container for inspection\ndocker run -d --name nginx-inspect -p 8080:80 nginx\n\n# Inspect a container\ndocker inspect nginx-inspect\n\n# Get specific information using format flag\ndocker inspect --format='{{.State.Running}}' nginx-inspect\n\n# Get IP address of container\ndocker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' nginx-inspect\n\n# Inspect an image\ndocker inspect nginx:latest\n\n# Get image creation date\ndocker inspect --format='{{.Created}}' nginx:latest\n\n# Clean up\ndocker stop nginx-inspect\ndocker rm nginx-inspect\n</code></pre></li> </ul>"},{"location":"001-DockerCli/#docker-kill","title":"<code>docker kill</code>","text":"<ul> <li><code>docker kill</code> immediately terminates a running container.</li> <li>Unlike <code>docker stop</code>, it sends SIGKILL by default (no graceful shutdown).     <pre><code># Create a running container\ndocker run -d --name kill-test alpine sleep 10000\n\n# Kill the container immediately\ndocker kill kill-test\n\n# Verify the container is stopped\ndocker ps -a | grep kill-test\n\n# Kill with specific signal\ndocker run -d --name kill-test2 nginx\ndocker kill --signal=SIGTERM kill-test2\n\n# Kill multiple containers\ndocker run -d --name c1 alpine sleep 1000\ndocker run -d --name c2 alpine sleep 1000\ndocker kill c1 c2\n\n# Clean up\ndocker rm kill-test kill-test2 c1 c2\n</code></pre></li> </ul>"},{"location":"001-DockerCli/#docker-logs","title":"<code>docker logs</code>","text":"<ul> <li><code>docker logs</code> fetches the logs of a container.</li> <li>Useful for debugging and monitoring container output.</li> <li>By default, it shows all logs since the container started.</li> <li>You can use various options to filter and format the logs.     <pre><code># Create a container that generates logs\ndocker run -d --name log-example alpine sh -c \"while true; do echo 'Hello from container'; sleep 2; done\"\n\n# View container logs\ndocker logs log-example\n\n# Follow log output (like tail -f)\ndocker logs -f log-example\n\n# Show only last 10 lines\ndocker logs --tail 10 log-example\n\n# Show logs with timestamps\ndocker logs -t log-example\n\n# Show logs since specific time\ndocker logs --since 5m log-example\n\n# Combine options\ndocker logs -f --tail 5 -t log-example\n\n# Clean up (press Ctrl+C to stop following logs)\ndocker stop log-example\ndocker rm log-example\n</code></pre></li> </ul>"},{"location":"001-DockerCli/#docker-pause","title":"<code>docker pause</code>","text":"<ul> <li><code>docker pause</code> suspends all processes in a container.</li> <li> <p>The container continues to exist but is frozen.</p> <pre><code># Create a running container\ndocker run -d --name pause-test alpine sh -c \"while true; do echo 'Running'; sleep 1; done\"\n\n# Pause the container\ndocker pause pause-test\n\n# Verify the container is paused\ndocker ps -a | grep pause-test\n\n# Try to see logs (no new logs while paused)\ndocker logs --tail 5 pause-test\n\n# Note: Container remains paused until unpaused\n# See docker unpause to resume\n</code></pre> </li> </ul>"},{"location":"001-DockerCli/#docker-ps","title":"<code>docker ps</code>","text":"<ul> <li>The \u201cproblem\u201d with the previous command is that the container is not removed once it exits.</li> <li>Lets look at the list of containers that we have right now on the host machine     <pre><code># List exiting containers on our host machine\n\n# Display running containers\ndocker ps\n\n# Display all containers\ndocker ps -a\n\n# Display all containers ids\ndocker ps -aq\n</code></pre></li> </ul>"},{"location":"001-DockerCli/#docker-pull","title":"<code>docker pull</code>","text":"<ul> <li><code>docker pull</code> downloads an image from a Docker registry (like Docker Hub).</li> <li>This command is useful when you want to download an image without running it immediately.     <pre><code># Pull the latest version of an image\ndocker pull alpine:latest\n\n# Pull a specific version\ndocker pull nginx:1.21\n\n# Pull from a specific registry\ndocker pull gcr.io/google-containers/busybox\n\n# Pull all tags of a repository\ndocker pull --all-tags alpine\n\n# Verify the pulled image\ndocker images | grep alpine\n</code></pre></li> </ul>"},{"location":"001-DockerCli/#docker-push","title":"<code>docker push</code>","text":"<ul> <li><code>docker push</code> uploads an image to a Docker registry.</li> <li>You need to be logged in to the registry and have proper permissions.     <pre><code># Tag an image for pushing (replace 'yourusername' with your Docker Hub username)\ndocker tag alpine:latest yourusername/my-alpine:v1.0\n\n# Login to Docker Hub (you'll be prompted for credentials)\n# docker login\n\n# Push the image to Docker Hub\n# docker push yourusername/my-alpine:v1.0\n\n# Push all tags\n# docker push --all-tags yourusername/my-alpine\n\n# Note: The push commands are commented out to prevent accidental pushes\n# Uncomment and replace 'yourusername' with your actual username when ready to use\n</code></pre></li> </ul>"},{"location":"001-DockerCli/#docker-rename","title":"<code>docker rename</code>","text":"<ul> <li><code>docker rename</code> changes the name of an existing container.</li> <li> <p>Useful for organizing or clarifying container purposes.</p> <pre><code># Create a container with a generic name\ndocker run -d --name old-name alpine sleep 10000\n\n# Rename the container\ndocker rename old-name new-name\n\n# Verify the new name\ndocker ps | grep new-name\n\n# Clean up\ndocker stop new-name\ndocker rm new-name\n</code></pre> </li> </ul>"},{"location":"001-DockerCli/#docker-restart","title":"<code>docker restart</code>","text":"<ul> <li><code>docker restart</code> stops and then starts a container.</li> <li> <p>Combines <code>docker stop</code> and <code>docker start</code> in one command.</p> <pre><code># Create a running container\ndocker run -d --name restart-test nginx\n\n# Restart the container (default 10 second grace period)\ndocker restart restart-test\n\n# Restart with custom timeout\ndocker restart -t 30 restart-test\n\n# Restart multiple containers\ndocker run -d --name r1 alpine sleep 1000\ndocker run -d --name r2 alpine sleep 1000\ndocker restart r1 r2\n\n# Clean up\ndocker stop restart-test r1 r2\ndocker rm restart-test r1 r2\n</code></pre> </li> </ul>"},{"location":"001-DockerCli/#docker-rm","title":"<code>docker rm</code>","text":"<ul> <li>Lets clean and remove the containers which are not running anymore</li> <li><code>docker rm</code> removes one or more stopped containers from your system.</li> <li>This helps free up system resources by deleting containers that are no longer needed.</li> </ul> <p>Removing Containers</p> <ul> <li>You <code>cannot</code> remove a running container without stopping it first.</li> <li>Use <code>docker ps -a</code> to list all containers (including stopped ones) before removing them.</li> <li>Warning: This action is irreversible.</li> </ul> <ul> <li>Example:     <pre><code># Remove all stopped containers\n# We use docker rm with the previous command we learned docker ps\ndocker rm $(docker ps -aq)\n\n# Verify that only running containers are still running\ndocker ps -a\n\n# Remove a specific container (must be stopped first)\ndocker stop nginx\ndocker rm nginx\n\n# Force remove a running container\ndocker rm -f nginx\n\n# Remove multiple containers\ndocker rm container1 container2 container3\n</code></pre></li> </ul>"},{"location":"001-DockerCli/#docker-rmi","title":"<code>docker rmi</code>","text":"<ul> <li><code>docker rmi</code> removes one or more Docker images from your system.</li> <li>This helps free up disk space by removing unused images.</li> <li>You cannot remove an image that is being used by a running container.</li> <li>If you need to remove such an image, you must stop and remove the container using it first.</li> <li>Tip: Use <code>docker ps -a</code> to list all containers (including stopped ones) before removing them.     <pre><code># Remove a specific image\ndocker rmi alpine:latest\n\n# Remove multiple images\ndocker rmi image1:tag1 image2:tag2\n\n# Remove image by ID\ndocker rmi abc123def456\n\n# Force remove an image (even if containers are using it)\ndocker rmi -f nginx:latest\n\n# Remove all dangling images (untagged images)\ndocker rmi $(docker images -f \"dangling=true\" -q)\n\n# Remove all images\n# WARNING: This removes ALL images on your system\n# docker rmi $(docker images -q)\n</code></pre></li> </ul>"},{"location":"001-DockerCli/#docker-run","title":"<code>docker run</code>","text":"<ul> <li>The <code>run</code> command container many options (flags), we will not cover all of them</li> <li>docs.docker.com - run</li> <li>Run your first container:      <pre><code># Run the first container\ndocker run hello-world\n\n### Output:\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n...\n</code></pre></li> </ul>"},{"location":"001-DockerCli/#docker-run-a","title":"<code>docker run -a</code>","text":"<ul> <li>The <code>--attach</code> [<code>-a</code>] flag tells docker run to bind to the container\u2019s <code>STDIN</code>, <code>STDOUT</code> or <code>STDERR</code>.      <pre><code># Pass input from stdin to container\ndocker run -a stdout alpine echo \"Docker rocks !!\"\n\n# Redirect stdout logs to a file\ndocker run -a stdout -a stderr alpine echo 'Docker rocks again !!' &gt; log.txt 2&gt;&amp;1\n\n# Print the log content\ncat log.txt\n</code></pre></li> </ul>"},{"location":"001-DockerCli/#docker-run-d","title":"<code>docker run -d</code>","text":"<ul> <li>Spin up the container which will run in the background</li> <li>By default when you spin a docker container it will attach itself to the current terminal.</li> <li>In order to avoid it we will use the -d flag to specify that the container should be running in the background.     <pre><code># Spin an nginx in the background.\n# Add a sleep timeout so that the container will not exit immediately\ndocker run -d alpine sleep 10000\n\n# Verify that the container is still running\ndocker ps -a\n</code></pre></li> </ul>"},{"location":"001-DockerCli/#docker-run-it","title":"<code>docker run -it</code>","text":"<p>Interactive Terminal</p> <ul> <li>The flags <code>-it</code> stands for:  <code>-i</code> [<code>--interactive</code>]   keeps the container\u2019s STDIN open, and lets you send input to the container through standard input.   <code>-t</code> [<code>--tty</code>]   Attaches a pseudo-TTY to the container, connecting your terminal to the I/O streams of the container.</li> </ul> <ul> <li>Example - Run an interactive shell inside an alpine container     <pre><code># Execute a command on the container and interact with the container\n# This command will change the password for root user\ndocker run -it alpine passwd root\n</code></pre></li> </ul>"},{"location":"001-DockerCli/#docker-run-name","title":"<code>docker run -name</code>","text":"<ul> <li>By default the container will be assigned a semi-random name based upon the following code: docker-ce/names-generator.go</li> <li>We can assign our desired name to the container with the <code>--name</code> option     <pre><code># Spin an nginx in the background.\n# Add a sleep timeout so that the container will not exit immediately\ndocker run --name alpine001 alpine\n\n# Verify that the container has been created with the given name\ndocker ps -a |  grep alpine001\n</code></pre></li> </ul>"},{"location":"001-DockerCli/#docker-run-p","title":"<code>docker run -p</code>","text":"<p>Port Mapping</p> <ul> <li>We can specify the exact ports we wish to open <code>-p</code> or open them all <code>-P</code> </li> </ul> <ul> <li>Run a container and connect to a port on the host which will be used to connect to the container     <pre><code># Execute an nginx container and test the container\n\n# Remove any containers with the same name\ndocker stop nginx\ndocker rm   nginx\n\n# We will combine few flags here\ndocker  run   -it  --rm     \\\n              -d            \\\n              -p 8888:80    \\\n              --name nginx  \\\n              nginx\n\n# Wait for the container to be created\nsleep 3\n\n# test the container\ncurl -s localhost:8888\n\n# Remove the container\ndocker kill nginx\n</code></pre></li> </ul>"},{"location":"001-DockerCli/#docker-start","title":"<code>docker start</code>","text":"<ul> <li><code>docker start</code> starts one or more stopped containers.</li> <li>Unlike <code>docker run</code>, this command starts an existing container.</li> <li>It does not create a new container.     <pre><code># Create a container but don't start it immediately\ndocker create --name my-alpine alpine echo \"Hello World\"\n\n# Start the container\ndocker start my-alpine\n\n# Start and attach to container output\ndocker start -a my-alpine\n\n# Start multiple containers\ndocker start container1 container2 container3\n\n# Start a stopped container interactively\ndocker run -it --name interactive-alpine alpine sh\n# (exit the shell to stop the container)\ndocker start -ai interactive-alpine\n\n# Clean up\ndocker rm my-alpine interactive-alpine\n</code></pre></li> </ul>"},{"location":"001-DockerCli/#docker-stats","title":"<code>docker stats</code>","text":"<ul> <li><code>docker stats</code> displays a live stream of resource usage statistics for containers.</li> <li>Shows CPU, memory, network I/O, and disk I/O usage.</li> <li>Useful for monitoring container performance in real-time.</li> <li>Can be used to identify resource bottlenecks and optimize container performance.</li> <li>Supports filtering and formatting options for customized output.     <pre><code># Create some containers\ndocker run -d --name stats-test1 nginx\ndocker run -d --name stats-test2 alpine sleep 10000\n\n# Display stats for all running containers (live stream)\n# Press Ctrl+C to exit\ndocker stats\n\n# Display stats for specific containers\ndocker stats stats-test1 stats-test2\n\n# Display stats without streaming (single snapshot)\ndocker stats --no-stream\n\n# Custom format\ndocker stats --format \"table {{.Container}}\\t{{.CPUPerc}}\\t{{.MemUsage}}\"\n\n# Clean up\ndocker stop stats-test1 stats-test2\ndocker rm stats-test1 stats-test2\n</code></pre></li> </ul>"},{"location":"001-DockerCli/#docker-stop","title":"<code>docker stop</code>","text":"<ul> <li><code>docker stop</code> stops one or more running containers gracefully.</li> <li> <p>Sends SIGTERM signal first, then SIGKILL after grace period.</p> <pre><code># Create a running container\ndocker run -d --name test-nginx nginx\n\n# Stop the container (default 10 second grace period)\ndocker stop test-nginx\n\n# Stop with custom timeout\ndocker stop -t 30 test-nginx\n\n# Stop multiple containers\ndocker stop container1 container2 container3\n\n# Stop all running containers\ndocker stop $(docker ps -q)\n\n# Verify container is stopped\ndocker ps -a | grep test-nginx\n\n# Clean up\ndocker rm test-nginx\n</code></pre> </li> </ul>"},{"location":"001-DockerCli/#docker-top","title":"<code>docker top</code>","text":"<ul> <li><code>docker top</code> displays the running processes inside a container.</li> <li>Similar to the Linux <code>top</code> command but for containers.   <pre><code># Create a running container\ndocker run -d --name top-test nginx\n\n# Display running processes in the container\ndocker top top-test\n\n# Display with custom ps options\ndocker top top-test aux\n\n# Display specific columns\ndocker top top-test -eo pid,comm\n\n# Create a busier container to see more processes\ndocker run -d --name busy-container alpine sh -c \"sleep 100 &amp; sleep 200 &amp; sleep 300 &amp; wait\"\ndocker top busy-container\n\n# Clean up\ndocker stop top-test busy-container\ndocker rm top-test busy-container\n</code></pre></li> </ul>"},{"location":"001-DockerCli/#docker-unpause","title":"<code>docker unpause</code>","text":"<ul> <li><code>docker unpause</code> resumes all processes in a paused container.</li> <li>Used in conjunction with <code>docker pause</code>.   <pre><code># Create and pause a container\ndocker run -d --name unpause-test alpine sh -c \"while true; do echo 'Running'; sleep 1; done\"\ndocker pause unpause-test\n\n# Verify container is paused\ndocker ps -a | grep unpause-test\n\n# Unpause the container\ndocker unpause unpause-test\n\n# Verify container is running again\ndocker ps | grep unpause-test\n\n# Check logs to see it resumed\ndocker logs --tail 5 unpause-test\n\n# Clean up\ndocker stop unpause-test\ndocker rm unpause-test\n</code></pre></li> </ul>"},{"location":"001-DockerCli/#docker-wait","title":"<code>docker wait</code>","text":"<ul> <li><code>docker wait</code> blocks until one or more containers stop.</li> <li> <p>Returns the exit code of the container.</p> <pre><code># Create a container that will exit after 5 seconds\ndocker run -d --name wait-test alpine sh -c \"sleep 5; exit 42\"\n\n# Wait for the container to exit and get the exit code\necho \"Waiting for container to exit...\"\ndocker wait wait-test\n# This will return 42 after 5 seconds\n\n# Wait for multiple containers\ndocker run -d --name w1 alpine sh -c \"sleep 3; exit 0\"\ndocker run -d --name w2 alpine sh -c \"sleep 2; exit 1\"\ndocker wait w1 w2\n\n# Clean up\ndocker rm wait-test w1 w2\n</code></pre> </li> </ul>"},{"location":"002-DockerFile/","title":"002-DockerFile","text":""},{"location":"002-DockerFile/#lab-002-create-a-basic-container-using-dockerfile","title":"Lab 002 - Create a basic container using Dockerfile","text":"<ul> <li>In this lab we will create our first container using <code>Dockerfile</code></li> <li>The container will be used to serve a simple <code>NodeJs</code> web server</li> <li>No NodeJs knowledge is required</li> <li>You will need to create, build, tag &amp; push your container to DockerHub</li> <li> <p>The lab is divided into the several tasks.</p> <ul> <li>01. Prepare the server code</li> <li>02. Test the <code>server.js</code> code</li> <li>03. Write the <code>Dockerfile</code></li> <li>04. Build the image</li> <li>05. Login to DockerHub</li> <li>06. Push the image to DockerHub</li> <li>07. Verify the push</li> <li>08. Test the image</li> <li>09. Test the server</li> <li>10. Clean up</li> </ul> </li> </ul>"},{"location":"002-DockerFile/#01-prepare-the-server-code","title":"01. Prepare the server code","text":"<ul> <li>Our container will include the following NodeJs simple web server</li> <li>Copy the code below and save it to a file named <code>server.js</code> <pre><code>//\n// Filename: server.js\n//\n// Simple NodeJs Server\n// The server is listening by default to port 8888\n//\n\n// import the HTTP module\nvar http = require('http');\n\n// Define a port we want to listen to\n// Later on we will pass the port as env parameter\n// Default port is set to 8888\nconst PORT= process.env.PORT || 8888; \n\n// Create the server and listen for requests\n// Create the server and listen for requests\nhttp.createServer((request, response)=&gt;{\n    response.end('Server is running.!! You asked for: ' + request.url);\n}).listen(PORT, ()=&gt;{\n    // Callback is triggered when server is getting a request\n    console.log(\"Server listening on: http://localhost:%s\", PORT);\n});\n</code></pre></li> </ul>"},{"location":"002-DockerFile/#02-test-the-serverjs-code","title":"02. Test the <code>server.js</code> code","text":"<ul> <li>Before we \u201cpack\u201d our code in Docker image lets test the code</li> <li>We will test the code inside <code>nodejs</code> docker      <pre><code># Test the node code\n#   --rm            =   remove the container when done\n#   -d              =   run in detached mode\n#   -p              =   open the required ports\n#   -v              =   volume\n#   -w              =   workdir\n#   --name          =   the container name\n#   node            =   Execute a nodejs container to test our code\n#   node server.js  =   Execute the code\ndocker   run --rm -d       \\\n  -v     $(pwd):/usr/src  \\\n  -w     /usr/src         \\\n  -p     8888:8888        \\\n  --name node_server      \\\n  node                    \\\n  node server.js   \n</code></pre></li> </ul>"},{"location":"002-DockerFile/#03-write-the-dockerfile","title":"03. Write the <code>Dockerfile</code>","text":"<ul> <li>Now lets create a <code>Dockerfile</code> with the code we just created above</li> <li>The <code>Dockerfile</code> will be based upon <code>nodejs</code> image and will include our <code>server.js</code> <pre><code>#\n# Filename: Dockerfile\n#\n# Use node as our base image\nFROM      node\n\n# Optional: Set working directory\nWORKDIR   /usr/src\n\n# Copy the server code to our working directory [.]\nCOPY      server.js .\n\n# Mark the port which will required for the server\nEXPOSE    8888\n\n# Start the server when the container is started\nCMD       [\"node\", \"./server.js\"]\n</code></pre></li> </ul>"},{"location":"002-DockerFile/#04-build-the-image","title":"04 - Build the image","text":"<ul> <li>Once we have the docker file we can build the image</li> <li>Once the image is ready we will push it to DockerHub so you will need an account.</li> <li>We will name the image: \u201cYour Dockerhub username/repository:version\u201d     <pre><code>###\n### Build the image\n### Tag the image with the following \n###     DockerHub username/repository:version\n###\ndocker build -t nirgeier/docker-labs-002 .\n</code></pre></li> </ul>"},{"location":"002-DockerFile/#05-login-to-dockerhub","title":"05. Login to DockerHub","text":"<ul> <li>Login to DockerHub</li> <li>Execute <code>docker login</code> and enter your Docker Hub credentials when prompted</li> <li>If you don\u2019t have a DockerHub account, create one at https://hub.docker.com/signup</li> <li>You will need to push the image to DockerHub in the next step</li> </ul>"},{"location":"002-DockerFile/#06-push-the-image-to-dockerhub","title":"06. - Push the image to DockerHub","text":"<p>Docker Login Required</p> <p>You must login to Docker Hub before you can push to DockerHub</p> <ul> <li>Example: <code>docker push username/image:tag</code> </li> </ul>"},{"location":"002-DockerFile/#07-verify-the-push","title":"07. Verify the push","text":"<ul> <li>Login to your DockerHub account and verify that the image exists under your DockerHub account.</li> </ul>"},{"location":"002-DockerFile/#08-test-the-image","title":"08 - Test the image","text":"<ul> <li>Last step is to test our image</li> <li>To do so we will pull and run the image from DockerHub</li> <li>Once the container is started we will test the server     <pre><code>###\n### Pull the image from DockerHub\n###  Replace the image tag with your image tag\n### \ndocker   run -d               \\\n        --name 002-container \\\n        -p 8888:8888         \\\n        nirgeier/docker-labs-002\n\n### Check that the container is working as expected\ndocker logs 002-container         \n</code></pre></li> </ul>"},{"location":"002-DockerFile/#09-test-the-server","title":"09. Test the server","text":"<ul> <li> <p>Test the server that he is running on docker.</p> <pre><code># Test the server that he is running on docker\ncurl -s localhost:8888\n\n### ExpectedOutput:\nServer is running.!! You asked for: /\n</code></pre> </li> </ul>"},{"location":"002-DockerFile/#10-clean-up","title":"10. Clean up","text":"<ul> <li>Stop and remove the container     <pre><code># Stop the container\n# If we used --rm the container should remove itself\ndocker stop 002-container\n\n# If not used --rm - remove the container\ndocker rm 002-container\n</code></pre></li> </ul>"},{"location":"003-DockerFile-MultiStage/","title":"003-DockerFile-MultiStage","text":""},{"location":"003-DockerFile-MultiStage/#lab-003-writing-docker-multi-stage-build","title":"Lab 003 - Writing Docker multi-stage build","text":"<ul> <li>In this lab we will learn how to write a multi-stage Docker file</li> <li>A multistage build allows you to use multiple images to build a final product. </li> <li>In a multistage build, you have a single Dockerfile which build up multiple images inside it to help build the final image.</li> </ul>"},{"location":"003-DockerFile-MultiStage/#why-use-multistage-builds","title":"Why Use Multistage Builds?","text":"<ul> <li> <p> Reduce Image Size</p> <ul> <li>Use a minimal base image for the final stage</li> <li>Only copy necessary artifacts to the final image</li> <li>Exclude build tools and intermediate files from production image</li> </ul> </li> <li> <p> Improve Security</p> <ul> <li>Exclude build tools and secrets from the runtime image</li> <li>Limit the attack surface by using a smaller final image</li> <li>Use a non-root user in the final stage</li> <li>Reduce vulnerabilities by minimizing installed packages</li> </ul> </li> <li> <p> Better Build Performance</p> <ul> <li>Leverage Docker\u2019s layer caching mechanism to speed up builds</li> <li>Only rebuild stages that have changed</li> <li>Each stage can be built and tested independently</li> <li>Parallel stage execution when possible</li> </ul> </li> <li> <p> Cleaner and More Maintainable Dockerfiles</p> <ul> <li>Separate concerns by using multiple named stages</li> <li>No need for manual cleanup of build dependencies</li> <li>Easier to read and maintain with clear stage purposes</li> <li>Use meaningful stage names for better clarity</li> <li>Add comments to explain each stage\u2019s role</li> </ul> </li> <li> <p> Flexible Dependency Management</p> <ul> <li>Install build dependencies in one stage and runtime dependencies in another</li> <li>Use different base images optimized for each stage (e.g., <code>golang:alpine</code> for build, <code>alpine</code> for runtime)</li> <li>Use the best-suited image for each stage without bloating the final image</li> </ul> </li> <li> <p> Simplified CI/CD Pipelines</p> <ul> <li>Combine build, test, and deploy stages in a single Dockerfile</li> <li>Use <code>--target</code> flag to build specific stages for different environments</li> <li>Consistent build process across development and production</li> </ul> </li> </ul>"},{"location":"003-DockerFile-MultiStage/#01-create-multi-stage-docker-file","title":"01. Create multi-stage docker file","text":"<ul> <li>The first step is to create a Dockerfile.</li> <li>Later on we will pass build time arguments to this file to build the desired image</li> <li><code>Dockerfile</code> <pre><code># Get the value of the desired image to build\nARG     BASE_IMG=curl\n\n# Build the base image \nFROM    alpine AS base_image\n\n# Add some content to the 2nd image\nFROM    base_image  AS build-curl\nRUN     echo -e \"This file is from curl image\" &gt; image.txt\n\n# Add some content to the 3rd image\nFROM    base_image  AS build-bash\nRUN     echo -e \"This file is from bash image\" &gt; image.txt\n\n# Build the desired image\nFROM    build-${BASE_IMG}\n\n# We can use the FROM command as we see in the previous line or use the\n# We can also use image index instead\n# COPY  --from=build-${BASE_IMG} image.txt . to copy a specific content\nRUN     cat image.txt\nCMD     [\"cat\", \"image.txt\"]\n</code></pre></li> </ul>"},{"location":"003-DockerFile-MultiStage/#02-build-the-desired-images","title":"02. Build the desired images","text":"<ul> <li>We will use the following script to build multiple images and to test the results     <pre><code>#!/bin/bash -x\n\n# Build The curl based image (no-cache)\ndocker build --build-arg BASE_IMG=curl --no-cache -t curl1 .\n\n# Build The bash based image (no-cache)\ndocker build --build-arg BASE_IMG=bash --no-cache -t bash1 .\n\n### Build with cache\necho -e \"\"\necho -e \"---------------------------------\"\necho -e \"\"\n# Build The curl based image (with cache)\ndocker build --build-arg BASE_IMG=curl -t curl2 .\n\n# Build The bash based image (with cache)\ndocker build --build-arg BASE_IMG=bash -t bash2 .\n</code></pre></li> </ul>"},{"location":"003-DockerFile-MultiStage/#03-test-the-images","title":"03. Test the images","text":"<ul> <li> <p>We will now test the 4 images we build perviously     <pre><code># Debug mode\nset -x\n\n# Test the output images\ndocker run curl1\ndocker run curl2\ndocker run bash1\ndocker run bash2\n</code></pre></p> </li> <li> <p>You should see output similar to this one:     <pre><code>+ docker run curl1\nThis file is from curl image\n+ docker run curl2\nThis file is from curl image\n+ docker run bash1\nThis file is from bash image\n+ docker run bash2\nThis file is from bash image\n</code></pre></p> </li> </ul>"},{"location":"003-DockerFile-MultiStage/#04-quiz","title":"04. Quiz","text":"<ul> <li>What will be the results of this docker file?</li> <li>Try to answer and then build the following <code>Dockerfile</code> to see the results         <pre><code># Build the base image\nFROM    alpine AS base_image\n\n# Add some packages to the base image\nFROM    base_image  AS build-curl\nRUN     echo -e \"\\033[1;33mThis file is from curl image\\033[0m\" &gt; image.txt\n\n# Add some packages to the base image\nFROM    base_image  AS build-bash\nRUN     echo -e \"\\033[1;32mThis file is from bash image\\033[0m\" &gt; image.txt\n\n#   Build the desired image\nFROM    build-curl\nCOPY    --from=2 image.txt .\nRUN cat image.txt\nCMD [\"cat\", \"image.txt\"]\n</code></pre></li> <li> <p>Test your answer with the following command</p> <pre><code>docker build -f Dockerfile2 .\n</code></pre> </li> </ul>"},{"location":"003-DockerFile-MultiStage/#05-build-a-specific-target","title":"05. Build a specific target","text":"<ul> <li>We can build our specific image and stop at the desired stage</li> <li> <p>In other words we don\u2019t need to build all the images within the docker file</p> <pre><code>docker build --target build-curl -f Dockerfile2 .\n</code></pre> </li> </ul>"},{"location":"003-DockerFile-MultiStage/#06-in-class-exercise","title":"06. In-Class Exercise","text":"<ul> <li>Create a <code>multi-stage</code> docker file that will build 2 images</li> <li>The first image will be based on <code>alpine</code> and will create a file named <code>alpine.txt</code> with the content: <code>This is alpine image</code></li> <li>The second image will be based on <code>node</code> and will create a file named <code>node.txt</code> with the content: <code>This is node image</code></li> <li>The final image should be based on <code>alpine</code> and should copy the files which you created from the previous stages and display their content when the container will run.</li> <li>Hint: Use the <code>COPY --from=</code> command to copy files from previous stages</li> </ul> Solution  ### Dockerfile Solution  Create a file named `Dockerfile-exercise`:  <pre><code># First stage: Alpine image\nFROM  alpine AS alpine-stage\nRUN   echo \"This is alpine image\" &gt; alpine.txt\n\n# Second stage: Node image\nFROM  node AS node-stage\nRUN   echo \"This is node image\" &gt; node.txt\n\n# Final stage: Alpine with files from previous stages\nFROM  alpine\nCOPY  --from=alpine-stage alpine.txt  .\nCOPY  --from=node-stage node.txt      .\n\n# Run the command to display contents\nCMD   cat alpine.txt &amp;&amp; cat node.txt\n</code></pre>  ### Build and Test  Build the image:  <pre><code>docker build -f Dockerfile-exercise -t exercise-solution .\n</code></pre>  Run the container:  <pre><code>docker run exercise-solution\n</code></pre>  Expected output:  <pre><code>This is alpine image\nThis is node image\n</code></pre>  ### Explanation  1. **First Stage (alpine-stage)**: Based on `alpine`, creates `alpine.txt` with the required content 2. **Second Stage (node-stage)**: Based on `node`, creates `node.txt` with the required content 3. **Final Stage**: Based on `alpine` (lightweight), copies both files from previous stages using `COPY --from=` and displays their content when run"},{"location":"004-LocalRegistry/","title":"004-LocalRegistry","text":""},{"location":"004-LocalRegistry/#lab-004-local-docker-registry","title":"Lab 004 - Local Docker Registry","text":"<ul> <li>A collection of Hands-on Docker labs.</li> <li>Each lab is a standalone lab and does not require to complete the previous labs.</li> </ul>"},{"location":"004-LocalRegistry/#pre-requirements","title":"Pre-Requirements","text":"<ul> <li>Docker installed</li> <li>Dockerfile knowledge </li> <li>DockerHub account</li> </ul>"},{"location":"004-LocalRegistry/#lab-setup-basic-local-docker-registry","title":"Lab: Setup Basic Local Docker Registry","text":"<ul> <li>In this lab we will learn how to create a local Docker registry.</li> <li>In this lab we will learn how to push and pull images from the local registry.</li> <li>in this lab we will be using the default configuration, but of course you can change it as you wish.</li> <li>Configuration docs: https://docs.docker.com/registry/configuration</li> </ul> <ul> <li>01. Create a basic local registry</li> <li>02. Prepare the local images</li> <li>02.01. Download busybox image</li> <li>02.01. Tag the image with the local registry prefix</li> <li>02.02. Push the image to the local registry</li> <li>03. Test local images</li> </ul>"},{"location":"004-LocalRegistry/#01-create-a-basic-local-registry","title":"01. Create a basic local registry","text":"<ul> <li>The first step is to create a local registry.</li> <li>For this we will use the <code>docker run</code> command with the docker <code>registry</code>- https://hub.docker.com/_/registry image.</li> </ul> <pre><code># Run the registry container\ndocker  run                     \\\n        -d                      \\\n        -p 5000:5000            \\\n        --restart always        \\\n        --name registry         \\\n        registry:latest\n</code></pre>"},{"location":"004-LocalRegistry/#02-prepare-the-local-images","title":"02. Prepare the local images","text":"<ul> <li>We will download the images from DockerHub and push them to the local registry.</li> </ul>"},{"location":"004-LocalRegistry/#0201-download-busybox-image","title":"02.01. Download busybox image","text":"<pre><code># download busybox image from docker-hub\ndocker pull busybox\n</code></pre>"},{"location":"004-LocalRegistry/#0201-tag-the-image-with-the-local-registry-prefix","title":"02.01. Tag the image with the local registry prefix","text":"<pre><code># Tag the busybox image with the local registry prefix\ndocker tag busybox localhost:5000/busybox\n</code></pre>"},{"location":"004-LocalRegistry/#0202-push-the-image-to-the-local-registry","title":"02.02. Push the image to the local registry","text":"<pre><code># Once we have the appropriate tag, we can push the image to the local registry\ndocker push localhost:5000/busybox\n</code></pre>"},{"location":"004-LocalRegistry/#03-test-local-images","title":"03. Test local images","text":"<pre><code># List all local repositories\ncurl -X GET http://localhost:5000/v2/_catalog\n\n# List all tags for a repository\ncurl -X GET https://myregistry:5000/v2/ubuntu/tags/list\n</code></pre>"},{"location":"004-LocalRegistry/AdvancedLocalRegistry/","title":"Index","text":""},{"location":"004-LocalRegistry/AdvancedLocalRegistry/#docker-hands-on-repository","title":"Docker Hands-on Repository","text":"<ul> <li>A collection of Hands-on Docker labs.</li> <li>Each lab is a standalone lab and does not require to complete the previous labs.</li> </ul>"},{"location":"004-LocalRegistry/AdvancedLocalRegistry/#ctrl-click-to-open-in-new-window","title":"CTRL + click to open in new window","text":""},{"location":"004-LocalRegistry/AdvancedLocalRegistry/#pre-requirements","title":"Pre-Requirements","text":"<ul> <li>Docker installation</li> </ul>"},{"location":"004-LocalRegistry/AdvancedLocalRegistry/#lab-0201-setup-advanced-local-docker-registry","title":"Lab 0201. Setup Advanced Local Docker Registry","text":"<ul> <li>In the previous lab we created a basic local registry.</li> <li>In this lab we will create a local registry with advanced features.</li> <li>The local registry will be accessible from the host machine and will be build upon</li> <li>Nginx</li> <li>Docker registry image</li> <li>Docker compose</li> <li>Secured with certificates</li> </ul>"},{"location":"004-LocalRegistry/AdvancedLocalRegistry/#step-01-create-registry-directories","title":"Step 01. Create Registry Directories","text":"<pre><code># Create the required directories for the advanced configuration\nmkdir -p                      \\\n      registry/nginx          \\\n      registry/nginx/conf.d   \\\n      registry/nginx/ssl      \\\n      registry/auth\necho 'Docker rocks !!!' | docker run -it -a stdin alpine cat -\n# On GCP shell we cont have tree by default, so lets install it\nsudo apt install -y tree\n\n# Verify that the directories were created\ncd registry &amp;&amp; tree\n\n# We should see the following structure\n.\n\u251c\u2500\u2500 auth\n\u2514\u2500\u2500 nginx\n    \u251c\u2500\u2500 conf.d\n    \u2514\u2500\u2500 ssl\n</code></pre>"},{"location":"004-LocalRegistry/AdvancedLocalRegistry/#step-02-create-docker-compose-for-the-registry-services","title":"Step 02. Create Docker-Compose for the registry services","text":"<pre><code># Create the docker-compose file in the registry directory\ncat &lt;&lt; EOF &gt; registry/docker-compose.yml\nversion: '3'\nservices:\n  # The docker registry service\n  registry:\n    # The name of the container\n    container_name: registry\n\n    # The registry image which we will use\n    image: registry:2\n    # Ensures to start Docker Registry\n    restart: always\n    # The port on which the registry will be listen on\n    ports:\n    - \"5000:5000\"\n    # Registry environment variables\n\n    # The service will mount the docker volume \"registrydata\" and\n    # the local directory \"auth\",\n    # along with its authentication file \"registry.passwd\".\n    environment:\n      REGISTRY_AUTH: htpasswd\n      REGISTRY_AUTH_HTPASSWD_REALM: Registry-Realm\n      REGISTRY_AUTH_HTPASSWD_PATH: /auth/registry.passwd\n      REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY: /data\n\n    # mounted volumes\n    volumes:\n      - registrydata:/data\n      - ./auth:/auth\n\n    # The desired network\n    networks:\n      - bridge_network\n\n  #### Nginx Service\n  nginx:\n\n    # The name of the container\n    container_name: nginx\n\n    # We depends on the registry service\n    depends_on:\n      - registry\n\n    # nginx image\n    image: nginx:alpine\n    container_name: nginx\n    restart: unless-stopped\n    tty: true\n\n    # The desired listen ports\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n\n    # The mounted volumes for the configuration files\n    # Mount the local directory for virtual configuration (conf.d)\n    # and SSL certificates (ssl).\n    volumes:\n      - ./nginx/conf.d/:/etc/nginx/conf.d/\n      - ./nginx/ssl/:/etc/nginx/ssl/\n\n    # The desired network\n    networks:\n      - bridge_network\n\n# Docker Networks for those services\nnetworks:\n  # Define a bridge network names \"bridge_network\"\n  bridge_network:\n    driver: bridge\n\n# Define custom volume for the registry data named \"registrydata\"\n# using the \"local\" driver\nvolumes:\n  registrydata:\n    driver: local\n\nEOF\n</code></pre>"},{"location":"004-LocalRegistry/AdvancedLocalRegistry/#step-03-create-the-nginx-configuration-file","title":"Step 03. Create the Nginx configuration file","text":"<ul> <li>The next step is configuring a Nginx virtual host for the Nginx service.</li> <li>Create a new virtual host file named <code>registry.conf</code> under <code>nginx/conf.d/</code></li> </ul> <pre><code># Create a new virtual host file for our nginx service\ncat &lt;&lt; EOF &gt; nginx/conf.d/registry.conf\nupstream docker-registry {\n  server registry:5000;\n}\n\nserver {\n  listen      443 ssl http2;\n\n  # SSL\n  ssl on;\n  ssl_certificate /etc/nginx/ssl/registry.crt;\n  ssl_certificate_key /etc/nginx/ssl/registry.key;\n\n  # disable any limits to avoid HTTP 413 for large image uploads\n  # We will use `/etc/nginx/conf.d/additional.conf` as well later below\n  client_max_body_size 0;\n\n  # required to avoid HTTP 411: see Issue #1486 (https://github.com/docker/docker/issues/1486)\n  chunked_transfer_encoding on;\n\n  location /v2/ {\n    # Do not allow connections from docker 1.5 and earlier\n    # docker pre-1.6.0 did not properly set the user agent on ping, catch \"Go *\" user agents\n    if (\\$http_user_agent ~ \"^(docker\\/1\\.(3|4|5(?!\\.[0-9]-dev))|Go ).*$\" ) {\n      return 404;\n    }\n\n    # To add basic authentication to v2 use auth_basic setting plus add_header\n    # auth_basic \"registry.localhost\";\n    # auth_basic_user_file /etc/nginx/conf.d/registry.password;\n    # add_header 'Docker-Distribution-Api-Version' 'registry/2.0' always;\n\n    proxy_pass                          http://docker-registry;\n    proxy_set_header  Host              \\$http_host;   # required for docker client's sake\n    proxy_set_header  X-Real-IP         \\$remote_addr; # pass on real client's IP\n    proxy_set_header  X-Forwarded-For   \\$proxy_add_x_forwarded_for;\n    proxy_set_header  X-Forwarded-Proto \\$scheme;\n    proxy_read_timeout                  900;\n  }\n}\nEOF\n</code></pre>"},{"location":"004-LocalRegistry/AdvancedLocalRegistry/#step-04-increase-nginx-file-upload-size","title":"Step 04. Increase Nginx File Upload Size","text":"<ul> <li>By default, Nginx limits the file upload size to <code>1MB</code>.</li> <li>Most Docker images exceed <code>1MB</code> in size so we will increase the maximum file size on our Nginx service to <code>2GB</code>.</li> </ul> <pre><code># Increase the maximum file upload size to 2GB\necho  'client_max_body_size 2G;' &gt; registry/nginx/conf.d/additional.conf\n</code></pre>"},{"location":"004-LocalRegistry/AdvancedLocalRegistry/#step-05-configure-ssl-certificate","title":"Step 05. Configure SSL Certificate","text":"<ul> <li>For our secured authentication we will use self-signed certificates.</li> <li>lets generate the certificate and key files</li> </ul> <pre><code># Generate the self-signed certificate\nopenssl \\\n    req                     \\\n    -x509                   \\\n    -sha256                 \\\n    -newkey   rsa:4096      \\\n    -days     3650          \\\n    -nodes                  \\\n    -subj \"/CN=localhost\"   \\\n    -addext \"subjectAltName=DNS:localhost,DNS:localhost,IP:127.0.0.1\" \\\n    -keyout   registry/nginx/ssl/registry.key  \\\n    -out      registry/nginx/ssl/registry.crt\n\n\n# Verify the certificate and the key\nopenssl x509 -text  -noout -in registry/nginx/ssl/registry.crt\n\n# Verify that the key and the certificate matches\nopenssl rsa -check -noout -in registry/nginx/ssl/registry.key\n\n# We search a matching certificate and key md5 fingerprint\necho 'registry.key Checksum is: ' \\\n      $(openssl rsa -modulus -noout -in registry/nginx/ssl/registry.key | openssl md5)\n\necho 'registry.crt Checksum is: '  \\\n      $(openssl x509 -modulus -noout -in registry/nginx/ssl/registry.crt | openssl md5)\n</code></pre>"},{"location":"004-LocalRegistry/AdvancedLocalRegistry/#step-06-configure-authentication","title":"Step 06. Configure authentication","text":"<ul> <li>There are several ways to generate a password file for the registry.</li> </ul> <p>#### Option 01. using <code>htpasswd</code></p> <ul> <li>The <code>htpasswd</code> utility is a simple utility that can be used to create a password file.</li> <li>If you don\u2019t have this installed, first we need to instal it if is not installed.</li> </ul> <pre><code># install htpasswd\nsudo apt install -y apache2-utils\n\n# Now generate the password file\n# Note: You will need to enter the password twice\n#       `-c` - Create a new file.\n#       -B   - Force bcrypt encryption of the password (very secure).\nhtpasswd -Bc registry/nginx/ssl/registry.passwd $USER\n</code></pre> <p>#### Option 02. using <code>openssl</code></p> <pre><code># Generate a random password\nprintf \\\n        \"USER:$(openssl passwd -crypt PASSWORD)\\n\" &gt;&gt; \\\n        registry/nginx/ssl/registry.passwd\n\n# Verify that the password was generated\ncat registry/nginx/ssl/registry.passwd\n</code></pre>"},{"location":"004-LocalRegistry/AdvancedLocalRegistry/#step-07-add-the-root-ca-certificate","title":"Step 07. Add the Root CA Certificate","text":"<ul> <li>Next step is to add the Root CA certificate to Docker</li> <li>We will place certificates under the docker certificates folder for our domain</li> </ul> <pre><code># Create the required folders for the certificate\nsudo mkdir -p /etc/docker/certs.d/registry.codewizard.co.il\n\n# Copy the certificate to the docker certificates folder\nsudo cp \\\n        registry/nginx/ssl/registry.crt \\\n        /etc/docker/certs.d/registry.codewizard.co.il/rootCA.crt\n\n# Create the second folder for the certificate\nsudo mkdir -p /etc/docker/certs.d/codewizard.co.il\n\n# Copy the certificate to the second certificates folder\nsudo cp \\\n        registry/nginx/ssl/registry.crt \\\n        /etc/docker/certs.d/codewizard.co.il/rootCA.crt\n\n# Copy the certificate into /usr/share/ca-certificate/extra\nsudo mkdir -p /usr/local/share/ca-certificates/\nsudo cp \\\n        registry/nginx/ssl/registry.crt \\\n        /usr/local/share/ca-certificates/rootCA.crt\n</code></pre> <ul> <li>Once we have the certificates in place we can add them to the list of trusted certificates.</li> </ul> <pre><code># Add the certificate to the list of trusted certificates\nsudo update-ca-certificates\n</code></pre>"},{"location":"004-LocalRegistry/AdvancedLocalRegistry/#step-08-restart-docker-registry","title":"Step 08. Restart Docker registry","text":"<p>/etc/docker/daemon.json</p>"},{"location":"005-DockerCompose-Basics/","title":"005-DockerCompose-Basics","text":""},{"location":"005-DockerCompose-Basics/#lab-005-docker-compose-wordpress-mariadb","title":"Lab 005 - Docker Compose - WordPress &amp; MariaDB","text":"<ul> <li>This lab demonstrates how to use Docker Compose to orchestrate a simple multi-container application: WordPress with a MariaDB database backend.</li> </ul>"},{"location":"005-DockerCompose-Basics/#overview","title":"Overview","text":"<ul> <li>The provided <code>docker-compose.yaml</code> file defines two main services:<ul> <li>db: Runs a MariaDB database (can be switched to MySQL if desired).</li> <li>wordpress: Runs the latest WordPress application, connected to the database.</li> </ul> </li> <li>A named volume <code>db_data</code> is used to persist database data.</li> </ul>"},{"location":"005-DockerCompose-Basics/#docker-composeyaml-breakdown","title":"docker-compose.yaml Breakdown","text":""},{"location":"005-DockerCompose-Basics/#db-service","title":"db service","text":"<ul> <li>Uses the <code>mariadb:10.6.4-focal</code> image (or optionally MySQL).</li> <li>Sets up environment variables for root password, database, user, and password.</li> <li>Persists data in a Docker volume.</li> <li>Exposes ports 3306 and 33060 (internal only).</li> </ul>"},{"location":"005-DockerCompose-Basics/#wordpress-service","title":"wordpress service","text":"<ul> <li>Uses the latest WordPress image.</li> <li>Maps port 80 on the host to port 80 in the container.</li> <li>Configures environment variables to connect to the database.</li> </ul>"},{"location":"005-DockerCompose-Basics/#volumes","title":"volumes","text":"<ul> <li><code>db_data</code>: Persists MariaDB data between container restarts.</li> </ul>"},{"location":"005-DockerCompose-Basics/#bonus-demo","title":"Bonus Demo","text":"<ul> <li>I prepared a demo of this lab, which you can view on KillerCoda: Portainder Demo.</li> <li>The demo is showcases for setting and running multuple containers using Docker Compose</li> <li>The demo is available on KillerCoda.</li> </ul>"},{"location":"005-DockerCompose-Basics/#how-to-run-the-lab","title":"How to Run the Lab","text":"<ol> <li> <p>Navigate to the lab directory: <pre><code>cd Labs/005-DockerCompose\n</code></pre></p> </li> <li> <p>Start the services: <pre><code>docker compose up -d\n</code></pre></p> </li> <li> <p>This will pull the required images (if not already present) and start both the database and WordPress containers in detached mode.</p> </li> <li> <p>Access WordPress:</p> </li> <li>Open your browser and go to http://localhost</li> <li> <p>Complete the WordPress setup wizard.</p> </li> <li> <p>Stop the services: <pre><code>docker compose down\n</code></pre>    This will stop and remove the containers, but the database data will persist in the <code>db_data</code> volume.</p> </li> </ol>"},{"location":"005-DockerCompose-Basics/#notes","title":"Notes","text":"<ul> <li>To use MySQL instead of MariaDB, uncomment the relevant line in the compose file and comment out the MariaDB image line.</li> <li>The database credentials are set for demonstration purposes. For production, use secure passwords.</li> <li>The <code>db</code> service is only accessible to the <code>wordpress</code> service (not exposed to the host).</li> </ul>"},{"location":"005-DockerCompose-Basics/#advanced-concepts","title":"Advanced Concepts","text":""},{"location":"005-DockerCompose-Basics/#docker-compose-networks","title":"Docker Compose Networks","text":"<p>Docker Compose automatically creates a default network for your services. However, you can define custom networks for better isolation and control:</p> <pre><code>services:\n  wordpress:\n    networks:\n      - frontend\n      - backend\n\n  db:\n    networks:\n      - backend\n\nnetworks:\n  frontend:\n    driver: bridge\n  backend:\n    driver: bridge\n    internal: true  # No external access\n</code></pre> <p>Network Types: - <code>bridge</code>: Default network driver for standalone containers - <code>host</code>: Use the host\u2019s network directly - <code>overlay</code>: For multi-host networking (Docker Swarm) - <code>macvlan</code>: Assign MAC addresses to containers - <code>none</code>: Disable networking</p>"},{"location":"005-DockerCompose-Basics/#volume-management","title":"Volume Management","text":"<p>Named Volumes vs Bind Mounts:</p> <pre><code>services:\n  wordpress:\n    volumes:\n      # Named volume (managed by Docker)\n      - wp_data:/var/www/html\n\n      # Bind mount (host directory)\n      - ./my-theme:/var/www/html/wp-content/themes/my-theme\n\n      # Anonymous volume\n      - /var/www/html/tmp\n\nvolumes:\n  wp_data:\n    driver: local\n    driver_opts:\n      type: none\n      o: bind\n      device: /path/on/host\n</code></pre> <p>Volume Best Practices: - Use named volumes for data persistence - Use bind mounts for development (live code updates) - Use anonymous volumes for temporary data - Always backup volumes before major updates</p>"},{"location":"005-DockerCompose-Basics/#health-checks","title":"Health Checks","text":"<p>Add health checks to ensure services are running correctly:</p> <pre><code>services:\n  wordpress:\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 40s\n\n  db:\n    healthcheck:\n      test: [\"CMD\", \"mysqladmin\", \"ping\", \"-h\", \"localhost\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n</code></pre>"},{"location":"005-DockerCompose-Basics/#resource-limits","title":"Resource Limits","text":"<p>Control resource allocation to prevent any service from consuming all resources:</p> <pre><code>services:\n  wordpress:\n    deploy:\n      resources:\n        limits:\n          cpus: '0.50'\n          memory: 512M\n        reservations:\n          cpus: '0.25'\n          memory: 256M\n</code></pre>"},{"location":"005-DockerCompose-Basics/#restart-policies","title":"Restart Policies","text":"<p>Configure how containers should restart:</p> <pre><code>services:\n  wordpress:\n    restart: unless-stopped  # Options: no, always, on-failure, unless-stopped\n</code></pre>"},{"location":"005-DockerCompose-Basics/#dependency-management","title":"Dependency Management","text":"<p>Control startup order with <code>depends_on</code>:</p> <pre><code>services:\n  wordpress:\n    depends_on:\n      db:\n        condition: service_healthy  # Wait for db to be healthy\n</code></pre>"},{"location":"005-DockerCompose-Basics/#logging-configuration","title":"Logging Configuration","text":"<p>Configure log drivers and options:</p> <pre><code>services:\n  wordpress:\n    logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"10m\"\n        max-file: \"3\"\n</code></pre>"},{"location":"005-DockerCompose-Basics/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use Environment Variables: Store configuration in <code>.env</code> files    <pre><code># .env file\nMYSQL_ROOT_PASSWORD=secure_password\nMYSQL_DATABASE=wordpress\n</code></pre></p> </li> <li> <p>Version Your Compose Files: Always specify the Compose version    <pre><code>version: '3.8'\n</code></pre></p> </li> <li> <p>Use Specific Image Tags: Avoid <code>latest</code> tag in production    <pre><code>image: mariadb:10.6.4-focal\n</code></pre></p> </li> <li> <p>Organize Services Logically: Group related services</p> </li> <li>Frontend services</li> <li>Backend services</li> <li>Database services</li> <li> <p>Cache services</p> </li> <li> <p>Use Multi-Stage Builds: For custom images, use multi-stage Dockerfiles</p> </li> <li> <p>Implement Health Checks: Always add health checks for critical services</p> </li> <li> <p>Secure Secrets: Use Docker secrets or external secret management    <pre><code>services:\n  db:\n    secrets:\n      - db_password\n\nsecrets:\n  db_password:\n    file: ./secrets/db_password.txt\n</code></pre></p> </li> <li> <p>Use .dockerignore: Exclude unnecessary files from build context</p> </li> <li> <p>Network Isolation: Use custom networks to isolate services</p> </li> <li> <p>Monitor Resources: Set resource limits to prevent resource exhaustion</p> </li> </ol>"},{"location":"005-DockerCompose-Basics/#useful-docker-compose-commands","title":"Useful Docker Compose Commands","text":"<pre><code># View configuration (merged from all compose files)\ndocker compose config\n\n# Validate compose file\ndocker compose config --quiet\n\n# Pull all images\ndocker compose pull\n\n# Build services\ndocker compose build\n\n# Start services\ndocker compose up -d\n\n# View running services\ndocker compose ps\n\n# View logs\ndocker compose logs -f [service_name]\n\n# Execute command in running container\ndocker compose exec wordpress bash\n\n# Scale services\ndocker compose up -d --scale wordpress=3\n\n# Stop services\ndocker compose stop\n\n# Stop and remove containers\ndocker compose down\n\n# Stop, remove containers and volumes\ndocker compose down -v\n\n# Restart services\ndocker compose restart\n\n# Pause services\ndocker compose pause\n\n# Unpause services\ndocker compose unpause\n\n# View resource usage\ndocker compose top\n</code></pre>"},{"location":"005-DockerCompose-Basics/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If you encounter port conflicts, ensure nothing else is running on port 80.</li> <li>To view logs for a service:   <pre><code>docker compose logs wordpress\ndocker compose logs db\n</code></pre></li> <li>Check service health:   <pre><code>docker compose ps\n</code></pre></li> <li>Inspect network connectivity:   <pre><code>docker network inspect 005-dockercompose_default\n</code></pre></li> <li>Debug startup issues:   <pre><code>docker compose up --no-start\ndocker compose start\n</code></pre></li> </ul> <p>This lab is part of the DockerLabs series. See other labs for more Docker scenarios and hands-on exercises.</p>"},{"location":"006-DockerCompose-env/","title":"006-DockerCompose-env","text":""},{"location":"006-DockerCompose-env/#lab-006-multi-environment-docker-compose-setup","title":"Lab 006 - Multi-Environment Docker Compose Setup","text":"<ul> <li>A comprehensive example of structuring Docker Compose files for multiple environments</li> <li>Demonstrates environment-specific overrides and configuration management</li> <li>Each environment is fully isolated with its own configuration and services</li> </ul>"},{"location":"006-DockerCompose-env/#pre-requirements","title":"Pre-Requirements","text":"<ul> <li>Docker installed</li> <li>Docker Compose knowledge</li> <li>Basic understanding of environment variables</li> </ul> <ul> <li>Overview</li> <li>Project Structure</li> <li>Step 01 - Understanding the Base Configuration</li> <li>Step 02 - Development Environment</li> <li>Step 03 - Production Environment</li> <li>Step 04 - Environment Variables</li> <li>Shared Variables (<code>.env</code>)</li> <li>Development Variables (<code>.env.dev</code>)</li> <li>Production Variables (<code>.env.prod</code>)</li> <li>Step 05 - Quick Start with Scripts</li> <li>Using the run.sh Script</li> <li>Step 06 - Manual Commands</li> <li>Start Specific Environment</li> <li>Stop Services</li> <li>View Logs</li> <li>Scale Services (Production)</li> <li>Step 07 - Testing the Setup</li> <li>Interactive Demo</li> <li>Manual Testing</li> <li>Step 08 - Clean Up</li> <li>Best Practices</li> <li>Troubleshooting</li> <li>Common Issues</li> <li>Debugging Commands</li> <li>Environment-Specific Notes</li> </ul>"},{"location":"006-DockerCompose-env/#overview","title":"Overview","text":"<p>This lab demonstrates how to structure Docker Compose files for multiple environments using:</p> <ul> <li>Base Configuration: Common services shared across environments</li> <li>Environment Overrides: Specific configurations for development and production</li> <li>Environment Variables: Centralized configuration management</li> <li>Utility Scripts: Easy environment management</li> </ul>"},{"location":"006-DockerCompose-env/#project-structure","title":"Project Structure","text":"<pre><code>Labs/006-DockerCompose-env/\n\u251c\u2500\u2500 docker-compose.yml          # Base services configuration\n\u251c\u2500\u2500 docker-compose.dev.yml      # Development overrides\n\u251c\u2500\u2500 docker-compose.prod.yml     # Production overrides\n\u251c\u2500\u2500 .env                        # Shared environment variables\n\u251c\u2500\u2500 .env.dev                    # Development-specific variables\n\u251c\u2500\u2500 .env.prod                   # Production-specific variables\n\u251c\u2500\u2500 README.md                   # This documentation\n\u251c\u2500\u2500 run.sh                      # Bash script for environment management\n\u251c\u2500\u2500 demo.sh                     # Interactive demonstration\n\u251c\u2500\u2500 init.sql                    # Database initialization\n\u251c\u2500\u2500 html/\n\u2502   \u2514\u2500\u2500 index.html             # Sample web application\n\u2514\u2500\u2500 api/\n    \u251c\u2500\u2500 package.json           # Node.js API dependencies\n    \u2514\u2500\u2500 server.js              # Sample API server\n</code></pre>"},{"location":"006-DockerCompose-env/#step-01-understanding-the-base-configuration","title":"Step 01 - Understanding the Base Configuration","text":"<p>The <code>docker-compose.yml</code> file contains the core service definitions that are common across all environments:</p> <ul> <li>Web Service: Nginx web server</li> <li>API Service: Node.js backend application  </li> <li>Database Service: PostgreSQL database</li> <li>Cache Service: Redis caching layer</li> </ul> <p>All services use environment variables with default values using the <code>${VARIABLE:-default}</code> syntax for flexibility.</p>"},{"location":"006-DockerCompose-env/#step-02-development-environment","title":"Step 02 - Development Environment","text":"<p>The development environment (<code>docker-compose.dev.yml</code>) provides developer-friendly features:</p> <pre><code># Start development environment\ndocker-compose --env-file .env.dev -f docker-compose.yml -f docker-compose.dev.yml up -d\n</code></pre> <p>Development Features:</p> <ul> <li>Hot reload enabled for API service (nodemon)</li> <li>Debug ports exposed (9229 for Node.js debugging)</li> <li>Additional development tools:</li> <li>Adminer for database management</li> <li>MailCatcher for email testing</li> <li>Read-write volumes for live code editing</li> <li>Detailed logging enabled</li> <li>Non-standard ports to avoid conflicts (8000, 3001, 5433, 6380)</li> </ul> <p>Development Services Access:</p> <ul> <li>Web Application: <code>http://localhost:8000</code></li> <li>API: <code>http://localhost:3001</code></li> <li>Database Admin (Adminer): <code>http://localhost:8080</code></li> <li>Mail Catcher: <code>http://localhost:1080</code></li> </ul>"},{"location":"006-DockerCompose-env/#step-03-production-environment","title":"Step 03 - Production Environment","text":"<p>The production environment (<code>docker-compose.prod.yml</code>) focuses on performance and security:</p> <pre><code># Start production environment\ndocker-compose --env-file .env.prod -f docker-compose.yml -f docker-compose.prod.yml up -d\n</code></pre> <p>Production Features:</p> <ul> <li>Multiple service replicas for high availability</li> <li>Read-only volumes for security</li> <li>Optimized restart policies</li> <li>Structured logging with rotation</li> <li>Monitoring with Prometheus</li> <li>Standard service ports (80, 3000, 5432, 6379)</li> </ul> <p>Production Services Access:</p> <ul> <li>Web Application: <code>http://localhost:80</code></li> <li>API: <code>http://localhost:3000</code></li> <li>Monitoring (Prometheus): <code>http://localhost:9090</code></li> </ul>"},{"location":"006-DockerCompose-env/#step-04-environment-variables","title":"Step 04 - Environment Variables","text":""},{"location":"006-DockerCompose-env/#shared-variables-env","title":"Shared Variables (<code>.env</code>)","text":"<p>Common configuration across all environments:</p> Variable Description <code>APP_NAME</code> Application name for container naming <code>ENVIRONMENT</code> Current environment identifier <code>DB_NAME</code> Database name <code>DB_USER</code> Database user <code>API_SECRET</code> API authentication secret <code>LOG_LEVEL</code> Logging verbosity"},{"location":"006-DockerCompose-env/#development-variables-envdev","title":"Development Variables (<code>.env.dev</code>)","text":"Variable/Setting Description Non-standard ports Avoid conflicts with other services Debug-friendly config Enables debug mode and verbose logging Dev DB credentials Uses development database credentials Enhanced logging More detailed logs for debugging"},{"location":"006-DockerCompose-env/#production-variables-envprod","title":"Production Variables (<code>.env.prod</code>)","text":"Variable/Setting Description Standard service ports Uses standard ports for production Strong, secure passwords Enforces strong credentials Optimized timeouts Sets timeouts suitable for production Security-focused config Enables production security best practices"},{"location":"006-DockerCompose-env/#step-05-quick-start-with-scripts","title":"Step 05 - Quick Start with Scripts","text":""},{"location":"006-DockerCompose-env/#using-the-runsh-script","title":"Using the run.sh Script","text":"<pre><code># Development environment\n./run.sh dev up        # Start development\n./run.sh dev down      # Stop development\n./run.sh dev logs      # View development logs\n./run.sh dev ps        # Show service status\n\n# Production environment\n./run.sh prod up       # Start production\n./run.sh prod down     # Stop production\n./run.sh prod logs     # View production logs\n\n# Help\n./run.sh help          # Show usage information\n</code></pre>"},{"location":"006-DockerCompose-env/#step-06-manual-commands","title":"Step 06 - Manual Commands","text":""},{"location":"006-DockerCompose-env/#start-specific-environment","title":"Start Specific Environment","text":"<pre><code># Development\ndocker-compose --env-file .env.dev -f docker-compose.yml -f docker-compose.dev.yml up -d\n\n# Production\ndocker-compose --env-file .env.prod -f docker-compose.yml -f docker-compose.prod.yml up -d\n</code></pre>"},{"location":"006-DockerCompose-env/#stop-services","title":"Stop Services","text":"<pre><code># Development\ndocker-compose --env-file .env.dev -f docker-compose.yml -f docker-compose.dev.yml down\n\n# Production  \ndocker-compose --env-file .env.prod -f docker-compose.yml -f docker-compose.prod.yml down\n</code></pre>"},{"location":"006-DockerCompose-env/#view-logs","title":"View Logs","text":"<pre><code># All services logs\ndocker-compose --env-file .env.dev -f docker-compose.yml -f docker-compose.dev.yml logs -f\n\n# Specific service logs\ndocker-compose --env-file .env.dev -f docker-compose.yml -f docker-compose.dev.yml logs -f api\n</code></pre>"},{"location":"006-DockerCompose-env/#scale-services-production","title":"Scale Services (Production)","text":"<pre><code># Scale API service to 5 replicas\ndocker-compose --env-file .env.prod -f docker-compose.yml -f docker-compose.prod.yml up -d --scale api=5\n</code></pre>"},{"location":"006-DockerCompose-env/#step-07-testing-the-setup","title":"Step 07 - Testing the Setup","text":""},{"location":"006-DockerCompose-env/#interactive-demo","title":"Interactive Demo","text":"<p>Run the complete demonstration:</p> <pre><code># Run the interactive demo\n./demo.sh\n</code></pre> <p>The demo will:</p> <ol> <li>Start development environment</li> <li>Test the application</li> <li>Switch to production environment  </li> <li>Show differences between environments</li> <li>Clean up</li> </ol>"},{"location":"006-DockerCompose-env/#manual-testing","title":"Manual Testing","text":"<pre><code># Start development environment\n./run.sh dev up\n\n# Test the API\ncurl -s http://localhost:3001/health | python3 -m json.tool\n\n# Test the web application\ncurl -s http://localhost:8000\n\n# Check service status\n./run.sh dev ps\n</code></pre>"},{"location":"006-DockerCompose-env/#step-08-clean-up","title":"Step 08 - Clean Up","text":"<pre><code># Stop current environment\n./run.sh dev down     # or ./run.sh prod down\n\n# Complete cleanup (removes volumes)\n./run.sh dev down &amp;&amp; ./run.sh prod down\ndocker system prune -f\n\n# Or manually\ndocker-compose --env-file .env.dev -f docker-compose.yml -f docker-compose.dev.yml down -v\ndocker-compose --env-file .env.prod -f docker-compose.yml -f docker-compose.prod.yml down -v\n</code></pre>"},{"location":"006-DockerCompose-env/#best-practices","title":"Best Practices","text":"<ol> <li>Environment Separation: Clear separation between dev, staging, and production configurations</li> <li>Security: Different secrets and passwords per environment</li> <li>Scalability: Production setup with multiple replicas and monitoring</li> <li>Development Experience: Hot reload, debugging ports, and development tools</li> <li>Configuration Management: Centralized environment variable management</li> <li>Volume Management: Read-only volumes in production, read-write in development</li> <li>Logging: Environment-appropriate logging levels and rotation</li> <li>Networking: Consistent network setup across environments</li> </ol>"},{"location":"006-DockerCompose-env/#advanced-docker-compose-techniques","title":"Advanced Docker Compose Techniques","text":""},{"location":"006-DockerCompose-env/#yaml-anchors-and-aliases","title":"YAML Anchors and Aliases","text":"<p>Docker Compose supports YAML anchors (<code>&amp;</code>) and aliases (<code>*</code>) to reduce duplication in your compose files. This is particularly useful when multiple services share common configuration.</p> <p>Basic Anchors Example:</p> <pre><code># Define reusable configuration blocks\nx-logging: &amp;default-logging\n  driver: json-file\n  options:\n    max-size: \"10m\"\n    max-file: \"3\"\n\nx-healthcheck: &amp;default-healthcheck\n  interval: 30s\n  timeout: 10s\n  retries: 3\n  start_period: 40s\n\nservices:\n  web:\n    image: nginx:alpine\n    logging: *default-logging\n    healthcheck:\n      &lt;&lt;: *default-healthcheck\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost\"]\n\n  api:\n    image: node:18-alpine\n    logging: *default-logging\n    healthcheck:\n      &lt;&lt;: *default-healthcheck\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:3000/health\"]\n</code></pre>"},{"location":"006-DockerCompose-env/#extension-fields-x-","title":"Extension Fields (x-*)","text":"<p>Extension fields are special YAML keys that start with <code>x-</code> and are ignored by Docker Compose. They\u2019re perfect for defining reusable configuration fragments.</p> <p>Common Configuration Patterns:</p> <pre><code># Define common configurations as extension fields\nx-common-variables: &amp;common-vars\n  TZ: UTC\n  LOG_LEVEL: info\n\nx-restart-policy: &amp;restart-policy\n  restart: unless-stopped\n\nx-resource-limits: &amp;resource-limits\n  deploy:\n    resources:\n      limits:\n        cpus: '0.50'\n        memory: 512M\n      reservations:\n        cpus: '0.25'\n        memory: 256M\n\nservices:\n  service1:\n    &lt;&lt;: *restart-policy\n    &lt;&lt;: *resource-limits\n    environment:\n      &lt;&lt;: *common-vars\n      SERVICE_NAME: service1\n\n  service2:\n    &lt;&lt;: *restart-policy\n    &lt;&lt;: *resource-limits\n    environment:\n      &lt;&lt;: *common-vars\n      SERVICE_NAME: service2\n</code></pre>"},{"location":"006-DockerCompose-env/#merge-keys","title":"Merge Keys (&lt;&lt;:)","text":"<p>The merge key <code>&lt;&lt;:</code> allows you to merge one or more mappings into the current mapping. You can merge multiple anchors:</p> <pre><code>x-base-service: &amp;base-service\n  restart: unless-stopped\n  networks:\n    - app-network\n\nx-logging-config: &amp;logging-config\n  logging:\n    driver: json-file\n    options:\n      max-size: \"10m\"\n\nx-health-config: &amp;health-config\n  healthcheck:\n    interval: 30s\n    timeout: 10s\n    retries: 3\n\nservices:\n  web:\n    &lt;&lt;: [*base-service, *logging-config, *health-config]\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n\n  api:\n    &lt;&lt;: [*base-service, *logging-config, *health-config]\n    image: node:18-alpine\n    ports:\n      - \"3000:3000\"\n</code></pre>"},{"location":"006-DockerCompose-env/#complex-fragment-patterns","title":"Complex Fragment Patterns","text":"<p>Service Templates:</p> <pre><code># Define a complete service template\nx-app-template: &amp;app-template\n  restart: unless-stopped\n  networks:\n    - backend\n  logging:\n    driver: json-file\n    options:\n      max-size: \"10m\"\n      max-file: \"3\"\n  deploy:\n    resources:\n      limits:\n        cpus: '1.0'\n        memory: 1G\n\n# Define environment-specific configurations\nx-dev-config: &amp;dev-config\n  build:\n    context: .\n    target: development\n  volumes:\n    - ./src:/app/src\n  environment:\n    NODE_ENV: development\n\nx-prod-config: &amp;prod-config\n  image: myapp:latest\n  read_only: true\n  environment:\n    NODE_ENV: production\n\nservices:\n  # Development service\n  app-dev:\n    &lt;&lt;: [*app-template, *dev-config]\n    ports:\n      - \"3001:3000\"\n\n  # Production service\n  app-prod:\n    &lt;&lt;: [*app-template, *prod-config]\n    ports:\n      - \"3000:3000\"\n</code></pre>"},{"location":"006-DockerCompose-env/#combining-anchors-with-override","title":"Combining Anchors with Override","text":"<p>You can override specific values from anchors:</p> <pre><code>x-database: &amp;database-config\n  image: postgres:15-alpine\n  restart: unless-stopped\n  networks:\n    - db-network\n  healthcheck:\n    test: [\"CMD-SHELL\", \"pg_isready\"]\n    interval: 10s\n    timeout: 5s\n    retries: 5\n\nservices:\n  main-db:\n    &lt;&lt;: *database-config\n    container_name: main-database\n    environment:\n      POSTGRES_DB: maindb\n    volumes:\n      - main-db-data:/var/lib/postgresql/data\n\n  test-db:\n    &lt;&lt;: *database-config\n    container_name: test-database\n    environment:\n      POSTGRES_DB: testdb\n    volumes:\n      - test-db-data:/var/lib/postgresql/data\n    # Override the restart policy for test db\n    restart: \"no\"\n</code></pre>"},{"location":"006-DockerCompose-env/#real-world-example-microservices","title":"Real-World Example: Microservices","text":"<pre><code>version: '3.8'\n\n# Common configurations\nx-service-defaults: &amp;service-defaults\n  restart: unless-stopped\n  networks:\n    - app-network\n  logging: &amp;logging-config\n    driver: json-file\n    options:\n      max-size: \"10m\"\n      max-file: \"3\"\n\nx-node-service: &amp;node-service\n  &lt;&lt;: *service-defaults\n  image: node:18-alpine\n  healthcheck:\n    interval: 30s\n    timeout: 10s\n    retries: 3\n\nx-environment-common: &amp;env-common\n  NODE_ENV: ${NODE_ENV:-production}\n  LOG_LEVEL: ${LOG_LEVEL:-info}\n  DATABASE_URL: postgresql://db:5432/${DB_NAME}\n\nservices:\n  user-service:\n    &lt;&lt;: *node-service\n    container_name: user-service\n    environment:\n      &lt;&lt;: *env-common\n      SERVICE_NAME: user-service\n      PORT: 3001\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"-q\", \"-O\", \"-\", \"http://localhost:3001/health\"]\n    ports:\n      - \"3001:3001\"\n\n  order-service:\n    &lt;&lt;: *node-service\n    container_name: order-service\n    environment:\n      &lt;&lt;: *env-common\n      SERVICE_NAME: order-service\n      PORT: 3002\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"-q\", \"-O\", \"-\", \"http://localhost:3002/health\"]\n    ports:\n      - \"3002:3002\"\n\n  payment-service:\n    &lt;&lt;: *node-service\n    container_name: payment-service\n    environment:\n      &lt;&lt;: *env-common\n      SERVICE_NAME: payment-service\n      PORT: 3003\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"-q\", \"-O\", \"-\", \"http://localhost:3003/health\"]\n    ports:\n      - \"3003:3003\"\n\nnetworks:\n  app-network:\n    driver: bridge\n\nvolumes:\n  db-data:\n</code></pre>"},{"location":"006-DockerCompose-env/#benefits-of-using-fragments","title":"Benefits of Using Fragments","text":"<ol> <li>DRY Principle: Don\u2019t Repeat Yourself - define common configuration once</li> <li>Consistency: Ensure all services use the same base configuration</li> <li>Maintainability: Update configuration in one place</li> <li>Readability: Cleaner, more organized compose files</li> <li>Scalability: Easy to add new services with standard configuration</li> </ol>"},{"location":"006-DockerCompose-env/#tips-for-using-fragments","title":"Tips for Using Fragments","text":"<ul> <li>Use meaningful names for your anchors (e.g., <code>&amp;common-logging</code>, <code>&amp;base-service</code>)</li> <li>Group related configuration into logical fragments</li> <li>Place extension fields at the top of your compose file</li> <li>Document what each fragment contains</li> <li>Test your merged configuration with <code>docker compose config</code></li> <li>Use fragments for environment-specific configurations</li> <li>Combine fragments with environment variables for maximum flexibility</li> </ul>"},{"location":"006-DockerCompose-env/#troubleshooting","title":"Troubleshooting","text":""},{"location":"006-DockerCompose-env/#common-issues","title":"Common Issues","text":"<ul> <li>Port conflicts: Ensure no other services are using the same ports</li> <li>Environment variables: Verify all required variables are set in <code>.env</code> files</li> <li>Docker daemon: Ensure Docker is running and accessible</li> </ul>"},{"location":"006-DockerCompose-env/#debugging-commands","title":"Debugging Commands","text":"<pre><code># Check service logs\ndocker-compose --env-file .env.dev -f docker-compose.yml -f docker-compose.dev.yml logs service_name\n\n# Check service status\ndocker-compose --env-file .env.dev -f docker-compose.yml -f docker-compose.dev.yml ps\n\n# Inspect container\ndocker inspect container_name\n\n# Execute command in container\ndocker-compose --env-file .env.dev -f docker-compose.yml -f docker-compose.dev.yml exec service_name bash\n</code></pre>"},{"location":"006-DockerCompose-env/#environment-specific-notes","title":"Environment-Specific Notes","text":"Environment/Aspect Notes Development Services may take longer to start due to volume mounts Production Services use restart policies and may auto-restart on failure Networking All services communicate through Docker networks"},{"location":"007-DockerCompose-fragments/","title":"007-DockerCompose-fragments","text":""},{"location":"007-DockerCompose-fragments/#lab-007-advanced-docker-compose-fragments-includes-extends","title":"Lab 007 - Advanced Docker Compose - Fragments, Includes &amp; Extends","text":"<p>This lab covers advanced Docker Compose techniques including YAML fragments, composition patterns, includes, and the extends keyword. Learn how to build maintainable, reusable, and DRY (Don\u2019t Repeat Yourself) Docker Compose configurations.</p>"},{"location":"007-DockerCompose-fragments/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Table of Contents</li> <li>Overview</li> <li>Prerequisites</li> <li>YAML Anchors and Aliases</li> <li>Basic Anchors</li> <li>Extension Fields</li> <li>Merge Keys</li> <li>Docker Compose Include</li> <li>Basic Include Syntax</li> <li>Include with Path</li> <li>Include Best Practices</li> <li>Docker Compose Extends (Legacy)</li> <li>Extends Syntax</li> <li>When to Use Extends</li> <li>Real-World Examples</li> <li>Example 1: Microservices Architecture</li> <li>Example 2: Multi-Environment Setup</li> <li>Example 3: Modular Configuration</li> <li>Best Practices</li> <li>Common Patterns</li> <li>Pattern 1: Base Service Template</li> <li>Pattern 2: Environment Overrides</li> <li>Pattern 3: Multi-Container Application</li> <li>Troubleshooting</li> <li>Common Issues and Solutions</li> <li>Debugging Commands</li> <li>Hands-On Exercises</li> <li>Exercise 1: Create a Microservices Setup</li> <li>Exercise 2: Multi-Environment Configuration</li> <li>Exercise 3: Modular Infrastructure</li> <li>Useful Commands</li> </ul>"},{"location":"007-DockerCompose-fragments/#overview","title":"Overview","text":"<p>As Docker Compose configurations grow, managing multiple services with similar configurations becomes challenging. This lab teaches you advanced composition techniques to:</p> <ul> <li>Reduce duplication using YAML anchors and fragments</li> <li>Modularize configurations with includes</li> <li>Share common settings across services</li> <li>Manage multi-environment setups efficiently</li> <li>Create reusable templates for services</li> </ul>"},{"location":"007-DockerCompose-fragments/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed (version 20.10+)</li> <li>Docker Compose V2 (docker compose, not docker-compose)</li> <li>Basic understanding of YAML syntax</li> <li>Familiarity with basic Docker Compose concepts</li> </ul> <p>Verify your setup:</p> <pre><code>docker compose version\n# Should show: Docker Compose version v2.x.x or higher\n</code></pre>"},{"location":"007-DockerCompose-fragments/#yaml-anchors-and-aliases","title":"YAML Anchors and Aliases","text":"<p>YAML anchors (<code>&amp;</code>) and aliases (<code>*</code>) allow you to define reusable configuration blocks within a single YAML file.</p>"},{"location":"007-DockerCompose-fragments/#basic-anchors","title":"Basic Anchors","text":"<p>Syntax:</p> <ul> <li><code>&amp;anchor-name</code> - Define an anchor</li> <li><code>*anchor-name</code> - Reference an anchor</li> <li><code>&lt;&lt;: *anchor-name</code> - Merge an anchor</li> </ul> <p>Simple Example:</p> <pre><code>version: '3.8'\n\n# Define a logging configuration anchor\nx-logging: &amp;default-logging\n  driver: json-file\n  options:\n    max-size: \"10m\"\n    max-file: \"3\"\n\nservices:\n  web:\n    image: nginx:alpine\n    logging: *default-logging\n\n  api:\n    image: node:18-alpine\n    logging: *default-logging\n\n  worker:\n    image: python:3.11-alpine\n    logging: *default-logging\n</code></pre>"},{"location":"007-DockerCompose-fragments/#extension-fields","title":"Extension Fields","text":"<p>Extension fields start with <code>x-</code> and are ignored by Docker Compose but can be used as anchors. This keeps your configuration organized.</p> <p>Complete Service Template:</p> <pre><code>version: '3.8'\n\n# Extension fields - ignored by Docker Compose\nx-common-variables: &amp;common-vars\n  TZ: UTC\n  LOG_LEVEL: ${LOG_LEVEL:-info}\n  ENVIRONMENT: ${ENVIRONMENT:-production}\n\nx-restart-policy: &amp;restart-policy\n  restart: unless-stopped\n\nx-healthcheck-defaults: &amp;healthcheck-defaults\n  interval: 30s\n  timeout: 10s\n  retries: 3\n  start_period: 40s\n\nx-resource-limits: &amp;resource-limits\n  deploy:\n    resources:\n      limits:\n        cpus: '1.0'\n        memory: 1G\n      reservations:\n        cpus: '0.5'\n        memory: 512M\n\nx-logging-config: &amp;logging-config\n  logging:\n    driver: json-file\n    options:\n      max-size: \"10m\"\n      max-file: \"3\"\n      labels: \"service\"\n\n# Actual services using the fragments\nservices:\n  web:\n    image: nginx:alpine\n    &lt;&lt;: [*restart-policy, *logging-config, *resource-limits]\n    environment:\n      &lt;&lt;: *common-vars\n      SERVICE_NAME: web\n    healthcheck:\n      &lt;&lt;: *healthcheck-defaults\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost\"]\n    ports:\n      - \"80:80\"\n    networks:\n      - frontend\n\n  api:\n    image: node:18-alpine\n    &lt;&lt;: [*restart-policy, *logging-config, *resource-limits]\n    environment:\n      &lt;&lt;: *common-vars\n      SERVICE_NAME: api\n    healthcheck:\n      &lt;&lt;: *healthcheck-defaults\n      test: [\"CMD\", \"wget\", \"-q\", \"-O\", \"-\", \"http://localhost:3000/health\"]\n    ports:\n      - \"3000:3000\"\n    networks:\n      - frontend\n      - backend\n\nnetworks:\n  frontend:\n  backend:\n</code></pre>"},{"location":"007-DockerCompose-fragments/#merge-keys","title":"Merge Keys","text":"<p>The merge key (<code>&lt;&lt;:</code>) allows combining multiple anchors:</p> <p>Multiple Anchor Merging:</p> <pre><code>version: '3.8'\n\n# Define separate configuration aspects\nx-base-config: &amp;base-config\n  restart: unless-stopped\n  networks:\n    - app-network\n\nx-monitoring: &amp;monitoring\n  labels:\n    - \"prometheus.scrape=true\"\n    - \"prometheus.port=9090\"\n\nx-security: &amp;security\n  security_opt:\n    - no-new-privileges:true\n  read_only: true\n\nx-node-service: &amp;node-service\n  image: node:18-alpine\n  healthcheck:\n    test: [\"CMD\", \"node\", \"--version\"]\n    interval: 30s\n\nservices:\n  # Merge multiple fragments\n  user-service:\n    &lt;&lt;: [*base-config, *monitoring, *security, *node-service]\n    container_name: user-service\n    environment:\n      SERVICE: users\n    ports:\n      - \"3001:3000\"\n\n  order-service:\n    &lt;&lt;: [*base-config, *monitoring, *security, *node-service]\n    container_name: order-service\n    environment:\n      SERVICE: orders\n    ports:\n      - \"3002:3000\"\n\nnetworks:\n  app-network:\n    driver: bridge\n</code></pre>"},{"location":"007-DockerCompose-fragments/#docker-compose-include","title":"Docker Compose Include","text":"<p>The <code>include</code> directive (Compose V2.20+) allows you to split your configuration across multiple files and combine them at runtime.</p>"},{"location":"007-DockerCompose-fragments/#basic-include-syntax","title":"Basic Include Syntax","text":"<p>Main compose file (<code>docker-compose.yml</code>):</p> <pre><code>include:\n  - ./compose-services.yml\n  - ./compose-networks.yml\n  - ./compose-volumes.yml\n\n# You can still define additional services here\nservices:\n  gateway:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n</code></pre> <p>Separate service file (<code>compose-services.yml</code>):</p> <pre><code>services:\n  api:\n    image: node:18-alpine\n    ports:\n      - \"3000:3000\"\n\n  worker:\n    image: python:3.11-alpine\n</code></pre>"},{"location":"007-DockerCompose-fragments/#include-with-path","title":"Include with Path","text":"<p>You can organize includes in subdirectories:</p> <p>Project Structure:</p> <pre><code>project/\n\u251c\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 compose/\n\u2502   \u251c\u2500\u2500 databases.yml\n\u2502   \u251c\u2500\u2500 services.yml\n\u2502   \u251c\u2500\u2500 monitoring.yml\n\u2502   \u2514\u2500\u2500 dev/\n\u2502       \u251c\u2500\u2500 overrides.yml\n\u2502       \u2514\u2500\u2500 debug.yml\n\u2514\u2500\u2500 .env\n</code></pre> <p>docker-compose.yml:</p> <pre><code>include:\n  - path: ./compose/databases.yml\n  - path: ./compose/services.yml\n  - path: ./compose/monitoring.yml\n  # Conditional includes based on environment\n  - path: ./compose/dev/overrides.yml\n    env_file: .env.dev\n</code></pre>"},{"location":"007-DockerCompose-fragments/#include-best-practices","title":"Include Best Practices","text":"<ol> <li>Logical Separation:</li> </ol> <pre><code># docker-compose.yml - Main orchestration\ninclude:\n  - ./infrastructure/databases.yml      # All database services\n  - ./infrastructure/cache.yml          # Redis, Memcached, etc.\n  - ./infrastructure/queues.yml         # RabbitMQ, Kafka, etc.\n  - ./application/backend-services.yml  # Backend microservices\n  - ./application/frontend-services.yml # Frontend services\n  - ./monitoring/observability.yml      # Prometheus, Grafana, etc.\n</code></pre> <ol> <li>Environment-Specific Includes:</li> </ol> <pre><code># docker-compose.yml\ninclude:\n  - ./base/services.yml\n  - path: ./envs/${ENVIRONMENT:-dev}.yml\n</code></pre> <ol> <li>Shared Fragments Across Includes:</li> </ol> <pre><code># shared/fragments.yml\nx-logging: &amp;default-logging\n  driver: json-file\n  options:\n    max-size: \"10m\"\n\n# services/api.yml\ninclude:\n  - path: ../shared/fragments.yml\n\nservices:\n  api:\n    image: node:18-alpine\n    logging: *default-logging\n</code></pre>"},{"location":"007-DockerCompose-fragments/#docker-compose-extends-legacy","title":"Docker Compose Extends (Legacy)","text":"<p>Note: The <code>extends</code> keyword is considered legacy. Modern Docker Compose recommends using <code>include</code> and YAML anchors instead. However, it\u2019s still supported for backward compatibility.</p>"},{"location":"007-DockerCompose-fragments/#extends-syntax","title":"Extends Syntax","text":"<p>Base service file (<code>common.yml</code>):</p> <pre><code>services:\n  base-service:\n    image: node:18-alpine\n    restart: unless-stopped\n    logging:\n      driver: json-file\n      options:\n        max-size: \"10m\"\n</code></pre> <p>Main compose file:</p> <pre><code>services:\n  api:\n    extends:\n      file: common.yml\n      service: base-service\n    container_name: api-service\n    ports:\n      - \"3000:3000\"\n    environment:\n      SERVICE_NAME: api\n</code></pre>"},{"location":"007-DockerCompose-fragments/#when-to-use-extends","title":"When to Use Extends","text":"<p>Use <code>extends</code> when:</p> <ul> <li>Working with legacy Compose files</li> <li>Sharing configuration between different Compose files</li> <li>Need to override specific service configurations</li> </ul> <p>Prefer <code>include</code> and YAML anchors for new projects as they provide:</p> <ul> <li>Better performance</li> <li>Clearer composition</li> <li>More flexibility</li> <li>Better tooling support</li> </ul>"},{"location":"007-DockerCompose-fragments/#real-world-examples","title":"Real-World Examples","text":""},{"location":"007-DockerCompose-fragments/#example-1-microservices-architecture","title":"Example 1: Microservices Architecture","text":"<p>File Structure:</p> <pre><code>microservices/\n\u251c\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 fragments/\n\u2502   \u2514\u2500\u2500 common.yml\n\u251c\u2500\u2500 infrastructure/\n\u2502   \u251c\u2500\u2500 databases.yml\n\u2502   \u251c\u2500\u2500 cache.yml\n\u2502   \u2514\u2500\u2500 messaging.yml\n\u2514\u2500\u2500 services/\n    \u251c\u2500\u2500 user-service.yml\n    \u251c\u2500\u2500 order-service.yml\n    \u2514\u2500\u2500 payment-service.yml\n</code></pre> <p>fragments/common.yml:</p> <pre><code># Common configurations as extension fields\nx-service-defaults: &amp;service-defaults\n  restart: unless-stopped\n  networks:\n    - microservices\n  logging: &amp;logging\n    driver: json-file\n    options:\n      max-size: \"10m\"\n      max-file: \"3\"\n\nx-healthcheck: &amp;healthcheck\n  interval: 30s\n  timeout: 10s\n  retries: 3\n  start_period: 40s\n\nx-node-service: &amp;node-service\n  &lt;&lt;: *service-defaults\n  image: node:18-alpine\n  healthcheck:\n    &lt;&lt;: *healthcheck\n\nx-environment-common: &amp;env-common\n  NODE_ENV: ${NODE_ENV:-production}\n  LOG_LEVEL: ${LOG_LEVEL:-info}\n  REDIS_URL: redis://cache:6379\n  DB_HOST: postgres\n</code></pre> <p>infrastructure/databases.yml:</p> <pre><code>include:\n  - path: ../fragments/common.yml\n\nservices:\n  postgres:\n    &lt;&lt;: *service-defaults\n    image: postgres:15-alpine\n    environment:\n      POSTGRES_DB: ${DB_NAME:-appdb}\n      POSTGRES_USER: ${DB_USER:-admin}\n      POSTGRES_PASSWORD: ${DB_PASSWORD}\n    volumes:\n      - postgres-data:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U ${DB_USER:-admin}\"]\n      &lt;&lt;: *healthcheck\n\nvolumes:\n  postgres-data:\n</code></pre> <p>services/user-service.yml:</p> <pre><code>include:\n  - path: ../fragments/common.yml\n\nservices:\n  user-service:\n    &lt;&lt;: *node-service\n    build:\n      context: ./user-service\n      dockerfile: Dockerfile\n    environment:\n      &lt;&lt;: *env-common\n      SERVICE_NAME: user-service\n      PORT: 3001\n    ports:\n      - \"3001:3001\"\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"-q\", \"-O\", \"-\", \"http://localhost:3001/health\"]\n      &lt;&lt;: *healthcheck\n    depends_on:\n      postgres:\n        condition: service_healthy\n</code></pre> <p>docker-compose.yml (Main):</p> <pre><code>include:\n  - ./infrastructure/databases.yml\n  - ./infrastructure/cache.yml\n  - ./infrastructure/messaging.yml\n  - ./services/user-service.yml\n  - ./services/order-service.yml\n  - ./services/payment-service.yml\n\nnetworks:\n  microservices:\n    driver: bridge\n</code></pre>"},{"location":"007-DockerCompose-fragments/#example-2-multi-environment-setup","title":"Example 2: Multi-Environment Setup","text":"<p>Structure:</p> <pre><code>project/\n\u251c\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 compose/\n\u2502   \u251c\u2500\u2500 base.yml\n\u2502   \u251c\u2500\u2500 fragments.yml\n\u2502   \u251c\u2500\u2500 dev.yml\n\u2502   \u2514\u2500\u2500 prod.yml\n\u2514\u2500\u2500 .env\n</code></pre> <p>compose/fragments.yml:</p> <pre><code>x-app-base: &amp;app-base\n  restart: unless-stopped\n  networks:\n    - app-net\n\nx-dev-settings: &amp;dev-settings\n  build:\n    target: development\n  volumes:\n    - ./src:/app/src:rw\n  environment:\n    NODE_ENV: development\n    DEBUG: \"*\"\n\nx-prod-settings: &amp;prod-settings\n  image: ${REGISTRY}/app:${VERSION}\n  read_only: true\n  security_opt:\n    - no-new-privileges:true\n  environment:\n    NODE_ENV: production\n</code></pre> <p>compose/dev.yml:</p> <pre><code>include:\n  - path: ./fragments.yml\n\nservices:\n  app-dev:\n    &lt;&lt;: [*app-base, *dev-settings]\n    container_name: app-dev\n    ports:\n      - \"3001:3000\"\n    command: npm run dev\n\n  # Development tools\n  adminer:\n    image: adminer:latest\n    ports:\n      - \"8080:8080\"\n    networks:\n      - app-net\n</code></pre> <p>compose/prod.yml:</p> <pre><code>include:\n  - path: ./fragments.yml\n\nservices:\n  app-prod:\n    &lt;&lt;: [*app-base, *prod-settings]\n    deploy:\n      replicas: 3\n      resources:\n        limits:\n          cpus: '1.0'\n          memory: 1G\n    ports:\n      - \"3000:3000\"\n    command: npm start\n</code></pre> <p>docker-compose.yml:</p> <pre><code>include:\n  - compose/base.yml\n  - path: compose/${ENVIRONMENT:-dev}.yml\n\nnetworks:\n  app-net:\n    driver: bridge\n</code></pre> <p>Usage:</p> <pre><code># Development\nENVIRONMENT=dev docker compose up\n\n# Production\nENVIRONMENT=prod docker compose up\n</code></pre>"},{"location":"007-DockerCompose-fragments/#example-3-modular-configuration","title":"Example 3: Modular Configuration","text":"<p>Complete modular setup with fragments and includes:</p> <pre><code># docker-compose.yml\ninclude:\n  # Core infrastructure\n  - path: ./infrastructure/postgres.yml\n  - path: ./infrastructure/redis.yml\n  - path: ./infrastructure/nginx.yml\n\n  # Application services\n  - path: ./services/api.yml\n  - path: ./services/worker.yml\n  - path: ./services/scheduler.yml\n\n  # Monitoring stack\n  - path: ./monitoring/prometheus.yml\n  - path: ./monitoring/grafana.yml\n\n  # Environment-specific overrides\n  - path: ./overrides/${ENV:-development}.yml\n    env_file: .env.${ENV:-development}\n\n# Global networks\nnetworks:\n  frontend:\n    driver: bridge\n  backend:\n    driver: bridge\n    internal: true\n  monitoring:\n    driver: bridge\n\n# Global volumes\nvolumes:\n  postgres_data:\n  redis_data:\n  prometheus_data:\n  grafana_data:\n</code></pre>"},{"location":"007-DockerCompose-fragments/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use Extension Fields for Reusable Fragments: <pre><code>x-logging: &amp;logging\n  driver: json-file\n  options:\n    max-size: \"10m\"\n</code></pre></p> </li> <li> <p>Organize Includes Logically:</p> <ul> <li>Group by function (infrastructure, services, monitoring)</li> <li>Separate environment-specific configurations</li> <li>Use subdirectories for clarity</li> </ul> </li> <li> <p>Name Anchors Descriptively:</p> <pre><code>x-node-service-defaults: &amp;node-service-defaults\nx-python-service-defaults: &amp;python-service-defaults\nx-database-healthcheck: &amp;database-healthcheck\n</code></pre> </li> <li> <p>Validate Merged Configuration:</p> <ul> <li>Use Docker Compose Config Command:</li> </ul> <pre><code>docker compose config\n</code></pre> </li> <li> <p>Use Environment Variables:</p> <ul> <li>Use Common Environment Variables:</li> </ul> <pre><code>x-common-env: &amp;common-env\n  ENVIRONMENT: ${ENVIRONMENT:-production}\n  LOG_LEVEL: ${LOG_LEVEL:-info}\n</code></pre> </li> <li> <p>Document Your Fragments:</p> <ul> <li>Include comments in your fragment files to explain their purpose and usage.</li> </ul> <pre><code># Logging configuration - 10MB max size, 3 file rotation\nx-logging: &amp;logging\n  driver: json-file\n  options:\n    max-size: \"10m\"\n    max-file: \"3\"\n</code></pre> </li> <li> <p>Keep Fragments DRY but Readable:</p> <ul> <li>Don\u2019t over-fragment</li> <li>Balance reusability with readability</li> <li>Use fragments for truly common configurations</li> </ul> </li> <li> <p>Version Control:</p> <ul> <li>Commit all fragment files</li> <li>Document the composition structure in README</li> <li>Use <code>.env.example</code> for required variables</li> </ul> </li> </ol>"},{"location":"007-DockerCompose-fragments/#common-patterns","title":"Common Patterns","text":""},{"location":"007-DockerCompose-fragments/#pattern-1-base-service-template","title":"Pattern 1: Base Service Template","text":"<pre><code>x-app-template: &amp;app\n  restart: unless-stopped\n  networks:\n    - app-network\n  logging:\n    driver: json-file\n    options:\n      max-size: \"10m\"\n  healthcheck:\n    interval: 30s\n    timeout: 10s\n    retries: 3\n\nservices:\n  service1:\n    &lt;&lt;: *app\n    image: service1:latest\n\n  service2:\n    &lt;&lt;: *app\n    image: service2:latest\n</code></pre>"},{"location":"007-DockerCompose-fragments/#pattern-2-environment-overrides","title":"Pattern 2: Environment Overrides","text":"<pre><code>x-base: &amp;base\n  image: app:latest\n\nx-dev: &amp;dev\n  &lt;&lt;: *base\n  volumes:\n    - ./src:/app/src\n  environment:\n    DEBUG: \"true\"\n\nx-prod: &amp;prod\n  &lt;&lt;: *base\n  read_only: true\n  environment:\n    DEBUG: \"false\"\n</code></pre>"},{"location":"007-DockerCompose-fragments/#pattern-3-multi-container-application","title":"Pattern 3: Multi-Container Application","text":"<pre><code>x-defaults: &amp;defaults\n  restart: unless-stopped\n  networks:\n    - app\n\nservices:\n  web:\n    &lt;&lt;: *defaults\n    image: nginx\n    depends_on:\n      - api\n\n  api:\n    &lt;&lt;: *defaults\n    image: node\n    depends_on:\n      - db\n\n  db:\n    &lt;&lt;: *defaults\n    image: postgres\n</code></pre>"},{"location":"007-DockerCompose-fragments/#troubleshooting","title":"Troubleshooting","text":""},{"location":"007-DockerCompose-fragments/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<ol> <li> <p>Anchor Not Found:</p> <p><pre><code>Error: Unknown anchor 'service-defaults'\n</code></pre> Solution: Ensure the anchor is defined before it\u2019s referenced. Anchors must be defined in the same file or in an included file that\u2019s loaded first.</p> </li> <li> <p>Merge Conflicts:</p> <pre><code># This will override, not merge\nservice:\n  &lt;&lt;: *base\n  environment:  # This replaces entire environment from *base\n    NEW_VAR: value\n\n# Correct way to merge:\nservice:\n  &lt;&lt;: *base\n  environment:\n    &lt;&lt;: *base-env  # Merge base environment\n    NEW_VAR: value # Add new variable\n</code></pre> </li> <li> <p>Include Path Issues:</p> <pre><code>Error: include path not found: ./compose/services.yml\n</code></pre> <p>Solution: Use paths relative to the main compose file location.</p> </li> <li> <p>Circular Dependencies:</p> <pre><code># Avoid this\ninclude:\n  - a.yml  # includes b.yml\n  - b.yml  # includes a.yml\n</code></pre> </li> <li> <p>Validation Errors:</p> <pre><code># Validate your configuration\ndocker compose config --quiet\n\n# View merged configuration\ndocker compose config &gt; merged-config.yml\n</code></pre> </li> </ol>"},{"location":"007-DockerCompose-fragments/#debugging-commands","title":"Debugging Commands","text":"<ul> <li> <p>Debug with Docker Compose Config:</p> <pre><code># Show final merged configuration\ndocker compose config\n\n# Validate without starting services\ndocker compose config --quiet\n\n# Show configuration for specific service\ndocker compose config api\n\n# List all services\ndocker compose config --services\n\n# Show volumes\ndocker compose config --volumes\n\n# Show networks\ndocker compose config --networks\n\n# Resolve environment variables\ndocker compose config --resolve-image-digests\n</code></pre> </li> </ul>"},{"location":"007-DockerCompose-fragments/#hands-on-exercises","title":"Hands-On Exercises","text":""},{"location":"007-DockerCompose-fragments/#exercise-1-create-a-microservices-setup","title":"Exercise 1: Create a Microservices Setup","text":"<p>Create a compose configuration with:</p> <ul> <li>3 microservices using the same base template</li> <li>Shared logging configuration</li> <li>Individual health checks</li> <li>Common environment variables</li> </ul>"},{"location":"007-DockerCompose-fragments/#exercise-2-multi-environment-configuration","title":"Exercise 2: Multi-Environment Configuration","text":"<p>Build a setup that supports:</p> <ul> <li>Development environment with hot-reload</li> <li>Staging environment with production-like settings</li> <li>Production environment with security hardening</li> <li>All using shared base configuration</li> </ul>"},{"location":"007-DockerCompose-fragments/#exercise-3-modular-infrastructure","title":"Exercise 3: Modular Infrastructure","text":"<p>Design a modular compose setup:</p> <ul> <li>Separate files for databases, caching, messaging</li> <li>Include-based composition</li> <li>Environment-specific overrides</li> <li>Shared network and volume definitions</li> </ul>"},{"location":"007-DockerCompose-fragments/#useful-commands","title":"Useful Commands","text":"<pre><code># View merged configuration\ndocker compose config\n\n# Validate compose file\ndocker compose config --quiet\n\n# Start with specific environment\nENV=production docker compose up -d\n\n# View specific service configuration\ndocker compose config service-name\n\n# List all services\ndocker compose config --services\n\n# Pull all images\ndocker compose pull\n\n# Build all services\ndocker compose build\n\n# Up with build\ndocker compose up --build\n\n# Scale specific service\ndocker compose up -d --scale api=3\n\n# View logs\ndocker compose logs -f service-name\n\n# Stop all services\ndocker compose down\n\n# Remove volumes\ndocker compose down -v\n</code></pre>"},{"location":"008-crictl/","title":"008-crictl","text":""},{"location":"008-crictl/#lab-008-debugging-containers-with-crictl","title":"Lab 008 - Debugging Containers with crictl","text":"<ul> <li>In this lab we will explore <code>crictl</code>, a command-line interface for CRI-compatible container runtimes</li> <li><code>crictl</code> is designed for debugging and inspecting containers and images on Kubernetes nodes</li> <li>We will learn how to install, configure, and use <code>crictl</code> to debug Docker containers</li> <li>This tool is essential for troubleshooting container issues in Kubernetes environments</li> <li> <p>The lab is divided into several tasks:</p> </li> <li> <p>01. What is crictl?</p> </li> <li>02. Prerequisites</li> <li>03. Installation</li> <li>04. Basic Configuration</li> <li>05. Working with Container Images</li> <li>06. Container Operations</li> <li>07. Pod Operations</li> <li>08. Debugging Containers</li> <li>09. Inspecting Container Resources</li> <li>10. Logs and Troubleshooting</li> <li>11. Advanced Debugging Techniques</li> <li>12. Clean up</li> </ul>"},{"location":"008-crictl/#01-what-is-crictl","title":"01. What is crictl?","text":"<p>crictl (CRI CLI) is a command-line interface for interacting with CRI-compatible container runtimes. It provides:</p> <ul> <li>Container Runtime Interface (CRI): Direct interaction with container runtimes like containerd, CRI-O, and Docker (via dockershim)</li> <li>Debugging Tool: Designed specifically for debugging containers in Kubernetes environments</li> <li>Inspection Capabilities: Detailed inspection of containers, pods, and images</li> <li>Troubleshooting: Essential for diagnosing container issues on Kubernetes nodes</li> </ul>"},{"location":"008-crictl/#key-features","title":"Key Features","text":"<ul> <li>\u2705 List and inspect containers and pods</li> <li>\u2705 View container logs and execute commands</li> <li>\u2705 Pull and manage container images</li> <li>\u2705 Monitor container resource usage</li> <li>\u2705 Debug container networking and storage</li> <li>\u2705 Compatible with multiple container runtimes</li> </ul>"},{"location":"008-crictl/#crictl-vs-docker-cli","title":"crictl vs docker CLI","text":"Feature crictl docker Purpose Kubernetes debugging General container management Scope CRI-compatible runtimes Docker Engine only Pod Support Native No Use Case K8s troubleshooting Development &amp; production"},{"location":"008-crictl/#02-prerequisites","title":"02. Prerequisites","text":"<p>Before installing <code>crictl</code>, ensure you have:</p> <ul> <li>Operating System: Linux or macOS</li> <li>Container Runtime: One of the following:</li> <li>containerd (standalone or via Kubernetes)</li> <li>CRI-O</li> <li>Docker Desktop (macOS/Windows)</li> <li>Minikube, kind, or k3s (with containerd)</li> <li>Root/Sudo Access: Required for most operations</li> <li>curl or wget: For downloading the binary</li> </ul> <p>OrbStack Users</p> <p>OrbStack doesn\u2019t expose containerd\u2019s CRI socket directly. To use crictl:</p> <ol> <li>Option 1: Use Minikube, kind, or k3s which provide CRI access</li> <li>Option 2: Use Docker CLI for container debugging instead</li> <li>Option 3: Use a Kubernetes cluster (local or remote)</li> </ol> <p>This lab is designed for environments with direct CRI access.</p>"},{"location":"008-crictl/#check-existing-runtime","title":"Check Existing Runtime","text":"<pre><code># Check if containerd is running\nsystemctl status containerd\n\n# Check if Docker is running\nsystemctl status docker\n\n# Check runtime socket locations\nls -la /var/run/containerd/containerd.sock\nls -la /var/run/dockershim.sock\nls -la /var/run/crio/crio.sock\n</code></pre>"},{"location":"008-crictl/#03-installation","title":"03. Installation","text":""},{"location":"008-crictl/#option-1-download-pre-built-binary-recommended","title":"Option 1: Download Pre-built Binary (Recommended)","text":"<pre><code># Set the version\nVERSION=\"v1.28.0\"\n\n# Download crictl\nwget https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-$VERSION-linux-amd64.tar.gz\n\n# Extract the binary\nsudo tar zxvf crictl-$VERSION-linux-amd64.tar.gz -C /usr/local/bin\n\n# Remove the archive\nrm -f crictl-$VERSION-linux-amd64.tar.gz\n\n# Verify installation\ncrictl --version\n</code></pre>"},{"location":"008-crictl/#option-2-install-via-package-manager","title":"Option 2: Install via Package Manager","text":"<p>On Ubuntu/Debian:</p> <pre><code># Install dependencies\nsudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl\n\n# Add Kubernetes repository\ncurl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\necho \"deb https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list\n\n# Install cri-tools\nsudo apt-get update\nsudo apt-get install -y cri-tools\n\n# Verify installation\ncrictl --version\n</code></pre> <p>On macOS:</p> <pre><code># Using Homebrew\nbrew install crictl\n\n# Verify installation\ncrictl --version\n</code></pre>"},{"location":"008-crictl/#option-3-build-from-source","title":"Option 3: Build from Source","text":"<pre><code># Install Go (if not already installed)\nwget https://go.dev/dl/go1.21.0.linux-amd64.tar.gz\nsudo tar -C /usr/local -xzf go1.21.0.linux-amd64.tar.gz\nexport PATH=$PATH:/usr/local/go/bin\n\n# Clone the repository\ngit clone https://github.com/kubernetes-sigs/cri-tools.git\ncd cri-tools\n\n# Build crictl\nmake\n\n# Install the binary\nsudo install -m 755 build/bin/linux/amd64/crictl /usr/local/bin/crictl\n\n# Verify installation\ncrictl --version\n</code></pre>"},{"location":"008-crictl/#04-basic-configuration","title":"04. Basic Configuration","text":""},{"location":"008-crictl/#create-configuration-file","title":"Create Configuration File","text":"<p><code>crictl</code> uses a configuration file to determine which runtime socket to connect to:</p> <pre><code># Create the config directory\nsudo mkdir -p /etc/crictl\n\n# Create the configuration file\nsudo tee /etc/crictl/crictl.yaml &gt; /dev/null &lt;&lt;EOF\nruntime-endpoint: unix:///var/run/containerd/containerd.sock\nimage-endpoint: unix:///var/run/containerd/containerd.sock\ntimeout: 10\ndebug: false\npull-image-on-create: false\nEOF\n</code></pre>"},{"location":"008-crictl/#configuration-for-different-runtimes","title":"Configuration for Different Runtimes","text":"<p>For Docker Desktop (macOS/Windows):</p> <p>Docker Desktop Uses Containerd</p> <p>Docker Desktop uses containerd as its runtime. Use the containerd socket, not the Docker socket.</p> <pre><code># For macOS with Docker Desktop\nmkdir -p ~/.config/crictl\ncat &gt; ~/.config/crictl/crictl.yaml &lt;&lt;EOF\nruntime-endpoint: unix:///var/run/containerd/containerd.sock\nimage-endpoint: unix:///var/run/containerd/containerd.sock\ntimeout: 10\ndebug: false\nEOF\n</code></pre> <p>For Docker Engine on Linux (using containerd):</p> <pre><code># Docker Engine uses containerd, not CRI directly\nsudo tee /etc/crictl/crictl.yaml &gt; /dev/null &lt;&lt;EOF\nruntime-endpoint: unix:///run/containerd/containerd.sock\nimage-endpoint: unix:///run/containerd/containerd.sock\ntimeout: 10\nEOF\n</code></pre> <p>For CRI-O:</p> <pre><code>sudo tee /etc/crictl/crictl.yaml &gt; /dev/null &lt;&lt;EOF\nruntime-endpoint: unix:///var/run/crio/crio.sock\nimage-endpoint: unix:///var/run/crio/crio.sock\ntimeout: 10\nEOF\n</code></pre>"},{"location":"008-crictl/#using-runtime-endpoint-flag","title":"Using Runtime Endpoint Flag","text":"<p>Instead of configuration file, you can specify the runtime endpoint directly:</p> <pre><code># Using containerd (Docker Desktop on macOS)\ncrictl --runtime-endpoint unix:///var/run/containerd/containerd.sock version\n\n# Using containerd (Linux)\ncrictl --runtime-endpoint unix:///run/containerd/containerd.sock version\n\n# Set as environment variable (macOS/Docker Desktop)\nexport CONTAINER_RUNTIME_ENDPOINT=unix:///var/run/containerd/containerd.sock\ncrictl version\n\n# Set as environment variable (Linux)\nexport CONTAINER_RUNTIME_ENDPOINT=unix:///run/containerd/containerd.sock\ncrictl version\n</code></pre>"},{"location":"008-crictl/#find-your-container-runtime-socket","title":"Find Your Container Runtime Socket","text":"<pre><code># List all potential runtime sockets\nls -la /var/run/containerd/*.sock 2&gt;/dev/null\nls -la /run/containerd/*.sock 2&gt;/dev/null\nls -la /var/run/crio/*.sock 2&gt;/dev/null\n\n# For Docker Desktop on macOS\nls -la /var/run/containerd/containerd.sock\n\n# Test connectivity\ncrictl --runtime-endpoint unix:///var/run/containerd/containerd.sock version\n</code></pre>"},{"location":"008-crictl/#verify-configuration","title":"Verify Configuration","text":"<pre><code># Check runtime version\ncrictl version\n\n# Get runtime info\ncrictl info\n\n### Expected Output:\n# {\n#   \"status\": {\n#     \"conditions\": [\n#       {\n#         \"type\": \"RuntimeReady\",\n#         \"status\": true,\n#         \"message\": \"\"\n#       }\n#     ]\n#   }\n# }\n</code></pre>"},{"location":"008-crictl/#alternative-using-crictl-with-minikube","title":"Alternative: Using crictl with Minikube","text":"<p>If you\u2019re using OrbStack or another runtime without direct CRI access, you can use Minikube:</p> <pre><code># Start Minikube with containerd\nminikube start --container-runtime=containerd\n\n# Get the socket path\nminikube ssh \"ls -la /run/containerd/containerd.sock\"\n\n# Configure crictl to use Minikube\ncat &gt; ~/.config/crictl/crictl.yaml &lt;&lt;EOF\nruntime-endpoint: unix:///run/containerd/containerd.sock\nimage-endpoint: unix:///run/containerd/containerd.sock\ntimeout: 10\nEOF\n\n# SSH into Minikube and use crictl\nminikube ssh\n\n# Now inside Minikube VM:\ncrictl version\ncrictl images\ncrictl ps\n</code></pre>"},{"location":"008-crictl/#alternative-using-crictl-with-kind","title":"Alternative: Using crictl with kind","text":"<pre><code># Create a kind cluster\nkind create cluster --name crictl-demo\n\n# Get the container ID of the kind control plane\nKIND_CONTAINER=$(docker ps --filter \"name=crictl-demo-control-plane\" -q)\n\n# Execute crictl commands in the kind container\ndocker exec $KIND_CONTAINER crictl version\ndocker exec $KIND_CONTAINER crictl images\ndocker exec $KIND_CONTAINER crictl ps -a\n</code></pre>"},{"location":"008-crictl/#05-working-with-container-images","title":"05. Working with Container Images","text":""},{"location":"008-crictl/#list-images","title":"List Images","text":"<pre><code># List all images\ncrictl images\n\n# List images with detailed output\ncrictl images -v\n\n# List specific image\ncrictl images nginx\n\n# List images in JSON format\ncrictl images -o json\n</code></pre>"},{"location":"008-crictl/#pull-images","title":"Pull Images","text":"<pre><code># Pull an image\ncrictl pull nginx:latest\n\n# Pull with specific registry\ncrictl pull docker.io/library/alpine:latest\n\n# Pull from private registry (requires authentication)\ncrictl pull myregistry.com/myimage:v1.0\n</code></pre>"},{"location":"008-crictl/#inspect-images","title":"Inspect Images","text":"<pre><code># Get detailed image information\ncrictl inspecti nginx:latest\n\n# Get image in JSON format\ncrictl inspecti --output json nginx:latest | jq .\n\n# Check image size and layers\ncrictl inspecti nginx:latest | grep -E 'size|layer'\n</code></pre>"},{"location":"008-crictl/#remove-images","title":"Remove Images","text":"<pre><code># Remove image by name\ncrictl rmi nginx:latest\n\n# Remove image by ID\ncrictl rmi a1b2c3d4e5f6\n\n# Remove all unused images\ncrictl rmi --prune\n\n# Force remove image\ncrictl rmi -f nginx:latest\n</code></pre>"},{"location":"008-crictl/#06-container-operations","title":"06. Container Operations","text":""},{"location":"008-crictl/#list-containers","title":"List Containers","text":"<pre><code># List all running containers\ncrictl ps\n\n# List all containers (including stopped)\ncrictl ps -a\n\n# List containers with detailed information\ncrictl ps -v\n\n# Filter containers by state\ncrictl ps --state running\ncrictl ps --state exited\n\n# Filter by name\ncrictl ps --name nginx\n\n# Filter by pod\ncrictl ps --pod mypod\n</code></pre>"},{"location":"008-crictl/#create-and-run-containers","title":"Create and Run Containers","text":"<p>First, we need to create a pod sandbox, then create containers within it:</p> <p>Step 1: Create Pod Configuration</p> <pre><code># Create pod config file\ncat &gt; pod-config.json &lt;&lt;EOF\n{\n    \"metadata\": {\n        \"name\": \"debug-pod\",\n        \"namespace\": \"default\",\n        \"uid\": \"debug-pod-uid\"\n    },\n    \"log_directory\": \"/tmp\",\n    \"linux\": {}\n}\nEOF\n</code></pre> <p>Step 2: Create Container Configuration</p> <pre><code># Create container config file\ncat &gt; container-config.json &lt;&lt;EOF\n{\n    \"metadata\": {\n        \"name\": \"debug-container\"\n    },\n    \"image\": {\n        \"image\": \"nginx:latest\"\n    },\n    \"command\": [\n        \"nginx\",\n        \"-g\",\n        \"daemon off;\"\n    ],\n    \"log_path\": \"debug-container.log\",\n    \"linux\": {}\n}\nEOF\n</code></pre> <p>Step 3: Create Pod and Container</p> <pre><code># Create the pod sandbox\nPOD_ID=$(crictl runp pod-config.json)\necho \"Pod ID: $POD_ID\"\n\n# Create the container\nCONTAINER_ID=$(crictl create $POD_ID container-config.json pod-config.json)\necho \"Container ID: $CONTAINER_ID\"\n\n# Start the container\ncrictl start $CONTAINER_ID\n\n# Verify the container is running\ncrictl ps\n</code></pre>"},{"location":"008-crictl/#stop-and-remove-containers","title":"Stop and Remove Containers","text":"<pre><code># Stop a container\ncrictl stop $CONTAINER_ID\n\n# Stop with timeout\ncrictl stop --timeout 30 $CONTAINER_ID\n\n# Remove a container\ncrictl rm $CONTAINER_ID\n\n# Force remove a running container\ncrictl rm -f $CONTAINER_ID\n\n# Remove all stopped containers\ncrictl rm $(crictl ps -a -q --state exited)\n</code></pre>"},{"location":"008-crictl/#07-pod-operations","title":"07. Pod Operations","text":""},{"location":"008-crictl/#list-pods","title":"List Pods","text":"<pre><code># List all pods\ncrictl pods\n\n# List pods with details\ncrictl pods -v\n\n# List pods in specific namespace\ncrictl pods --namespace default\n\n# Filter by pod state\ncrictl pods --state ready\ncrictl pods --state notready\n\n# Get pod in JSON format\ncrictl pods -o json\n</code></pre>"},{"location":"008-crictl/#inspect-pods","title":"Inspect Pods","text":"<pre><code># Inspect specific pod\ncrictl inspectp $POD_ID\n\n# Get pod details in JSON\ncrictl inspectp --output json $POD_ID | jq .\n\n# Check pod metadata\ncrictl inspectp $POD_ID | grep -A 10 metadata\n</code></pre>"},{"location":"008-crictl/#pod-lifecycle-management","title":"Pod Lifecycle Management","text":"<pre><code># Create pod from config\ncrictl runp pod-config.json\n\n# Stop a pod (stops all containers in the pod)\ncrictl stopp $POD_ID\n\n# Remove a pod\ncrictl rmp $POD_ID\n\n# Force remove a pod\ncrictl rmp -f $POD_ID\n\n# Remove all stopped pods\ncrictl rmp $(crictl pods -q --state notready)\n</code></pre>"},{"location":"008-crictl/#08-debugging-containers","title":"08. Debugging Containers","text":""},{"location":"008-crictl/#execute-commands-in-containers","title":"Execute Commands in Containers","text":"<pre><code># Execute command in running container\ncrictl exec -it $CONTAINER_ID /bin/sh\n\n# Execute specific command\ncrictl exec $CONTAINER_ID ls -la /etc\n\n# Execute with environment variables\ncrictl exec -e ENV_VAR=value $CONTAINER_ID env\n\n# Execute as specific user\ncrictl exec -u 1000 $CONTAINER_ID whoami\n</code></pre>"},{"location":"008-crictl/#interactive-debugging-session","title":"Interactive Debugging Session","text":"<pre><code># Start interactive shell\ncrictl exec -it $CONTAINER_ID /bin/bash\n\n# Once inside, you can:\n# - Check running processes\nps aux\n\n# - Check network configuration\nip addr\nnetstat -tulpn\n\n# - Check file system\ndf -h\nls -la /\n\n# - Check environment variables\nenv\n\n# - Install debugging tools (if writable)\napt-get update &amp;&amp; apt-get install -y procps net-tools\n</code></pre>"},{"location":"008-crictl/#container-stats-and-resources","title":"Container Stats and Resources","text":"<pre><code># Get container stats (CPU, Memory, etc.)\ncrictl stats\n\n# Get stats for specific container\ncrictl stats $CONTAINER_ID\n\n# Continuous monitoring\nwatch -n 2 crictl stats\n\n# Get stats in JSON format\ncrictl stats -o json\n</code></pre>"},{"location":"008-crictl/#09-inspecting-container-resources","title":"09. Inspecting Container Resources","text":""},{"location":"008-crictl/#inspect-container-details","title":"Inspect Container Details","text":"<pre><code># Full container inspection\ncrictl inspect $CONTAINER_ID\n\n# Get specific fields using JSON query\ncrictl inspect $CONTAINER_ID | jq '.info.config'\n\n# Check container mounts\ncrictl inspect $CONTAINER_ID | jq '.info.mounts'\n\n# Check container environment\ncrictl inspect $CONTAINER_ID | jq '.info.config.envs'\n\n# Check container labels\ncrictl inspect $CONTAINER_ID | jq '.info.config.labels'\n</code></pre>"},{"location":"008-crictl/#check-container-state","title":"Check Container State","text":"<pre><code># Get container state\ncrictl inspect $CONTAINER_ID | jq '.status.state'\n\n# Check exit code\ncrictl inspect $CONTAINER_ID | jq '.status.exitCode'\n\n# Check started and finished times\ncrictl inspect $CONTAINER_ID | jq '.status | {started: .startedAt, finished: .finishedAt}'\n\n# Check container PID\ncrictl inspect $CONTAINER_ID | jq '.info.pid'\n</code></pre>"},{"location":"008-crictl/#inspect-container-networking","title":"Inspect Container Networking","text":"<pre><code># Get container IP address\ncrictl inspect $CONTAINER_ID | jq '.status.network.ip'\n\n# Check network namespace\ncrictl inspect $CONTAINER_ID | jq '.info.runtimeSpec.linux.namespaces[] | select(.type == \"network\")'\n\n# List network interfaces\ncrictl exec $CONTAINER_ID ip link show\n\n# Check DNS configuration\ncrictl exec $CONTAINER_ID cat /etc/resolv.conf\n</code></pre>"},{"location":"008-crictl/#10-logs-and-troubleshooting","title":"10. Logs and Troubleshooting","text":""},{"location":"008-crictl/#view-container-logs","title":"View Container Logs","text":"<pre><code># View container logs\ncrictl logs $CONTAINER_ID\n\n# Follow logs in real-time\ncrictl logs -f $CONTAINER_ID\n\n# Get last N lines\ncrictl logs --tail 50 $CONTAINER_ID\n\n# Show timestamps\ncrictl logs --timestamps $CONTAINER_ID\n\n# Logs since specific time\ncrictl logs --since 1h $CONTAINER_ID\n</code></pre>"},{"location":"008-crictl/#common-debugging-scenarios","title":"Common Debugging Scenarios","text":"<p>Scenario 1: Container Keeps Restarting</p> <pre><code># Check container status\ncrictl ps -a | grep mycontainer\n\n# Get container ID\nCONTAINER_ID=$(crictl ps -a --name mycontainer -q | head -1)\n\n# Check logs for errors\ncrictl logs $CONTAINER_ID\n\n# Inspect exit code\ncrictl inspect $CONTAINER_ID | jq '.status.exitCode'\n\n# Check restart count (from pod perspective)\ncrictl inspectp $(crictl inspect $CONTAINER_ID | jq -r '.info.sandboxID')\n</code></pre> <p>Scenario 2: Container Network Issues</p> <pre><code># Check container IP\ncrictl exec $CONTAINER_ID ip addr\n\n# Test connectivity from container\ncrictl exec $CONTAINER_ID ping -c 3 8.8.8.8\n\n# Check DNS resolution\ncrictl exec $CONTAINER_ID nslookup google.com\n\n# Check listening ports\ncrictl exec $CONTAINER_ID netstat -tuln\n\n# Inspect network configuration\ncrictl inspect $CONTAINER_ID | jq '.info.runtimeSpec.linux.namespaces'\n</code></pre> <p>Scenario 3: Container Storage Issues</p> <pre><code># Check disk usage in container\ncrictl exec $CONTAINER_ID df -h\n\n# List mounts\ncrictl inspect $CONTAINER_ID | jq '.info.mounts'\n\n# Check for read-only file systems\ncrictl exec $CONTAINER_ID mount | grep ro\n\n# Inspect volume mounts\ncrictl exec $CONTAINER_ID ls -la /var/lib/\n</code></pre> <p>Scenario 4: High Resource Usage</p> <pre><code># Check container resource usage\ncrictl stats $CONTAINER_ID\n\n# Check processes inside container\ncrictl exec $CONTAINER_ID ps aux\n\n# Check memory usage\ncrictl exec $CONTAINER_ID free -h\n\n# Check for memory leaks\ncrictl exec $CONTAINER_ID top -b -n 1\n</code></pre>"},{"location":"008-crictl/#11-advanced-debugging-techniques","title":"11. Advanced Debugging Techniques","text":""},{"location":"008-crictl/#attach-to-running-container","title":"Attach to Running Container","text":"<pre><code># Attach to container's main process\ncrictl attach $CONTAINER_ID\n\n# Note: This attaches to the main process (PID 1)\n# Use Ctrl+P, Ctrl+Q to detach without stopping\n</code></pre>"},{"location":"008-crictl/#port-forwarding-for-debugging","title":"Port Forwarding for Debugging","text":"<pre><code># Get container IP\nCONTAINER_IP=$(crictl inspect $CONTAINER_ID | jq -r '.status.network.ip')\n\n# Access service from host\ncurl http://$CONTAINER_IP:80\n\n# Check open ports\ncrictl exec $CONTAINER_ID netstat -tuln\n</code></pre>"},{"location":"008-crictl/#debugging-with-nsenter","title":"Debugging with nsenter","text":"<p>Access container namespaces directly from the host:</p> <pre><code># Get container PID\nPID=$(crictl inspect $CONTAINER_ID | jq -r '.info.pid')\n\n# Enter network namespace\nsudo nsenter -t $PID -n ip addr\n\n# Enter mount namespace\nsudo nsenter -t $PID -m ls /\n\n# Enter all namespaces\nsudo nsenter -t $PID -a /bin/bash\n</code></pre>"},{"location":"008-crictl/#copy-files-tofrom-containers","title":"Copy Files To/From Containers","text":"<p>While crictl doesn\u2019t have a built-in cp command, you can use alternative methods:</p> <pre><code># Copy file from host to container\ncrictl exec $CONTAINER_ID sh -c 'cat &gt; /tmp/file.txt' &lt; local-file.txt\n\n# Copy file from container to host\ncrictl exec $CONTAINER_ID cat /etc/config.yaml &gt; local-config.yaml\n\n# Using tar for directories\ntar -cf - /local/dir | crictl exec -i $CONTAINER_ID tar -xf - -C /container/path\n</code></pre>"},{"location":"008-crictl/#performance-profiling","title":"Performance Profiling","text":"<pre><code># CPU profile\ncrictl exec $CONTAINER_ID top -b -n 1\n\n# Memory profile\ncrictl exec $CONTAINER_ID cat /proc/meminfo\n\n# I/O stats\ncrictl exec $CONTAINER_ID iostat -x 1 5\n\n# Network stats\ncrictl exec $CONTAINER_ID cat /proc/net/dev\n</code></pre>"},{"location":"008-crictl/#debugging-container-images","title":"Debugging Container Images","text":"<pre><code># Create a debug container with custom entrypoint\ncat &gt; debug-container.json &lt;&lt;EOF\n{\n    \"metadata\": {\n        \"name\": \"debug-container\"\n    },\n    \"image\": {\n        \"image\": \"nginx:latest\"\n    },\n    \"command\": [\n        \"/bin/sh\",\n        \"-c\",\n        \"sleep 3600\"\n    ],\n    \"log_path\": \"debug.log\",\n    \"linux\": {}\n}\nEOF\n\n# Create and start debug container\nDEBUG_CONTAINER=$(crictl create $POD_ID debug-container.json pod-config.json)\ncrictl start $DEBUG_CONTAINER\n\n# Now you can exec into it\ncrictl exec -it $DEBUG_CONTAINER /bin/sh\n</code></pre>"},{"location":"008-crictl/#12-clean-up","title":"12. Clean up","text":""},{"location":"008-crictl/#remove-test-resources","title":"Remove Test Resources","text":"<pre><code># Stop and remove containers\nfor container in $(crictl ps -q); do\n    crictl stop $container\n    crictl rm $container\ndone\n\n# Remove all stopped containers\ncrictl rm $(crictl ps -a -q --state exited)\n\n# Stop and remove pods\nfor pod in $(crictl pods -q); do\n    crictl stopp $pod\n    crictl rmp $pod\ndone\n\n# Remove unused images\ncrictl rmi --prune\n\n# Clean up test files\nrm -f pod-config.json container-config.json debug-container.json\n</code></pre>"},{"location":"008-crictl/#complete-cleanup","title":"Complete Cleanup","text":"<pre><code># Remove ALL containers (use with caution)\ncrictl rm -a -f\n\n# Remove ALL pods (use with caution)\ncrictl rmp -a -f\n\n# Remove ALL images (use with caution)\ncrictl rmi -a\n\n# Remove crictl config\nsudo rm -f /etc/crictl/crictl.yaml\n</code></pre>"},{"location":"008-crictl/#additional-resources","title":"Additional Resources","text":""},{"location":"008-crictl/#quick-reference-commands","title":"Quick Reference Commands","text":"<pre><code># Images\ncrictl images                  # List images\ncrictl pull &lt;image&gt;           # Pull image\ncrictl rmi &lt;image&gt;            # Remove image\ncrictl inspecti &lt;image&gt;       # Inspect image\n\n# Containers\ncrictl ps                     # List running containers\ncrictl ps -a                  # List all containers\ncrictl inspect &lt;container&gt;    # Inspect container\ncrictl logs &lt;container&gt;       # View logs\ncrictl exec -it &lt;container&gt;   # Execute command\ncrictl stats                  # Container stats\n\n# Pods\ncrictl pods                   # List pods\ncrictl runp &lt;config&gt;          # Create pod\ncrictl stopp &lt;pod&gt;            # Stop pod\ncrictl rmp &lt;pod&gt;              # Remove pod\ncrictl inspectp &lt;pod&gt;         # Inspect pod\n\n# System\ncrictl version                # Version info\ncrictl info                   # Runtime info\n</code></pre>"},{"location":"008-crictl/#useful-debugging-tips","title":"Useful Debugging Tips","text":"<ol> <li>Always check logs first: <code>crictl logs &lt;container-id&gt;</code></li> <li>Use <code>-v</code> flag for verbose output: Provides more details</li> <li>Combine with jq for JSON parsing: <code>crictl inspect &lt;id&gt; | jq .</code></li> <li>Use \u2013state flag to filter: Quickly find stopped containers</li> <li>Export CONTAINER_RUNTIME_ENDPOINT: Save typing for repeated commands</li> <li>Use watch for monitoring: <code>watch -n 2 crictl stats</code></li> </ol>"},{"location":"008-crictl/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<p>Issue: \u201cerror reading server preface: http2: frame too large\u201d or \u201cCRI v1 image API\u201d error</p> <pre><code># Problem: Trying to connect to Docker socket instead of containerd\n# Docker doesn't support CRI protocol directly\n\n# Solution 1: Use containerd socket (macOS/Docker Desktop)\nmkdir -p ~/.config/crictl\ncat &gt; ~/.config/crictl/crictl.yaml &lt;&lt;EOF\nruntime-endpoint: unix:///var/run/containerd/containerd.sock\nimage-endpoint: unix:///var/run/containerd/containerd.sock\ntimeout: 10\nEOF\n\n# Solution 2: Find correct socket location\nls -la /var/run/containerd/containerd.sock\nls -la /run/containerd/containerd.sock\n\n# Solution 3: Test with explicit endpoint\ncrictl --runtime-endpoint unix:///var/run/containerd/containerd.sock version\n</code></pre> <p>Issue: \u201cconnection refused\u201d error</p> <pre><code># Solution: Check runtime socket exists and crictl config\nls -la /var/run/containerd/containerd.sock\ncat ~/.config/crictl/crictl.yaml\ncat /etc/crictl/crictl.yaml\n\n# Verify containerd is running (macOS/Docker Desktop)\nps aux | grep containerd\n\n# Check Docker Desktop is running\ndocker version\n</code></pre> <p>Issue: Permission denied</p> <pre><code># Solution: Run with sudo or add user to docker/containerd group\nsudo crictl ps\n\n# Or add user to docker group (Linux)\nsudo usermod -aG docker $USER\nnewgrp docker\n\n# On macOS with Docker Desktop, usually no sudo needed\n# Just ensure Docker Desktop is running\n</code></pre> <p>Issue: Container not starting</p> <pre><code># Solution: Check logs and inspect container\ncrictl logs &lt;container-id&gt;\ncrictl inspect &lt;container-id&gt; | jq '.status'\n\n# Check if container exists\ncrictl ps -a | grep &lt;container-name&gt;\n\n# Check pod status\ncrictl pods\n</code></pre> <p>Issue: \u201cnamespace not found\u201d on macOS</p> <pre><code># crictl shows containers from containerd namespace, not Docker namespace\n# To see Docker containers, you need to use Docker CLI\n\n# List containerd namespaces\nsudo ctr --address /var/run/containerd/containerd.sock namespaces list\n\n# List containers in specific namespace\nsudo ctr --address /var/run/containerd/containerd.sock --namespace k8s.io containers list\n\n# Note: Docker containers run in 'moby' namespace usually\nsudo ctr --address /var/run/containerd/containerd.sock --namespace moby containers list\n</code></pre>"},{"location":"008-crictl/#documentation-links","title":"Documentation Links","text":"<ul> <li>Official Documentation: https://github.com/kubernetes-sigs/cri-tools</li> <li>CRI Specification: https://github.com/kubernetes/cri-api</li> <li>Containerd: https://containerd.io/</li> <li>Kubernetes Debugging: https://kubernetes.io/docs/tasks/debug/</li> </ul>"},{"location":"009-dive-layers/","title":"009-dive-layers","text":""},{"location":"009-dive-layers/#lab-009-inspect-docker-image-layers-with-dive","title":"Lab 009 - Inspect Docker Image Layers with Dive","text":"<ul> <li>This lab introduces the <code>dive</code> tool for analyzing Docker image layers.</li> <li><code>dive</code> is a tool for exploring each layer in a Docker image, helping you understand what\u2019s changed in each layer and identify ways to optimize your images.</li> <li>In this lab, we\u2019ll create a multi-layer Docker image and use <code>dive</code> to inspect its layers.</li> <li>By the end of this lab, you\u2019ll understand how Docker images are built layer by layer and how to use <code>dive</code> for image analysis.</li> </ul>"},{"location":"009-dive-layers/#what-is-dive","title":"What is Dive?","text":"<ul> <li><code>dive</code> is a tool for exploring the contents of Docker images.</li> <li>It allows you to see what files were added, modified, or removed in each layer.</li> <li>This is useful for optimizing image size, understanding image composition, and debugging build issues.</li> <li><code>dive</code> can be run as a Docker container itself or installed locally.</li> </ul>"},{"location":"009-dive-layers/#01-setup-dive","title":"01. Setup Dive","text":"<ul> <li>First, let\u2019s pull the <code>dive</code> image from Docker Hub.</li> <li>If <code>dive</code> is not installed locally, we can use it via Docker.</li> </ul> <pre><code># Pull the dive image\ndocker pull wagoodman/dive\n</code></pre> <ul> <li>Check if <code>dive</code> is installed locally. If not, we\u2019ll alias it to run via Docker.</li> </ul> <pre><code># Check if dive is installed\nif ! command -v dive &amp;&gt; /dev/null\nthen\n  echo \"dive could not be found, using Docker alias...\"\n  # Alias dive to run via Docker\n  alias dive=\"docker run -ti --rm -v /var/run/docker.sock:/var/run/docker.sock docker.io/wagoodman/dive\"\nfi\n</code></pre>"},{"location":"009-dive-layers/#02-create-a-multi-layer-dockerfile","title":"02. Create a Multi-Layer Dockerfile","text":"<ul> <li>Let\u2019s create a Dockerfile with several layers to demonstrate how <code>dive</code> works.</li> <li>Each <code>RUN</code> command creates a new layer.</li> </ul> <pre><code># Create the Dockerfile\ncat &lt;&lt; EOF &gt; Dockerfile\n### Layer 00\nFROM alpine\n\n## Layer 01\nWORKDIR /__app\n\n## Layer 02\nRUN echo \"Hello World\" &gt; file1.txt\n\n## Layer 03\nRUN echo \"10.10.10.10\" &gt; /etc/hosts\n\n## Layer 04\nRUN cat /etc/hosts &gt; hosts.txt\n\nEOF\n</code></pre>"},{"location":"009-dive-layers/#03-build-the-image","title":"03. Build the Image","text":"<ul> <li>Build the Docker image from the Dockerfile.</li> </ul> <pre><code># Build the image\ndocker build -t nirgeier/docker-labs-07-dive -f Dockerfile .\n</code></pre>"},{"location":"009-dive-layers/#04-print-the-dockerfile-layers","title":"04. Print the Dockerfile Layers","text":"<ul> <li>Let\u2019s review what each layer does:</li> </ul> <pre><code>echo \"=== Dockerfile Layers ===\"\necho \"Layer 00: FROM alpine\"\necho \"Layer 01: WORKDIR /__app\"\necho \"Layer 02: RUN echo \\\"Hello World\\\" &gt; file1.txt\"\necho \"Layer 03: RUN echo \\\"10.10.10.10\\\" &gt; /etc/hosts\"\necho \"Layer 04: RUN cat /etc/hosts &gt; hosts.txt\"\necho \"=========================\"\n</code></pre>"},{"location":"009-dive-layers/#05-inspect-layers-with-dive","title":"05. Inspect Layers with Dive","text":"<ul> <li>Now, let\u2019s use <code>dive</code> to inspect the image layers.</li> <li><code>dive</code> will analyze the image and show you what\u2019s in each layer.</li> <li>We\u2019ll also generate a JSON output file for further analysis.</li> </ul> <pre><code># Run dive on the image and generate JSON output\ndive nirgeier/docker-labs-07-dive --json output.json\n</code></pre> <ul> <li>When <code>dive</code> runs, you\u2019ll see an interactive interface showing:</li> <li>The layers on the left</li> <li>File changes in each layer on the right</li> <li>Use arrow keys to navigate, and press <code>Ctrl+C</code> to exit</li> </ul>"},{"location":"009-dive-layers/#06-analyze-the-json-output","title":"06. Analyze the JSON Output","text":"<ul> <li>After running <code>dive</code>, we can analyze the <code>output.json</code> file.</li> <li>Let\u2019s use <code>jq</code> to extract information about files changed in layers 2 and above.</li> </ul> <pre><code># Search for files changed in layers starting from index 2\njq '.layer[] | select(.index &gt;= 2) | \"\\(.command) -&gt; \\(.fileList[]?.path)\"' output.json\n</code></pre> <ul> <li>This will show you which files were added or modified in each layer.</li> </ul>"},{"location":"009-dive-layers/#07-clean-up","title":"07. Clean Up","text":"<ul> <li>Remove the created image and files.</li> </ul> <pre><code># Remove the image\ndocker rmi nirgeier/docker-labs-07-dive\n\n# Remove the Dockerfile and output.json\nrm -f Dockerfile output.json\n</code></pre>"},{"location":"010-bake/","title":"010-bake","text":""},{"location":"010-bake/#lab-010-docker-bake","title":"Lab 010 - Docker Bake","text":""},{"location":"010-bake/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Introduction</li> <li>What is Docker <code>Docker Bake</code>?</li> <li>What is Docker <code>BuildKit</code>?</li> <li>Key Features</li> <li>Comparison: Docker Bake vs Docker Compose</li> <li>Docker Bake syntax</li> <li>Common Commands</li> <li>Advanced Features</li> <li>Multi-Stage Builds with Matrix</li> <li>Build Secrets</li> <li>Cache Configuration / Build Caching</li> <li>Cache Configuration / Build Caching Workflow</li> <li>Target Inheritance</li> </ul>"},{"location":"010-bake/#1-introduction","title":"1. Introduction","text":""},{"location":"010-bake/#what-is-docker-docker-bake","title":"What is Docker <code>Docker Bake</code>?","text":"<ul> <li> <p>\ud83d\udc4d <code>Docker Bake</code> is a feature of <code>BuildKit</code> / <code>Docker Buildx</code> that allows you    to define and orchestrate complex builds using a high-level build tool that uses to build Docker images defined in configuration files.</p> </li> <li> <p>\ud83d\udc4d <code>Docker Bake</code> is written in a declarative language (<code>HCL</code>, <code>JSON</code>, or <code>YAML</code>). </p> </li> <li> <p>\ud83d\udc4d <code>Docker Bake</code> is designed for building multiple images or targets concurrently, making it ideal for monorepos, microservices, or any project with several <code>Dockerfiles</code>.</p> </li> <li> <p>\ud83d\udc4d <code>Docker Bake</code> extends beyond single <code>Dockerfile</code>s to support complex build scenarios with multiple targets, platforms, and configurations.</p> </li> <li> <p>\ud83d\udc4d <code>Docker Bake</code> is designed for CI/CD pipelines, multi-platform builds, and complex build workflows.</p> </li> </ul>"},{"location":"010-bake/#what-is-docker-buildkit","title":"What is Docker <code>BuildKit</code>?","text":"<ul> <li> <p>\ud83d\udc4d <code>BuildKit</code> is a modern build subsystem for Docker that improves performance, storage management, and caching  </p> </li> <li> <p>\ud83d\udc4d <code>BuildKit</code> is the engine behind <code>Docker Bake</code>, enabling advanced features like multi-platform builds, caching, and build secrets</p> </li> <li> <p>\ud83d\udc4d <code>BuildKit</code> is the default build engine for Docker, replacing the traditional docker build mechanism.</p> </li> <li> <p>\ud83d\udc4d <code>BuildKit</code> allows for parallel builds, advanced caching, and better resource utilization.</p> </li> <li> <p>\ud83d\udc4d <code>BuildKit</code> supports advanced features like build secrets, SSH forwarding, and more.</p> </li> <li> <p>\ud83d\udc4d <code>BuildKit</code> is a key component of the new Docker Engine 20.10 release, which includes the <code>buildkit</code> subcommand and the <code>docker buildx</code> command.</p> </li> </ul>"},{"location":"010-bake/#key-features","title":"Key Features","text":"Feature Description Advanced BuildKit Features Leverage BuildKit for caching, multi-platform builds, and advanced outputs. Advanced caching Sophisticated caching strategies for faster builds Build matrix Define multiple build configurations and targets Build secrets Securely pass sensitive information during builds Cache configuration Fine-tune caching strategies for faster builds CI/CD integration Ideal for automated build pipelines and workflows Concurrent Builds Build multiple images in parallel with a single command. Custom build arguments Specify different build arguments for each target Custom build contexts Specify different directories for each target Custom Dockerfiles Use different Dockerfiles for each target Declarative Build Configuration Define build targets, groups, arguments, tags, and more in a single file. Declarative syntax Use HCL, JSON, or YAML to define build configurations Extensibility Supports custom build steps and plugins Flexible File Formats Supports HCL (HashiCorp Configuration Language), JSON, and YAML (Compose extension). Group targets Organize multiple build targets into logical groups Inheritance Reuse common configurations across multiple targets Integration with Docker Compose You can use Compose files as Bake files for seamless migration. Matrix Builds Easily define build matrices for different platforms, versions, or configurations. Multi-platform builds Build for multiple architectures simultaneously (ARM, AMD64, etc.) Variable interpolation Use variables and functions for dynamic configurations"},{"location":"010-bake/#comparison-docker-bake-vs-docker-compose","title":"Comparison: Docker Bake vs Docker Compose","text":"Feature Docker Bake Docker Compose Purpose Build orchestration Service orchestration File Format HCL, JSON, YAML (Compose) YAML Parallel Builds Yes No Multi-platform Yes (via BuildKit) Limited Service Deploy No Yes Caching Advanced BuildKit caching Basic Inheritance Yes No CI/CD Ideal for CI/CD pipelines Limited Extensibility Supports custom build steps Limited Advanced Advanced build features Basic <p>[!WARNING]  <code>Docker Bake</code> requires Docker Engine v20.10.0 or higher. <code>Docker Bake</code> requires BuildKit for advanced build features.</p>"},{"location":"010-bake/#docker-bake-syntax","title":"Docker Bake syntax","text":"<p>[!IMPORTANT]  <code>docker-bake.hcl</code> is a declarative build configuration file format.  </p> <ul> <li>Here\u2019s a basic <code>docker-bake.hcl</code> file structure:</li> <li>It uses the HashiCorp Configuration Language (HCL) syntax for defining build configurations.</li> <li>This example defines two targets: <code>frontend</code> and <code>backend-api</code>, each with its own context, Dockerfile, tags, platforms, and build arguments.</li> </ul> <pre><code># Define variables\nvariable \"TAG\" {\n  default = \"latest\"\n}\n\n# Define a user variable for tagging images\n# This can be your Docker Hub username or any identifier\nvariable \"USER\" {\n  default = \"username\"\n}\n\n# Define a group of targets for default builds\ngroup \"default\" {\n  targets = [\"frontend\", \"backend-api\"]\n}\n\n# Frontend service\ntarget \"frontend\" {\n  # Directory containing the frontend code\n  context  = \"./frontend\"           \n\n  # Use a specific Dockerfile for production\n  dockerfile = \"Dockerfile.prod\" \n\n  # Tag the image with user and tag         \n  tags     = [\"${USER}/frontend:${TAG}\"]    \n\n  # Build for multiple architectures\n  platforms  = [\"linux/amd64\", \"linux/arm64\"]   \n\n  # Pass build-time variables to the Dockerfile \n  args = {\n    # Set Node.js environment to production\n    NODE_ENV = \"production\" \n\n    # Set API URL for the frontend\n    API_URL = \"https://api.codewizard.com\"\n  }\n}\n\n# Node.js backend service\ntarget \"backend-api\" {\n  context  = \"./backend\" \n  dockerfile = \"Dockerfile\"\n  tags     = [\"${USER}/backend:${TAG}\"]\n  platforms  = [\"linux/amd64\", \"linux/arm64\"]\n   args = {\n    NODE_ENV = \"production\"\n  }\n}\n</code></pre> <ul> <li>Here is another example of a <code>Docker Bake</code> configuration file: in JSON</li> </ul> <pre><code>{\n  \"variable\": {\n    \"TAG\": {\n      \"default\": \"latest\"\n    },\n    \"USER\": {\n      \"default\": \"username\"\n    }\n  },\n  \"group\": {\n    \"default\": {\n      \"targets\": [\"frontend\", \"backend-api\"]\n    }\n  },\n  \"target\": {\n    \"frontend\": {\n      \"context\": \"./frontend\",\n      \"dockerfile\": \"Dockerfile.prod\",\n      \"tags\": [\"${USER}/frontend:${TAG}\"],\n      \"platforms\": [\"linux/amd64\", \"linux/arm64\"],\n      \"args\": {\n        \"NODE_ENV\": \"production\",\n        \"API_URL\": \"https://api.codewizard.com\"\n      }\n    },\n    \"backend-api\": {\n      \"context\": \"./backend\",\n      \"dockerfile\": \"Dockerfile\",\n      \"tags\": [\"${USER}/backend:${TAG}\"],\n      \"platforms\": [\"linux/amd64\", \"linux/arm64\"],\n      \"args\": {\n        \"NODE_ENV\": \"production\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"010-bake/#common-commands","title":"Common Commands","text":"Feature Description Command Build Default Group Build all targets in the default group <code>docker buildx bake</code> Build Specific Target Build a specific target by name <code>docker buildx bake &lt;target&gt;</code> Build with Custom Variables Override variables for a specific build <code>docker buildx bake --set &lt;target&gt;.&lt;var&gt;=&lt;value&gt;</code> Build and Push Build images and push them to a registry <code>docker buildx bake --push</code> Build for Specific Platform Build images for a specific platform architecture <code>docker buildx bake --platform &lt;platform&gt;</code> View Build Configuration Print the effective build configuration <code>docker buildx bake --print</code> Build with Custom File Use a custom bake file instead of the default <code>docker buildx bake -f &lt;file&gt;</code>"},{"location":"010-bake/#advanced-features","title":"Advanced Features","text":""},{"location":"010-bake/#multi-stage-builds-with-matrix","title":"Multi-Stage Builds with Matrix","text":"<ul> <li>What is a Build Matrix?</li> <li>A build matrix allows you to create multiple image variations from a single configuration. </li> <li>This is useful when you need to build multiple versions of your application or service.</li> <li>For example, you might need to build different versions of your application for different environments (e.g., development, staging, production).</li> <li>By using a build matrix, you can specify multiple combinations of values for each configuration option.</li> <li> <p>Instead of writing separate targets for each combination, Docker Bake automatically generates all possible combinations.</p> </li> <li> <p>Example: Building multiple Node.js versions for different environments</p> </li> </ul> <pre><code>target \"app\" {\n  matrix = {\n    version = [\"18\", \"20\"]\n    env = [\"dev\", \"prod\"]\n  }\n\n  name = \"app-node${version}-${env}\"\n  context = \".\"\n  dockerfile = \"Dockerfile\"\n  tags = [\"myapp:node${version}-${env}\"]\n\n  args = {\n    NODE_VERSION = \"${version}\"\n    NODE_ENV     = \"${env}\"\n  }\n}\n</code></pre> <ul> <li>Quiz: What will be the output of the above example?</li> </ul> Answer <ul> <li>app-node18-dev</li> <li>app-node18-prod</li> <li>app-node20-dev</li> <li>app-node20-prod</li> </ul>    - Each build will use the specified Node.js version and environment.   - The `name` field dynamically constructs the image name based on the matrix values.   - The `args` field passes the Node.js version and environment as build arguments to the Dockerfile.   - The `tags` field tags the built images with the corresponding Node.js version and environment.   - The `context` and `dockerfile` fields specify the build context and Dockerfile to use for all builds.   - The `matrix` field defines the combinations of `version` and `env` to build."},{"location":"010-bake/#build-secrets","title":"Build Secrets","text":"<ul> <li><code>Build secrets</code> allow you to securely pass sensitive information (like API keys, passwords, or certificates) to your Docker build process without including them in the final image or build history.</li> <li>This is particularly useful for keeping sensitive data out of your Docker images and ensuring that it is only available during the build process. </li> <li><code>Build secrets</code> can be stored in a secure location, such as a secure vault or a secret management service, and then referenced during the build process.</li> <li><code>Build secrets</code> are mounted into the build environment at runtime, allowing you to use them without exposing them in the final image.</li> <li><code>Build secrets</code> are not stored in the final image layers, ensuring that sensitive information is not leaked.</li> <li><code>Build secrets</code> are only available during the build process and are not accessible to the final image or container.</li> <li><code>Build secrets</code> are defined in the <code>docker-bake.hcl</code> file using the <code>secret</code> field.</li> <li>You can use the <code>--secret</code> flag to pass secrets to the build process.</li> <li> <p>Secrets can also be defined in the Dockerfile using the <code>RUN --mount=type=secret</code> syntax.</p> </li> <li> <p>Example: Using a secret file during build</p> </li> </ul> <pre><code>target \"secure-app\" {\n  context = \".\"\n  dockerfile = \"Dockerfile\"\n  secret = [\n    \"id=mysecret,src=./secret.txt\"\n  ]\n  tags = [\"myapp:secure\"]\n}\n</code></pre> <ul> <li><code>Dockerfile</code>:</li> </ul> <pre><code>FROM node:18\n\n# Use the secret during build (it wont be in the final image)\n# as mentioned and explained above\n\nRUN --mount=type=secret,id=mysecret \\\n    API_KEY=$(cat /run/secrets/mysecret) &amp;&amp; \\\n    npm install --registry=https://registry.example.com\n</code></pre>"},{"location":"010-bake/#cache-configuration-build-caching","title":"Cache Configuration / Build Caching","text":"<ul> <li><code>Docker Bake</code> supports advanced caching strategies to speed up builds and reduce resource usage.</li> <li>Caching allows you to reuse previously built layers, which can significantly speed up the build process</li> <li><code>Docker Bake</code> can use different cache sources, such as local files, remote registries, or GitHub Actions cache.</li> <li>Caching can be configured using the <code>cache-from</code> and <code>cache-to</code> fields in the <code>docker-bake.hcl</code> file.</li> <li>Caching can also be configured using the <code>--cache-from</code> and <code>--cache-to</code> flags when running the <code>docker buildx bake</code> command.</li> <li>Caching can be used to speed up builds by reusing previously built layers, which can significantly reduce build times.</li> <li>Caching can also be used to reduce resource usage by avoiding unnecessary rebuilds of unchanged layers.</li> <li>Caching can be used to share build layers between different builds, which can further speed up the build process.</li> <li>Caching can be used to optimize the build process by reusing previously built layers, which can significantly reduce build times.</li> <li>Caching can reduce build times from minutes to seconds for unchanged code.</li> <li>Example: Using GitHub Actions cache</li> </ul> <pre><code>target \"cached-app\" {\n  context = \".\"\n  dockerfile = \"Dockerfile\"\n  cache-from = [\"type=gha\"]\n  cache-to = [\"type=gha,mode=max\"]\n  tags = [\"myapp:cached\"]\n}\n</code></pre> <p>Cache Types:</p> Type Description <code>type=gha</code> GitHub Actions cache (for CI/CD) <code>type=local</code> Local filesystem cache <code>type=registry</code> Docker registry cache"},{"location":"010-bake/#cache-configuration-build-caching-workflow","title":"Cache Configuration / Build Caching Workflow","text":"<p><code>mermaid flowchart TD     A[First build] --&gt;|Takes full time, saves cache| B[Second build]     B --&gt;|Reuses cached layers, builds only changed parts| C[Result]     C --&gt;|Significantly faster build times| D[End]</code></p>"},{"location":"010-bake/#target-inheritance","title":"Target Inheritance","text":"<ul> <li><code>Docker Bake</code> allows you to create a base configuration and reuse it across multiple targets. </li> <li>This follows the DRY (Don\u2019t Repeat Yourself) principle - define common settings once and inherit them.</li> </ul> <p>Example: Sharing common settings across multiple builds</p> Setting Description <code>target \"base\"</code> This is a base target that defines common build settings <code>context = \".\"</code> Uses the current directory as the build context <code>dockerfile = \"Dockerfile\"</code> Uses the default Dockerfile <code>platforms = [\"linux/amd64\", \"linux/arm64\"]</code> Builds for both AMD64 and ARM64 architectures <code>tags = [\"myapp:latest\"]</code> Sets the default tag for the image <code>cache-from = [\"type=gha\"]</code> Uses GitHub Actions cache for the build <code>cache-to = [\"type=gha,mode=max\"]</code> Saves the cache to GitHub Actions cache with <pre><code># Base target with common settings\ntarget \"base\" {\n  context = \".\"\n  dockerfile = \"Dockerfile\"\n  platforms = [\"linux/amd64\", \"linux/arm64\"]\n  tags = [\"myapp:latest\"]\n  cache-from = [\"type=gha\"]\n  cache-to = [\"type=gha,mode=max\"]\n}\n\n# Example: Using inheritance to define common settings (1)\ntarget \"web\" {\n  inherits = [\"base\"]\n  tags = [\"myapp/web:latest\"]\n  target = \"web\"\n}\n\n# Example: Using inheritance to define common settings (2)\ntarget \"api\" {\n  inherits = [\"base\"]\n  tags = [\"myapp/api:latest\"]\n  target = \"api\"\n}\n</code></pre> <p>How it works:</p> <p><code>mermaid flowchart TD     A[Base target] --&gt;|Defines common settings | B[Child targets]     B --&gt;|Inherit from base and add their specific settings| C[Result]     C --&gt;|Both web and api targets automatically get the same platforms and build context| D[End]</code></p> <p>Benefits:</p> <ul> <li>Consistency: Sharing - ensures consistent build configurations</li> <li>Reusability: All targets use the same base configuration</li> <li>Maintainability: Supports multiple targets with the same settings platforms in one place, affects all children</li> <li>Cleaner code: No repetitive configuration</li> <li>DRY principle: Avoids duplication of common settings</li> <li>Flexibility: Each target can still have its own specific settings</li> <li>Scalability: Easily add new targets with shared settings</li> </ul>"},{"location":"011-Security%26Trust/","title":"011-Security&Trust","text":""},{"location":"011-Security%26Trust/#lab-011-docker-security-trust","title":"Lab 011 - Docker Security &amp; Trust","text":"<ul> <li>This lab covers advanced Docker security features and best practices for container security and privilege management.</li> <li>You\u2019ll learn about reducing the attack surface, implementing the Principle of Least Privilege, and using Docker\u2019s security mechanisms to protect your containers.</li> <li>Topics include non-root user execution, Linux capabilities, security options, and mandatory access control.</li> <li>By the end of this lab, you\u2019ll understand how to build and run secure Docker containers.</li> </ul>"},{"location":"011-Security%26Trust/#table-of-contents","title":"Table of Contents","text":"<ul> <li>\ud83d\udee1\ufe0f Non-Root User Execution with the <code>USER</code> Instruction</li> <li>How to Use <code>USER</code></li> <li>\ud83d\udd12 Advanced Security Issues and Features</li> <li>1. Privileged Containers (<code>--privileged</code>)</li> <li>2. Linux Capabilities (<code>--cap-drop</code>, <code>--cap-add</code>)</li> <li>3. Preventing Privilege Escalation</li> <li>4. Mandatory Access Control (MAC)</li> <li>5. User Namespace Remapping (UserNS)</li> <li>\ud83d\udd11 Summary of Security Best Practices</li> </ul> <ul> <li>Docker\u2019s advanced features offer critical mechanisms for container security and privilege management, primarily revolving around reducing the attack surface. </li> <li>The key is to adhere to the Principle of Least Privilege (PoLP).</li> </ul>"},{"location":"011-Security%26Trust/#non-root-user-execution-with-the-user-instruction","title":"\ud83d\udee1\ufe0f Non-Root User Execution with the <code>USER</code> Instruction","text":"<ul> <li>By default, the process inside a Docker container runs as the <code>root user (UID 0)</code>, which is a major security risk. </li> <li>If an attacker successfully exploits a vulnerability in the application, they gain root access inside the container. </li> <li>Depending on the container\u2019s configuration and any underlying kernel vulnerabilities, this could potentially lead to a container breakout and root access to the host machine.</li> <li>The <code>USER</code> instruction in a <code>Dockerfile</code> is essential for mitigating this risk:</li> </ul>"},{"location":"011-Security%26Trust/#how-to-use-user","title":"How to Use <code>USER</code>","text":"<ol> <li>Create a Non-Root User</li> <li>Use a <code>RUN</code> instruction to create a dedicated user and group before switching the user.     <pre><code># Create an unprivileged user (e.g., 'appuser' with UID 1001)\nRUN adduser -D appuser\n\n# Set the ownership of the application directory to the new user\nRUN mkdir /app &amp;&amp; chown -R appuser:appuser /app\n\n# Switch to the non-root user for all subsequent instructions and runtime\nUSER appuser \n\nWORKDIR /app\n# ... rest of your application commands (e.g., CMD)\n</code></pre></li> </ol> <p>adduser -D</p> <p>Using <code>adduser -D</code> (on Alpine) creates a system user without a password or home directory, which is more secure.</p>"},{"location":"011-Security%26Trust/#best-practices-for-permissions","title":"Best Practices for Permissions:","text":""},{"location":"011-Security%26Trust/#file-permissions","title":"File Permissions","text":"<ul> <li>Ensure the non-root user has the necessary read, write, and execute permissions for all files, directories, or temporary spaces the application needs. </li> <li>This often requires running <code>chown</code> in the Dockerfile.</li> </ul>"},{"location":"011-Security%26Trust/#multi-stage-builds","title":"Multi-Stage Builds","text":"<ul> <li>Use multi-stage builds to perform privileged operations (like installing system packages with <code>apt</code> or <code>yum</code>) in a \u201cbuilder\u201d stage run as root, and then copy only the necessary artifacts into a minimal, non-root \u201cruntime\u201d stage. </li> <li>This prevents residual root tools or sensitive build-time files from existing in the final image.</li> </ul>"},{"location":"011-Security%26Trust/#bind-ports-1024","title":"Bind Ports &gt; 1024","text":"<ul> <li>Only the root user can bind to privileged ports (0-1023). </li> <li>If your application runs as non-root, it must bind to ports 1024 or higher (e.g., port 8080).</li> </ul>"},{"location":"011-Security%26Trust/#user","title":"User","text":"<ul> <li>Always specify a non-root user with the <code>USER</code> instruction in your Dockerfile to prevent running as root.</li> <li>Avoid using <code>sudo</code> inside containers; instead, configure permissions properly at build time.</li> <li>Consider using well-known non-root users like</li> </ul>"},{"location":"011-Security%26Trust/#privileged-containers","title":"Privileged Containers","text":"<ul> <li>Avoid running containers with the <code>--privileged</code> flag, as it grants excessive permissions and can lead to host compromise.</li> <li>Instead, use specific capability additions (<code>--cap-add</code>) if certain elevated privileges are necessary.</li> <li>Always review and minimize the capabilities your container needs.</li> <li>Consider using tools like <code>seccomp</code> to restrict system calls your container can make.</li> </ul>"},{"location":"011-Security%26Trust/#advanced-security-issues-and-features","title":"\ud83d\udd12 Advanced Security Issues and Features","text":"<ul> <li>Beyond running as a non-root user, Docker provides several advanced features to lock down container security:</li> </ul>"},{"location":"011-Security%26Trust/#1-privileged-containers-privileged","title":"1. Privileged Containers (<code>--privileged</code>)","text":"<ul> <li> <p>A privileged container is the most dangerous security configuration and should be avoided at all costs unless absolutely necessary (e.g., for running Docker-in-Docker or a tool that needs to interact directly with the host kernel).</p> </li> <li> <p>Security Issue: </p> <ul> <li>Running a container with the <code>--privileged</code> flag grants it all Linux Capabilities (see below) and allows it to access all devices on the host. In a privileged container, the root user inside the container is essentially equivalent to the root user on the host machine. A container breakout is almost guaranteed.</li> <li>Mitigation: NEVER use <code>--privileged</code> in production.    Instead, identify the specific capabilities your application needs and grant only those using the <code>--cap-add</code> flag.</li> </ul> </li> </ul>"},{"location":"011-Security%26Trust/#2-linux-capabilities-cap-drop-cap-add","title":"2. Linux Capabilities (<code>--cap-drop</code>, <code>--cap-add</code>)","text":"<ul> <li>Linux Capabilities are a finer-grained way to manage root permissions. </li> <li>Traditional Unix divides processes into two categories: root (UID 0) and unprivileged. </li> <li> <p>Capabilities break down the powers of the root user into discrete units.</p> </li> <li> <p>By default, Docker grants a set of \u201csafe\u201d capabilities and drops dangerous ones.</p> </li> <li>Best Practice: Use the <code>--cap-drop=ALL</code> flag to remove all capabilities, then use <code>--cap-add</code> to grant only the handful that the application truly requires (PoLP).     <pre><code># Runs the container with no capabilities except NET_BIND_SERVICE\n# A container that needs to bind to a low-numbered port (e.g., port 80) \n# only needs the `NET_BIND_SERVICE` capability.\ndocker run --cap-drop=ALL --cap-add=NET_BIND_SERVICE my-image\n</code></pre></li> </ul>"},{"location":"011-Security%26Trust/#3-preventing-privilege-escalation","title":"3. Preventing Privilege Escalation","text":"<ul> <li> <p>Even if a container starts as non-root, a malicious process could attempt to escalate its privileges.</p> </li> <li> <p><code>no-new-privileges</code>: Use the <code>--security-opt=no-new-privileges</code> flag to prevent a process from gaining new privileges via mechanisms like <code>setuid</code> or <code>setgid</code> binaries. This is a crucial security layer.</p> </li> </ul>"},{"location":"011-Security%26Trust/#4-mandatory-access-control-mac","title":"4. Mandatory Access Control (MAC)","text":"<ul> <li>Docker integrates with two major Linux security modules for deeper kernel-level protection:</li> </ul> Feature Description Purpose Seccomp (Secure Computing) Filters which system calls (syscalls) a container process can make to the Linux kernel. Blocks dangerous syscalls that could be used for container breakout, even if the process is running as root. Docker uses a restrictive default profile. AppArmor (Application Armor) Creates Mandatory Access Control (MAC) profiles that limit what files, network resources, and other capabilities a container can access. Provides an extra layer of defense-in-depth by confining the container process\u2019s access to host resources. Docker loads a moderately protective <code>docker-default</code> profile."},{"location":"011-Security%26Trust/#5-user-namespace-remapping-userns","title":"5. User Namespace Remapping (UserNS)","text":"<ul> <li> <p>This is one of the strongest isolation features for mitigating the risk of a container breakout.</p> </li> <li> <p>How it Works: It maps the root user (UID 0) inside the container to an unprivileged user (a high UID, e.g., 100000) on the host machine.</p> </li> <li>Security Benefit: If an attacker does manage to escape the container, they only have the privileges of the unprivileged mapped user on the host, not actual root access. This significantly reduces the potential for host compromise. This feature can be enabled at the Docker daemon level.</li> </ul>"},{"location":"011-Security%26Trust/#summary-of-security-best-practices","title":"\ud83d\udd11 Summary of Security Best Practices","text":"Security Principle Docker Feature/Instruction Impact Least Privilege <code>USER non-root</code> in <code>Dockerfile</code> Prevents an exploit from gaining root access inside the container. Runtime Hardening <code>--cap-drop=ALL</code>, <code>--cap-add=&lt;needed-caps&gt;</code> Removes unnecessary superuser powers, reducing the attack surface. Kernel Isolation Seccomp/AppArmor Limits the dangerous operations (syscalls, file access) a process can perform. No Escalation <code>--security-opt=no-new-privileges</code> Prevents a non-root process from gaining root powers during execution. Host Protection User Namespace Remapping Ensures that an escaped container process runs as an unprivileged user on the host."},{"location":"011-Security%26Trust/#running-the-demos","title":"Running the Demos","text":"<p>This lab includes individual demo scripts for each security topic:</p> Demo Script Description <code>demo1-nonroot.sh</code> Demonstrates non-root user execution <code>demo2-capabilities.sh</code> Shows Linux capabilities management <code>demo3-security-options.sh</code> Demonstrates security options like no-new-privileges <code>demo4-mac.sh</code> Checks Mandatory Access Control (Seccomp/AppArmor) status <code>demo5-userns.sh</code> Explores User Namespace Remapping configuration <p>To run a demo:</p> <pre><code>chmod +x demo1-nonroot.sh\n./demo1-nonroot.sh\n</code></pre> <p>Each demo is self-contained with automatic cleanup.</p>"},{"location":"012-gvisor/","title":"012 - gVisor Lab","text":"<ul> <li>This lab demonstrates the use of gVisor, a sandbox runtime for containers that provides an additional layer of security by intercepting and filtering system calls.</li> </ul>"},{"location":"012-gvisor/#overview","title":"Overview","text":"<ul> <li><code>gVisor</code> is an application <code>kernel</code>, written in Go, that implements a substantial portion of the Linux system call interface. </li> <li>It provides a strong isolation boundary between the application and the host kernel, making it harder for attackers to compromise the host system even if they gain control of a container.</li> </ul>"},{"location":"012-gvisor/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed</li> <li>gVisor runtime installed (<code>runsc</code>)</li> <li>Basic understanding of Docker and system calls</li> </ul>"},{"location":"012-gvisor/#installation","title":"Installation","text":"<ul> <li>To install gVisor:</li> </ul> <pre><code># Install gVisor\ncurl -fsSL https://gvisor.dev/archive.key | sudo gpg --dearmor -o /usr/share/keyrings/gvisor-archive-keyring.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/gvisor-archive-keyring.gpg] https://storage.googleapis.com/gvisor/releases release main\" | sudo tee /etc/apt/sources.list.d/gvisor.list &gt; /dev/null\nsudo apt-get update &amp;&amp; sudo apt-get install -y runsc\n</code></pre>"},{"location":"012-gvisor/#examples","title":"Examples","text":""},{"location":"012-gvisor/#example-1-blocking-user-creation","title":"Example 1: Blocking User Creation","text":"<ul> <li> <p>This example demonstrates how to use gVisor with a <code>seccomp</code> profile to block user creation syscalls.</p> <p>Files:</p> <ul> <li><code>demo.sh</code>: Script to run the demo</li> <li><code>block-user-creation.json</code>: Seccomp profile that blocks user-related syscalls</li> </ul> <p>Run the demo:</p> <pre><code>./demo.sh\n</code></pre> </li> <li> <p>This will attempt to create a user inside a container running with gVisor and the seccomp profile. </p> </li> <li>The operation should fail, demonstrating the security isolation.</li> </ul>"},{"location":"012-gvisor/#example-2-blocking-mount-operations","title":"Example 2: Blocking Mount Operations","text":"<ul> <li>This example shows how to restrict mount operations using gVisor and seccomp.</li> </ul> <p>Files:</p> <ul> <li><code>demo-mount.sh</code>: Script to run the mount demo</li> <li><code>block-mount.json</code>: Seccomp profile that blocks mount-related syscalls</li> <li><code>Dockerfile</code>: Alpine image that attempts to mount tmpfs</li> </ul> <p>Run the demo:</p> <pre><code>./demo-mount.sh\n</code></pre> <ul> <li>This will build an Alpine image and attempt to mount a <code>tmpfs</code> inside the container. </li> <li>The mount operation should be blocked.</li> </ul>"},{"location":"012-gvisor/#key-concepts","title":"Key Concepts","text":"<ul> <li>Seccomp Profiles: JSON files that define which syscalls are allowed or blocked</li> <li>gVisor Runtime: <code>runsc</code> provides the sandboxed execution environment</li> <li>System Call Filtering: Prevents potentially dangerous operations</li> </ul>"},{"location":"012-gvisor/#additional-resources","title":"Additional Resources","text":"<ul> <li>gVisor Documentation</li> <li>Seccomp Profiles</li> <li>Docker Runtime Options</li> </ul>"},{"location":"013-Cgroup/","title":"013-Cgroup","text":""},{"location":"013-Cgroup/#lab-013-resource-isolation-with-linux-cgroups","title":"Lab 013 - Resource Isolation with Linux Cgroups","text":"<ul> <li>This lab covers resource isolation using Linux Cgroups in Docker containers.</li> <li>You will learn how to limit CPU, memory, and other resources for containers to prevent resource contention and ensure fair sharing.</li> <li>By the end of this lab, you will understand how to use Docker commands to enforce resource constraints.</li> </ul>"},{"location":"013-Cgroup/#cpu-limits","title":"CPU Limits","text":"<ul> <li>Use <code>--cpus</code> to limit CPU usage as a fraction of available cores.</li> </ul> <pre><code># Limit container to 0.5 CPU cores\ndocker run -d --cpus=0.5 --name cpu-limited nginx\n\n# Check resource usage\ndocker stats cpu-limited\n\n# Clean up\ndocker stop cpu-limited\ndocker rm cpu-limited\n</code></pre>"},{"location":"013-Cgroup/#memory-limits","title":"Memory Limits","text":"<ul> <li>Use <code>--memory</code> to set a hard memory limit.</li> </ul> <pre><code># Limit container to 128MB RAM\ndocker run -d --memory=128m --name mem-limited nginx\n\n# Check resource usage\ndocker stats mem-limited\n\n# Test memory limit by running a memory-intensive process\ndocker run --memory=50m stress --vm 1 --vm-bytes 100m\n\n# Clean up\ndocker stop mem-limited\ndocker rm mem-limited\n</code></pre>"},{"location":"013-Cgroup/#cpu-sets","title":"CPU Sets","text":"<ul> <li>Use <code>--cpuset-cpus</code> to pin container to specific CPU cores.</li> </ul> <pre><code># Pin container to CPU cores 0 and 1\ndocker run -d --cpuset-cpus=0,1 --name cpu-pinned nginx\n\n# Check which CPUs the container is using\ndocker exec cpu-pinned cat /proc/self/status | grep Cpus_allowed\n\n# Clean up\ndocker stop cpu-pinned\ndocker rm cpu-pinned\n</code></pre>"},{"location":"013-Cgroup/#combined-resource-limits","title":"Combined Resource Limits","text":"<ul> <li>Combine multiple resource constraints for comprehensive control.</li> </ul> <pre><code># Limit both CPU and memory\ndocker run -d --cpus=1.0 --memory=256m --name combined-limits nginx\n\n# Check all resource limits\ndocker inspect combined-limits | grep -A 10 \"HostConfig\"\n\n# Clean up\ndocker stop combined-limits\ndocker rm combined-limits\n</code></pre>"},{"location":"013-Cgroup/#monitoring-resource-usage","title":"Monitoring Resource Usage","text":"<ul> <li>Use <code>docker stats</code> to monitor container resource usage in real-time.</li> </ul> <pre><code># Start multiple containers with different limits\ndocker run -d --cpus=0.5 --memory=128m --name container1 nginx\ndocker run -d --cpus=1.0 --memory=256m --name container2 nginx\n\n# Monitor resource usage\ndocker stats\n\n# Clean up\ndocker stop container1 container2\ndocker rm container1 container2\n</code></pre>"},{"location":"014-DockerDaemon/","title":"014-DockerDaemon","text":""},{"location":"014-DockerDaemon/#lab-014-docker-daemon","title":"Lab 014 - Docker Daemon","text":"<ul> <li>This lab covers the Docker daemon (dockerd), its configuration, and advanced features.</li> <li>You\u2019ll learn how to customize the Docker daemon behavior, configure private registries, enable rootless mode, and implement various security and performance optimizations.</li> <li>Topics include daemon configuration file, logging options, storage drivers, network settings, and experimental features.</li> <li>By the end of this lab, you\u2019ll understand how to configure and manage the Docker daemon for production environments.</li> </ul>"},{"location":"014-DockerDaemon/#table-of-contents","title":"Table of Contents","text":"<ul> <li>\ud83d\udc33 Understanding the Docker Daemon</li> <li>\u2699\ufe0f Docker Daemon Configuration</li> <li>Configuration File Location</li> <li>Basic Configuration Structure</li> <li>\ud83d\udd27 Common Daemon Configurations</li> <li>Logging Configuration</li> <li>Storage Configuration</li> <li>Network Configuration</li> <li>Security Configuration</li> <li>\ud83c\udfe2 Private Registry Configuration</li> <li>\ud83d\udc64 Rootless Docker</li> <li>\ud83d\ude80 Experimental Features</li> <li>\ud83d\udcca Monitoring and Debugging</li> <li>\ud83d\udd12 Security Best Practices</li> </ul>"},{"location":"014-DockerDaemon/#understanding-the-docker-daemon","title":"\ud83d\udc33 Understanding the Docker Daemon","text":"<p>The Docker daemon (<code>dockerd</code>) is the persistent process that manages Docker containers, images, networks, and volumes. It listens for Docker API requests and manages Docker objects.</p>"},{"location":"014-DockerDaemon/#key-responsibilities","title":"Key Responsibilities","text":"<ul> <li>Container Management: Creating, starting, stopping, and monitoring containers</li> <li>Image Management: Pulling, pushing, and building images</li> <li>Network Management: Creating and managing container networks</li> <li>Volume Management: Handling persistent data storage</li> <li>API Server: Providing REST API for Docker client communication</li> </ul>"},{"location":"014-DockerDaemon/#daemon-lifecycle","title":"Daemon Lifecycle","text":"<pre><code># Check if daemon is running\ndocker version\n\n# View daemon info\ndocker info\n\n# Restart daemon (Linux)\nsudo systemctl restart docker\n\n# View daemon logs (Linux)\nsudo journalctl -u docker -f\n</code></pre>"},{"location":"014-DockerDaemon/#docker-daemon-configuration","title":"\u2699\ufe0f Docker Daemon Configuration","text":""},{"location":"014-DockerDaemon/#configuration-file-location","title":"Configuration File Location","text":"<p>The Docker daemon can be configured using a JSON configuration file:</p> <p>Linux/macOS: <code>/etc/docker/daemon.json</code> Windows: <code>C:\\ProgramData\\docker\\config\\daemon.json</code></p>"},{"location":"014-DockerDaemon/#basic-configuration-structure","title":"Basic Configuration Structure","text":"<pre><code>{\n  \"debug\": false,\n  \"tls\": true,\n  \"tlscert\": \"/var/docker/server.pem\",\n  \"tlskey\": \"/var/docker/serverkey.pem\",\n  \"hosts\": [\"tcp://0.0.0.0:2376\"],\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"10m\",\n    \"max-file\": \"3\"\n  },\n  \"storage-driver\": \"overlay2\",\n  \"insecure-registries\": [\"myregistry.com:5000\"],\n  \"registry-mirrors\": [\"https://mirror.gcr.io\"]\n}\n</code></pre>"},{"location":"014-DockerDaemon/#common-daemon-configurations","title":"\ud83d\udd27 Common Daemon Configurations","text":""},{"location":"014-DockerDaemon/#logging-configuration","title":"Logging Configuration","text":"<p>Configure how Docker logs container output and daemon events:</p> <pre><code>{\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"10m\",\n    \"max-file\": \"3\",\n    \"labels\": \"production_status\",\n    \"env\": \"os,customer\"\n  }\n}\n</code></pre> <p>Available log drivers:</p> <ul> <li><code>json-file</code> (default): JSON formatted logs</li> <li><code>syslog</code>: System logging</li> <li><code>journald</code>: systemd journal</li> <li><code>fluentd</code>: Fluentd logging</li> <li><code>awslogs</code>: Amazon CloudWatch</li> <li><code>splunk</code>: Splunk logging</li> </ul>"},{"location":"014-DockerDaemon/#storage-configuration","title":"Storage Configuration","text":"<p>Configure storage driver and options:</p> <pre><code>{\n  \"storage-driver\": \"overlay2\",\n  \"storage-opts\": [\n    \"overlay2.override_kernel_check=true\"\n  ],\n  \"data-root\": \"/var/lib/docker\"\n}\n</code></pre> <p>Common storage drivers:</p> <ul> <li><code>overlay2</code> (recommended for modern Linux)</li> <li><code>btrfs</code> (for Btrfs filesystems)</li> <li><code>zfs</code> (for ZFS filesystems)</li> <li><code>devicemapper</code> (legacy, device mapper)</li> </ul>"},{"location":"014-DockerDaemon/#network-configuration","title":"Network Configuration","text":"<p>Configure networking options:</p> <pre><code>{\n  \"bridge\": \"docker0\",\n  \"fixed-cidr\": \"192.168.65.0/24\",\n  \"default-gateway\": \"192.168.65.1\",\n  \"dns\": [\"8.8.8.8\", \"8.8.4.4\"],\n  \"dns-opts\": [\"timeout:2\"],\n  \"dns-search\": [\"example.com\"],\n  \"iptables\": true,\n  \"ip-forward\": true\n}\n</code></pre>"},{"location":"014-DockerDaemon/#security-configuration","title":"Security Configuration","text":"<pre><code>{\n  \"userns-remap\": \"default\",\n  \"no-new-privileges\": true,\n  \"seccomp-profile\": \"/etc/docker/seccomp.json\",\n  \"selinux-enabled\": true,\n  \"live-restore\": true,\n  \"icc\": false,\n  \"userland-proxy\": false\n}\n</code></pre>"},{"location":"014-DockerDaemon/#private-registry-configuration","title":"\ud83c\udfe2 Private Registry Configuration","text":""},{"location":"014-DockerDaemon/#adding-insecure-registries","title":"Adding Insecure Registries","text":"<p>For registries without TLS certificates:</p> <pre><code>{\n  \"insecure-registries\": [\n    \"myregistry.com:5000\",\n    \"registry.internal.company.com\"\n  ]\n}\n</code></pre>"},{"location":"014-DockerDaemon/#registry-mirrors","title":"Registry Mirrors","text":"<p>Use registry mirrors to cache images:</p> <pre><code>{\n  \"registry-mirrors\": [\n    \"https://mirror.gcr.io\",\n    \"https://dockerhub.mirror.com\"\n  ]\n}\n</code></pre>"},{"location":"014-DockerDaemon/#authentication","title":"Authentication","text":"<p>Configure authentication for private registries:</p> <pre><code>{\n  \"auths\": {\n    \"https://index.docker.io/v1/\": {\n      \"auth\": \"dXNlcjpwYXNzd29yZA==\"\n    },\n    \"myregistry.com:5000\": {\n      \"auth\": \"dXNlcjpwYXNzd29yZA==\"\n    }\n  }\n}\n</code></pre>"},{"location":"014-DockerDaemon/#example-working-with-private-registry","title":"Example: Working with Private Registry","text":"<pre><code># Tag image for private registry\ndocker tag myapp:latest myregistry.com:5000/myapp:v1.0\n\n# Push to private registry\ndocker push myregistry.com:5000/myapp:v1.0\n\n# Pull from private registry\ndocker pull myregistry.com:5000/myapp:v1.0\n\n# Login to registry (if required)\ndocker login myregistry.com:5000\n</code></pre>"},{"location":"014-DockerDaemon/#rootless-docker","title":"\ud83d\udc64 Rootless Docker","text":"<p>Rootless Docker allows running the Docker daemon without root privileges, improving security by reducing the attack surface.</p>"},{"location":"014-DockerDaemon/#installation","title":"Installation","text":"<pre><code># Install rootless Docker\ncurl -fsSL https://get.docker.com/rootless | sh\n\n# Start rootless Docker\nsystemctl --user start docker\n\n# Enable on boot\nsystemctl --user enable docker\n\n# Add to PATH\nexport PATH=/home/$USER/bin:$PATH\nexport DOCKER_HOST=unix:///run/user/$(id -u)/docker.sock\n</code></pre>"},{"location":"014-DockerDaemon/#configuration","title":"Configuration","text":"<p>Rootless Docker uses different paths and configurations:</p> <pre><code># Rootless Docker socket\nexport DOCKER_HOST=unix:///run/user/$(id -u)/docker.sock\n\n# Rootless Docker data directory\nexport XDG_DATA_HOME=/home/$USER/.local/share\n\n# Rootless Docker config\nexport DOCKER_CONFIG=/home/$USER/.config/docker\n</code></pre>"},{"location":"014-DockerDaemon/#limitations","title":"Limitations","text":"<ul> <li>Some features may not work (e.g., AppArmor, checkpoint/restore)</li> <li>Port binding below 1024 requires additional setup</li> <li>Some storage drivers may have limitations</li> <li>Network features may be restricted</li> </ul>"},{"location":"014-DockerDaemon/#benefits","title":"Benefits","text":"<ul> <li>Security: No root access required</li> <li>Isolation: User-specific Docker environment</li> <li>Compliance: Meets security requirements for multi-tenant environments</li> </ul>"},{"location":"014-DockerDaemon/#experimental-features","title":"\ud83d\ude80 Experimental Features","text":"<p>Enable experimental features for cutting-edge functionality:</p> <pre><code>{\n  \"experimental\": true\n}\n</code></pre> <p>Available experimental features:</p> <ul> <li>BuildKit: Advanced build engine with improved performance</li> <li>Squash: Squash layers to reduce image size</li> <li>Checkpoint/Restore: Save and restore container state</li> <li>Rootless mode: Run daemon without root (now stable)</li> </ul>"},{"location":"014-DockerDaemon/#buildkit-configuration","title":"BuildKit Configuration","text":"<pre><code>{\n  \"experimental\": true,\n  \"features\": {\n    \"buildkit\": true\n  }\n}\n</code></pre> <p>Using BuildKit:</p> <pre><code># Enable BuildKit\nexport DOCKER_BUILDKIT=1\n\n# Build with BuildKit\ndocker build -t myapp .\n\n# Use advanced BuildKit features\ndocker build --target production -t myapp .\n</code></pre>"},{"location":"014-DockerDaemon/#monitoring-and-debugging","title":"\ud83d\udcca Monitoring and Debugging","text":""},{"location":"014-DockerDaemon/#daemon-monitoring","title":"Daemon Monitoring","text":"<pre><code># View daemon info\ndocker info\n\n# System-wide information\ndocker system info\n\n# Disk usage\ndocker system df\n\n# Events stream\ndocker events\n\n# Daemon logs\nsudo journalctl -u docker -f\n</code></pre>"},{"location":"014-DockerDaemon/#debugging-configuration","title":"Debugging Configuration","text":"<pre><code>{\n  \"debug\": true,\n  \"log-level\": \"debug\",\n  \"metrics-addr\": \"127.0.0.1:9323\"\n}\n</code></pre>"},{"location":"014-DockerDaemon/#health-checks","title":"Health Checks","text":"<p>Monitor daemon health:</p> <pre><code># Check daemon responsiveness\ndocker version\n\n# System events\ndocker system events --since 1h\n\n# Container events\ndocker events --filter type=container\n</code></pre>"},{"location":"014-DockerDaemon/#security-best-practices","title":"\ud83d\udd12 Security Best Practices","text":"<p>Always use TLS for daemon communication:</p> <pre><code>{\n  \"tls\": true,\n  \"tlscert\": \"/etc/docker/server.pem\",\n  \"tlskey\": \"/etc/docker/serverkey.pem\",\n  \"tlsverify\": true\n}\n</code></pre> <p>Enable user namespace remapping:</p> <pre><code>{\n  \"userns-remap\": \"default\"\n}\n</code></pre> <p>Use custom seccomp profiles:</p> <pre><code>{\n  \"seccomp-profile\": \"/etc/docker/seccomp.json\"\n}\n</code></pre> <p>Enable mandatory access control:</p> <pre><code>{\n  \"selinux-enabled\": true\n}\n</code></pre> <p>Enable detailed audit logging:</p> <pre><code>{\n  \"log-driver\": \"syslog\",\n  \"log-opts\": {\n    \"syslog-facility\": \"daemon\"\n  }\n}\n</code></pre> <p>Avoid insecure configurations:</p> <pre><code>{\n  \"icc\": false,\n  \"no-new-privileges\": true,\n  \"userland-proxy\": false\n}\n</code></pre>"},{"location":"014-DockerDaemon/#tls-configuration","title":"\ud83d\udd10 TLS Configuration","text":""},{"location":"014-DockerDaemon/#user-namespace","title":"\ud83d\udc65 User Namespace","text":""},{"location":"014-DockerDaemon/#seccomp-profiles","title":"\ud83d\udee1\ufe0f Seccomp Profiles","text":""},{"location":"014-DockerDaemon/#selinuxapparmor","title":"\ud83d\udd12 SELinux/AppArmor","text":""},{"location":"014-DockerDaemon/#audit-logging","title":"\ud83d\udcca Audit Logging","text":""},{"location":"014-DockerDaemon/#disable-insecure-features","title":"\ud83d\udeab Disable Insecure Features","text":""},{"location":"014-DockerDaemon/#lab-exercises","title":"\ud83d\udccb Lab Exercises","text":"<ol> <li> <p>Configure Basic Daemon Settings</p> <ul> <li>Create <code>/etc/docker/daemon.json</code> with basic configuration</li> <li>Restart Docker daemon and verify settings</li> </ul> </li> <li> <p>Set Up Private Registry</p> <ul> <li>Configure insecure registry in daemon.json</li> <li>Push and pull images from private registry</li> </ul> </li> <li> <p>Enable Rootless Docker</p> <ul> <li>Install and configure rootless Docker</li> <li>Test container operations without root</li> </ul> </li> <li> <p>Configure Logging</p> <ul> <li>Set up JSON logging with rotation</li> <li>View and analyze container logs</li> </ul> </li> <li> <p>Security Hardening</p> <ul> <li>Enable user namespaces</li> <li>Configure seccomp profiles</li> <li>Set up TLS authentication</li> </ul> </li> </ol>"},{"location":"014-DockerDaemon/#additional-resources","title":"\ud83d\udd17 Additional Resources","text":"<ul> <li>Docker Daemon Configuration</li> <li>Private Registry Setup</li> <li>Rootless Docker</li> <li>Docker Security Best Practices</li> <li>BuildKit Documentation</li> </ul>"},{"location":"015-Networking/","title":"015-Networking","text":""},{"location":"015-Networking/#lab-015-docker-networking","title":"Lab 015 - Docker Networking","text":"<ul> <li>This lab covers Docker networking fundamentals, including network drivers, custom networks, and advanced networking features.</li> <li>You\u2019ll learn how to create and manage container networks, configure network connectivity, and implement service discovery.</li> <li>Topics include bridge networks, overlay networks, host networking, and network troubleshooting.</li> <li>By the end of this lab, you\u2019ll understand how to design and manage container networking for various deployment scenarios.</li> </ul>"},{"location":"015-Networking/#table-of-contents","title":"Table of Contents","text":"<ul> <li>\ud83c\udf10 Understanding Docker Networking</li> <li>\ud83d\udd17 Network Drivers</li> <li>Bridge Network</li> <li>Host Network</li> <li>None Network</li> <li>Overlay Network</li> <li>Macvlan Network</li> <li>\u2699\ufe0f Custom Networks</li> <li>Creating Custom Networks</li> <li>Network Configuration</li> <li>\ud83d\udd0c Container Networking</li> <li>Connecting Containers</li> <li>Port Mapping</li> <li>Service Discovery</li> <li>\ud83d\ude80 Advanced Networking</li> <li>Network Plugins</li> <li>DNS Configuration</li> <li>Network Security</li> <li>\ud83d\udd27 Networking Commands</li> <li>\ud83d\udcca Monitoring and Troubleshooting</li> <li>\ud83d\udd12 Networking Best Practices</li> </ul>"},{"location":"015-Networking/#understanding-docker-networking","title":"\ud83c\udf10 Understanding Docker Networking","text":"<p>Docker networking enables containers to communicate with each other and external networks. Docker provides several network drivers to suit different use cases.</p>"},{"location":"015-Networking/#key-concepts","title":"Key Concepts","text":"<ul> <li>Network Drivers: Define how containers connect to networks</li> <li>Bridge Networks: Default network for single-host communication</li> <li>Overlay Networks: Multi-host networking for Swarm clusters</li> <li>Host Networks: Direct access to host network stack</li> <li>None Networks: Isolated containers with no networking</li> </ul>"},{"location":"015-Networking/#default-networks","title":"Default Networks","text":"<p>Docker creates three default networks:</p> <pre><code># List all networks\ndocker network ls\n\n# Inspect default bridge network\ndocker network inspect bridge\n</code></pre>"},{"location":"015-Networking/#network-drivers","title":"\ud83d\udd17 Network Drivers","text":""},{"location":"015-Networking/#bridge-network","title":"Bridge Network","text":"<p>The default network driver for containers. Creates an internal network on the host.</p> <p>Characteristics:</p> <ul> <li>Containers can communicate with each other</li> <li>Containers get IP addresses from Docker\u2019s subnet</li> <li>Port mapping required for external access</li> </ul> <p>Example:</p> <pre><code># Run container on bridge network\ndocker run -d --name web nginx\n\n# Inspect container network\ndocker inspect web | grep -A 10 NetworkSettings\n</code></pre>"},{"location":"015-Networking/#host-network","title":"Host Network","text":"<p>Containers share the host\u2019s network stack directly.</p> <p>Characteristics:</p> <ul> <li>No network isolation</li> <li>Best performance</li> <li>No port conflicts</li> <li>Limited to single host</li> </ul> <p>Example:</p> <pre><code># Run container with host networking\ndocker run -d --network host --name web nginx\n</code></pre>"},{"location":"015-Networking/#none-network","title":"None Network","text":"<p>Completely isolated containers with no network access.</p> <p>Characteristics:</p> <ul> <li>No network interfaces</li> <li>Maximum isolation</li> <li>Manual network setup required</li> </ul> <p>Example:</p> <pre><code># Run container with no networking\ndocker run -d --network none --name isolated alpine sleep 3600\n</code></pre>"},{"location":"015-Networking/#overlay-network","title":"Overlay Network","text":"<p>Multi-host networking for Docker Swarm clusters.</p> <p>Characteristics:</p> <ul> <li>Spans multiple hosts</li> <li>Built-in service discovery</li> <li>Load balancing</li> <li>Requires Swarm mode</li> </ul> <p>Example:</p> <pre><code># Create overlay network (in Swarm)\ndocker network create -d overlay my-overlay\n\n# Run service on overlay network\ndocker service create --network my-overlay --name web nginx\n</code></pre>"},{"location":"015-Networking/#macvlan-network","title":"Macvlan Network","text":"<p>Assigns MAC addresses to containers, making them appear as physical devices.</p> <p>Characteristics:</p> <ul> <li>Layer 2 networking</li> <li>Direct layer 2 access</li> <li>No port mapping needed</li> <li>Requires promiscuous mode on host interface</li> </ul> <p>Example:</p> <pre><code># Create macvlan network\ndocker network create -d macvlan \\\n  --subnet=192.168.1.0/24 \\\n  --gateway=192.168.1.1 \\\n  -o parent=eth0 \\\n  my-macvlan\n\n# Run container on macvlan\ndocker run -d --network my-macvlan --name web nginx\n</code></pre>"},{"location":"015-Networking/#custom-networks","title":"\u2699\ufe0f Custom Networks","text":""},{"location":"015-Networking/#creating-custom-networks","title":"Creating Custom Networks","text":"<p>Create user-defined networks for better control.</p> <pre><code># Create bridge network\ndocker network create my-bridge\n\n# Create network with custom subnet\ndocker network create --subnet 172.20.0.0/16 my-custom-net\n\n# Create network with options\ndocker network create \\\n  --driver bridge \\\n  --subnet 172.25.0.0/16 \\\n  --gateway 172.25.0.1 \\\n  --opt \"com.docker.network.bridge.name\"=\"my-bridge\" \\\n  my-network\n</code></pre>"},{"location":"015-Networking/#network-configuration","title":"Network Configuration","text":"<p>Configure network settings for containers.</p> <pre><code># Connect container to network\ndocker network connect my-network web\n\n# Disconnect from network\ndocker network disconnect bridge web\n\n# Run container with specific IP\ndocker run -d --network my-network --ip 172.20.0.10 --name web nginx\n\n# Inspect network\ndocker network inspect my-network\n</code></pre>"},{"location":"015-Networking/#container-networking","title":"\ud83d\udd0c Container Networking","text":""},{"location":"015-Networking/#connecting-containers","title":"Connecting Containers","text":"<p>Enable communication between containers.</p> <pre><code># Create network\ndocker network create app-network\n\n# Run database\ndocker run -d --network app-network --name db postgres\n\n# Run app connected to same network\ndocker run -d --network app-network --name app myapp\n\n# Containers can communicate by name\ndocker exec app ping db\n</code></pre>"},{"location":"015-Networking/#port-mapping","title":"Port Mapping","text":"<p>Expose container ports to host.</p> <pre><code># Map single port\ndocker run -d -p 8080:80 --name web nginx\n\n# Map multiple ports\ndocker run -d -p 8080:80 -p 8443:443 --name web nginx\n\n# Map to specific host interface\ndocker run -d -p 192.168.1.100:8080:80 --name web nginx\n\n# Dynamic port mapping\ndocker run -d -P --name web nginx\n</code></pre>"},{"location":"015-Networking/#service-discovery","title":"Service Discovery","text":"<p>Automatic service discovery in user-defined networks.</p> <pre><code># Create network\ndocker network create --driver bridge app-net\n\n# Run services\ndocker run -d --network app-net --name redis redis\ndocker run -d --network app-net --name web -e REDIS_HOST=redis myapp\n\n# Services resolve by container name\ndocker exec web nslookup redis\n</code></pre>"},{"location":"015-Networking/#advanced-networking","title":"\ud83d\ude80 Advanced Networking","text":""},{"location":"015-Networking/#network-plugins","title":"Network Plugins","text":"<p>Extend Docker networking with plugins.</p> <pre><code># Install network plugin (example: weave)\ndocker plugin install weaveworks/net-plugin:latest_release\n\n# Create network with plugin\ndocker network create -d weave my-weave-net\n</code></pre>"},{"location":"015-Networking/#dns-configuration","title":"DNS Configuration","text":"<p>Configure DNS for containers.</p> <pre><code># Use custom DNS\ndocker run -d --dns 8.8.8.8 --name web nginx\n\n# Use custom DNS search domains\ndocker run -d --dns-search example.com --name web nginx\n\n# Inspect DNS configuration\ndocker exec web cat /etc/resolv.conf\n</code></pre>"},{"location":"015-Networking/#network-security","title":"Network Security","text":"<p>Secure container communications.</p> <pre><code># Create encrypted overlay network\ndocker network create -d overlay \\\n  --opt encrypted \\\n  my-secure-net\n\n# Use network with iptables rules\ndocker network create --driver bridge \\\n  --opt \"com.docker.network.bridge.enable_icc\"=\"false\" \\\n  isolated-net\n</code></pre>"},{"location":"015-Networking/#networking-commands","title":"\ud83d\udd27 Networking Commands","text":""},{"location":"015-Networking/#network-management","title":"Network Management","text":"<pre><code># List networks\ndocker network ls\n\n# Create network\ndocker network create my-network\n\n# Remove network\ndocker network rm my-network\n\n# Prune unused networks\ndocker network prune\n</code></pre>"},{"location":"015-Networking/#container-network-commands","title":"Container Network Commands","text":"<pre><code># Connect container to network\ndocker network connect my-network container_name\n\n# Disconnect container from network\ndocker network disconnect bridge container_name\n\n# Inspect container networks\ndocker inspect container_name | jq .NetworkSettings.Networks\n</code></pre>"},{"location":"015-Networking/#troubleshooting-commands","title":"Troubleshooting Commands","text":"<pre><code># Check network connectivity\ndocker exec web ping google.com\n\n# View network interfaces\ndocker exec web ip addr\n\n# Check routing table\ndocker exec web ip route\n\n# View iptables rules\nsudo iptables -L -n\n</code></pre>"},{"location":"015-Networking/#monitoring-and-troubleshooting","title":"\ud83d\udcca Monitoring and Troubleshooting","text":""},{"location":"015-Networking/#network-monitoring","title":"Network Monitoring","text":"<pre><code># View network usage\ndocker network ls -q | xargs docker network inspect | jq '.[].Containers | length'\n\n# Monitor network traffic (requires tools)\ndocker run -d --net container:web nicolaka/netshoot tcpdump -i eth0\n\n# Check container connectivity\ndocker exec web curl -I http://other-container\n</code></pre>"},{"location":"015-Networking/#common-issues","title":"Common Issues","text":"<ul> <li>Container can\u2019t reach internet: Check DNS and gateway configuration</li> <li>Containers can\u2019t communicate: Verify network connectivity and firewall rules</li> <li>Port conflicts: Check host port usage</li> <li>Network isolation: Ensure containers are on the same network</li> </ul>"},{"location":"015-Networking/#debugging-steps","title":"Debugging Steps","text":"<pre><code># 1. Check container network settings\ndocker inspect container_name | jq .NetworkSettings\n\n# 2. Verify network connectivity\ndocker exec container_name ping 8.8.8.8\n\n# 3. Check DNS resolution\ndocker exec container_name nslookup google.com\n\n# 4. Inspect network details\ndocker network inspect network_name\n\n# 5. View Docker daemon logs\nsudo journalctl -u docker -f\n</code></pre>"},{"location":"015-Networking/#networking-best-practices","title":"\ud83d\udd12 Networking Best Practices","text":"<p>Prefer user-defined networks over default bridge for better isolation and service discovery.</p> <pre><code>docker network create app-network\ndocker run -d --network app-network --name app myapp\n</code></pre> <p>Separate applications into different networks for security.</p> <pre><code>docker network create frontend\ndocker network create backend\ndocker network create database\n</code></pre> <p>Only expose necessary ports and use specific IP bindings.</p> <pre><code>docker run -d -p 127.0.0.1:8080:80 --name web nginx\n</code></pre> <p>Enable encryption for sensitive communications.</p> <pre><code>docker network create -d overlay --opt encrypted secure-net\n</code></pre> <p>Regularly monitor and audit network communications.</p> <pre><code>docker network ls\ndocker network inspect &lt;network&gt; | jq .Containers\n</code></pre> <p>Remove unused networks to prevent clutter.</p> <pre><code>docker network prune\n</code></pre>"},{"location":"015-Networking/#use-user-defined-networks","title":"\ud83c\udf09 Use User-Defined Networks","text":""},{"location":"015-Networking/#implement-network-segmentation","title":"\ud83d\udd12 Implement Network Segmentation","text":""},{"location":"015-Networking/#minimize-port-exposure","title":"\ud83d\udeaa Minimize Port Exposure","text":""},{"location":"015-Networking/#use-encrypted-networks","title":"\ud83d\udd10 Use Encrypted Networks","text":""},{"location":"015-Networking/#monitor-network-traffic","title":"\ud83d\udcca Monitor Network Traffic","text":""},{"location":"015-Networking/#clean-up-unused-networks","title":"\ud83e\uddf9 Clean Up Unused Networks","text":""},{"location":"015-Networking/#lab-exercises","title":"\ud83d\udccb Lab Exercises","text":"<ol> <li> <p>Explore Default Networks</p> <ul> <li>List all Docker networks</li> <li>Inspect the bridge network configuration</li> <li>Run containers on default networks</li> </ul> </li> <li> <p>Create Custom Networks</p> <ul> <li>Create a user-defined bridge network</li> <li>Run containers on the custom network</li> <li>Test service discovery between containers</li> </ul> </li> <li> <p>Port Mapping and Exposure</p> <ul> <li>Run a web server with port mapping</li> <li>Access the application from host</li> <li>Test different port mapping options</li> </ul> </li> <li> <p>Network Isolation</p> <ul> <li>Create multiple networks</li> <li>Connect containers to specific networks</li> <li>Verify isolation between networks</li> </ul> </li> <li> <p>Advanced Networking</p> <ul> <li>Set up overlay networking (if Swarm available)</li> <li>Configure DNS settings</li> <li>Implement network security measures</li> </ul> </li> </ol>"},{"location":"015-Networking/#additional-resources","title":"\ud83d\udd17 Additional Resources","text":"<ul> <li>Docker Networking Overview</li> <li>Network Drivers</li> <li>Docker Swarm Networking</li> <li>Network Security</li> <li>Networking Best Practices</li> </ul>"},{"location":"016-Advanced-Build/","title":"016-Advanced-Build","text":""},{"location":"016-Advanced-Build/#lab-016-advanced-build","title":"Lab 016 - Advanced Build","text":"<ul> <li>This lab covers advanced Docker build techniques using BuildKit and BuildX.</li> <li>You will learn how to leverage BuildKit for faster and more efficient builds, and use BuildX to build multi-platform images locally.</li> <li>By the end of this lab, you will understand how to build Docker images for multiple architectures and platforms using the Docker CLI.</li> </ul>"},{"location":"016-Advanced-Build/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Desktop or Docker Engine with BuildX support</li> <li>Basic understanding of Dockerfiles and Docker CLI</li> </ul>"},{"location":"016-Advanced-Build/#buildkit","title":"BuildKit","text":"<p>BuildKit is an improved backend for building Docker images. It provides:</p> <ul> <li>Faster builds through parallel processing</li> <li>Better caching mechanisms</li> <li>Support for advanced Dockerfile features</li> <li>Improved security with rootless builds</li> </ul>"},{"location":"016-Advanced-Build/#enabling-buildkit","title":"Enabling BuildKit","text":"<p>BuildKit is enabled by default in Docker Desktop. For Docker Engine, you can enable it by setting the environment variable:</p> <pre><code>export DOCKER_BUILDKIT=1\n</code></pre> <p>Or add it to your shell profile:</p> <pre><code>echo 'export DOCKER_BUILDKIT=1' &gt;&gt; ~/.zshrc\nsource ~/.zshrc\n</code></pre>"},{"location":"016-Advanced-Build/#buildkit-features","title":"BuildKit Features","text":"<ul> <li>Parallel builds: Build stages can run in parallel</li> <li>Better caching: More granular cache invalidation</li> <li>Secrets management: Secure handling of sensitive data during builds</li> <li>Multi-stage builds: Improved support for multi-stage Dockerfiles</li> </ul> <p>Example Dockerfile using BuildKit features:</p> <pre><code># syntax=docker/dockerfile:1\n\nFROM alpine:latest AS base\nRUN apk add --no-cache git\n\nFROM base AS build\nWORKDIR /app\nCOPY . .\nRUN echo \"Building application...\"\n\nFROM alpine:latest\nCOPY --from=build /app /app\nCMD [\"echo\", \"Application built with BuildKit\"]\n</code></pre>"},{"location":"016-Advanced-Build/#buildx","title":"BuildX","text":"<p>BuildX is a Docker CLI plugin that extends the build capabilities with BuildKit. It provides:</p> <ul> <li>Multi-platform builds</li> <li>Advanced build options</li> <li>Custom build drivers</li> <li>Bake support for complex builds</li> </ul>"},{"location":"016-Advanced-Build/#installing-buildx","title":"Installing BuildX","text":"<p>BuildX comes pre-installed with Docker Desktop. For Docker Engine:</p> <pre><code># Download and install BuildX\nmkdir -p ~/.docker/cli-plugins\ncurl -L https://github.com/docker/buildx/releases/latest/download/buildx-linux-amd64 -o ~/.docker/cli-plugins/docker-buildx\nchmod +x ~/.docker/cli-plugins/docker-buildx\n</code></pre>"},{"location":"016-Advanced-Build/#building-multi-platform-images-locally","title":"Building Multi-Platform Images Locally","text":"<p>BuildX allows you to build images for multiple platforms simultaneously. To build for all platforms locally, you need to enable QEMU emulation:</p> <pre><code># Enable QEMU emulation for cross-platform builds\ndocker run --privileged --rm tonistiigi/binfmt --install all\n</code></pre> <p>Or using the Docker image for this purpose:</p> <pre><code># Use the docker/binfmt image to enable multi-platform support\ndocker run --privileged --rm docker/binfmt:a7996909642ee92942dcd6cff44b9b95f08dad64c\n</code></pre>"},{"location":"016-Advanced-Build/#using-tonistiigibinfmt-for-qemu-emulation","title":"Using tonistiigi/binfmt for QEMU Emulation","text":"<p>The tonistiigi/binfmt project provides a Docker image that installs QEMU and binfmt_misc support, enabling cross-platform architecture emulation. This is essential for building Docker images for different CPU architectures on your local machine.</p> <p>What it does:</p> <ul> <li>Installs QEMU user-mode emulation for various architectures</li> <li>Configures binfmt_misc to automatically use QEMU when running foreign binaries</li> <li>Enables building and testing multi-platform Docker images locally</li> </ul> <p>Installation and Setup:</p> <pre><code># Install QEMU and binfmt support for all architectures\ndocker run --privileged --rm tonistiigi/binfmt --install all\n\n# Install specific architectures only\ndocker run --privileged --rm tonistiigi/binfmt --install amd64,arm64,arm\n\n# Check installed formats\ndocker run --privileged --rm tonistiigi/binfmt\n\n# Uninstall (if needed)\ndocker run --privileged --rm tonistiigi/binfmt --uninstall all\n</code></pre> <p>Supported Architectures:</p> <ul> <li><code>amd64</code> (x86-64)</li> <li><code>arm64</code> (ARM 64-bit)</li> <li><code>arm</code> (ARM 32-bit)</li> <li><code>ppc64le</code> (PowerPC 64-bit little-endian)</li> <li><code>s390x</code> (IBM System z)</li> <li><code>riscv64</code> (RISC-V 64-bit)</li> <li>And more\u2026</li> </ul> <p>Persistent Setup: To make the setup persistent across Docker daemon restarts, you can create a systemd service or add it to your Docker startup script.</p> <p>Verification: After installation, verify that binfmt is working:</p> <pre><code># Check binfmt_misc entries\ncat /proc/sys/fs/binfmt_misc/qemu-aarch64\n\n# Test with a simple command\ndocker run --rm arm64v8/alpine uname -m  # Should show aarch64\n</code></pre> <p>Common Issues:</p> <ul> <li>Permission denied: Run with <code>--privileged</code> flag</li> <li>Already installed: The tool is idempotent, running it multiple times is safe</li> <li>Docker Desktop: May require restarting Docker Desktop after installation</li> </ul> <p>This tool is the foundation for local multi-platform Docker builds, allowing you to emulate different architectures without needing physical hardware.</p>"},{"location":"016-Advanced-Build/#building-on-macos","title":"Building on macOS","text":"<p>On macOS, Docker Desktop provides built-in support for multi-platform builds, but for full cross-platform emulation (especially when building ARM images on Intel Macs or vice versa), you need to set up QEMU using the Docker binfmt image.</p> <p>Prerequisites:</p> <ul> <li>Docker Desktop for Mac (latest version)</li> <li>At least 4GB of RAM allocated to Docker</li> </ul> <p>Setup QEMU on macOS:</p> <pre><code># Enable binfmt support for multi-architecture emulation\ndocker run --privileged --rm docker/binfmt:a7996909642ee92942dcd6cff44b9b95f08dad64c\n\n# Verify the setup\ndocker buildx inspect --bootstrap\n</code></pre> <p>Supported Platforms on macOS:</p> <ul> <li><code>linux/amd64</code> (Intel/AMD)</li> <li><code>linux/arm64</code> (Apple Silicon)</li> <li><code>linux/arm/v7</code> (32-bit ARM)</li> <li><code>linux/arm/v6</code> (Raspberry Pi)</li> </ul> <p>Example: Building for all platforms on macOS:</p> <pre><code># Create a builder with docker-container driver for better isolation\ndocker buildx create --use --name mac-multi-builder --driver docker-container\n\n# Build for all supported platforms\ndocker buildx build \\\n  --platform linux/amd64,linux/arm64,linux/arm/v7,linux/arm/v6 \\\n  -t myapp:all-platforms \\\n  --push \\\n  .\n\n# For local testing (load only native platform)\ndocker buildx build \\\n  --platform linux/amd64,linux/arm64 \\\n  -t myapp:native \\\n  --load \\\n  .\n</code></pre> <p>Troubleshooting macOS builds:</p> <ul> <li>If builds fail with QEMU errors, restart Docker Desktop</li> <li>Ensure Docker Desktop has enough RAM (8GB+ recommended for multi-platform builds)</li> <li>Use <code>--progress=plain</code> for detailed build logs</li> <li>Check available platforms: <code>docker buildx inspect --bootstrap | grep Platforms</code></li> </ul>"},{"location":"016-Advanced-Build/#basic-multi-platform-build","title":"Basic Multi-Platform Build","text":"<pre><code># Create a new builder instance\ndocker buildx create --use --name multi-platform-builder\n\n# Build for multiple platforms\ndocker buildx build --platform linux/amd64,linux/arm64,linux/arm/v7 -t myapp:multi .\n\n# Build and push to registry\ndocker buildx build --platform linux/amd64,linux/arm64,linux/arm/v7 -t myregistry.com/myapp:multi --push .\n</code></pre>"},{"location":"016-Advanced-Build/#advanced-buildx-commands","title":"Advanced BuildX Commands","text":"<ul> <li>List builders:</li> </ul> <pre><code>docker buildx ls\n</code></pre> <ul> <li>Inspect builder:</li> </ul> <pre><code>docker buildx inspect multi-platform-builder\n</code></pre> <ul> <li>Build with custom output:</li> </ul> <pre><code>docker buildx build --platform linux/amd64,linux/arm64 -t myapp:multi --output type=local,dest=./dist .\n</code></pre> <ul> <li>Build with bake (using docker-bake.hcl file):</li> </ul> <pre><code>docker buildx bake\n</code></pre>"},{"location":"016-Advanced-Build/#example-multi-platform-nodejs-application","title":"Example: Multi-Platform Node.js Application","text":"<p>Create a sample application:</p> <pre><code>mkdir multi-platform-example\ncd multi-platform-example\n\n# Create package.json\ncat &lt;&lt;EOF &gt; package.json\n{\n  \"name\": \"multi-platform-app\",\n  \"version\": \"1.0.0\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"start\": \"node index.js\"\n  },\n  \"dependencies\": {\n    \"express\": \"^4.18.2\"\n  }\n}\nEOF\n\n# Create index.js\ncat &lt;&lt;EOF &gt; index.js\nconst express = require('express');\nconst app = express();\nconst port = process.env.PORT || 3000;\n\napp.get('/', (req, res) =&gt; {\n  res.json({\n    message: 'Hello from multi-platform Docker app!',\n    platform: process.arch,\n    timestamp: new Date().toISOString()\n  });\n});\n\napp.listen(port, () =&gt; {\n  console.log(\\`App listening on port \\${port}\\`);\n});\nEOF\n\n# Create Dockerfile\ncat &lt;&lt;EOF &gt; Dockerfile\nFROM node:18-alpine\n\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\n\nCOPY . .\nEXPOSE 3000\nCMD [\"npm\", \"start\"]\nEOF\n</code></pre> <p>Build for multiple platforms:</p> <pre><code># Enable multi-platform support\ndocker run --privileged --rm docker/binfmt:a7996909642ee92942dcd6cff44b9b95f08dad64c\n\n# Create and use builder\ndocker buildx create --use --name multi-builder\n\n# Build for multiple platforms\ndocker buildx build --platform linux/amd64,linux/arm64,linux/arm/v7 -t multi-platform-node:latest --load .\n\n# Test the built image\ndocker run -p 3000:3000 multi-platform-node:latest\ncurl localhost:3000\n</code></pre>"},{"location":"016-Advanced-Build/#buildx-bake-for-complex-builds","title":"BuildX Bake for Complex Builds","text":"<p>Create a <code>docker-bake.hcl</code> file for complex multi-target builds:</p> <pre><code>group \"default\" {\n  targets = [\"app\", \"debug\"]\n}\n\ntarget \"app\" {\n  context = \".\"\n  dockerfile = \"Dockerfile\"\n  platforms = [\"linux/amd64\", \"linux/arm64\"]\n  tags = [\"myapp:latest\"]\n}\n\ntarget \"debug\" {\n  inherits = [\"app\"]\n  dockerfile = \"Dockerfile.debug\"\n  tags = [\"myapp:debug\"]\n}\n</code></pre> <p>Build using bake:</p> <pre><code>docker buildx bake\n</code></pre>"},{"location":"016-Advanced-Build/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>QEMU issues: Ensure QEMU is properly installed for cross-platform emulation</li> <li>Builder not found: Use <code>docker buildx create --use</code> to create a new builder</li> <li>Platform not supported: Check available platforms with <code>docker buildx inspect --bootstrap</code></li> <li>Build failures: Use <code>--progress=plain</code> for detailed build output</li> </ul>"},{"location":"016-Advanced-Build/#cleanup","title":"Cleanup","text":"<pre><code># Remove builder\ndocker buildx rm multi-platform-builder\n\n# Clean up images\ndocker image rm myapp:multi multi-platform-node:latest\n</code></pre> <p>This lab demonstrates the power of BuildKit and BuildX for advanced Docker builds, enabling efficient and multi-platform container development.</p>"},{"location":"017-Logging/","title":"Lab 017 - Logging with Fluentd","text":"<p>This lab demonstrates how to use Fluentd with Docker to collect and manage logs from Docker containers, Docker events, and syslog.</p>"},{"location":"017-Logging/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed and running</li> <li>Basic understanding of Docker and logging concepts</li> </ul>"},{"location":"017-Logging/#overview","title":"Overview","text":"<p>Fluentd is an open-source data collector that provides a unified logging layer. In this lab, we\u2019ll:</p> <ol> <li>Set up Fluentd as a logging driver for Docker containers</li> <li>Collect Docker events using Fluentd</li> <li>Configure syslog input for Fluentd</li> </ol>"},{"location":"017-Logging/#demo-scripts","title":"Demo Scripts","text":"<ul> <li><code>demo.sh</code>: Main demo script showing Fluentd setup with Docker logging</li> <li><code>fluentd.conf</code>: Fluentd configuration file</li> <li><code>docker-compose.yml</code>: Docker Compose file to run Fluentd</li> </ul>"},{"location":"017-Logging/#running-the-demo","title":"Running the Demo","text":"<ol> <li>Make sure Docker is running</li> <li>Run the demo script: <code>./demo.sh</code></li> <li>Follow the on-screen instructions</li> </ol>"},{"location":"017-Logging/#key-concepts","title":"Key Concepts","text":"<ul> <li>Fluentd Logging Driver: Docker can send container logs directly to Fluentd</li> <li>Docker Events: System events from Docker daemon (container start/stop, etc.)</li> <li>Syslog Input: Fluentd can receive syslog messages from various sources</li> </ul>"},{"location":"017-Logging/#cleanup","title":"Cleanup","text":"<p>After the demo, run:</p> <pre><code>docker-compose down\n</code></pre>"},{"location":"017-Logging/#additional-resources","title":"Additional Resources","text":"<ul> <li>Fluentd Documentation</li> <li>Docker Logging Drivers</li> </ul>"},{"location":"100-Hands-On/","title":"100-Hands-On","text":""},{"location":"100-Hands-On/#lab-100-basic-docker-hands-on","title":"Lab 100 - Basic Docker Hands-On","text":"<ul> <li>In this lab we will create a simple NodeJs server and run it inside a docker container. </li> <li>Next, we will create a second container which will print some text to the screen. </li> <li>We will also create a docker hub account and push our images to the cloud.</li> </ul>"},{"location":"100-Hands-On/#table-of-contents","title":"Table of Contents","text":"<ul> <li>1. Verify Docker installation</li> <li>2. Building our NodeJs server</li> <li>3. Test our server code</li> <li>3.1. Test if the server is running</li> <li>3.2. Stop the server</li> <li>4. Creating Docker containers</li> <li>4.1. Create <code>Dockerfile</code></li> <li>5. Build the Docker image</li> <li>5.1. Verifying that the container was created</li> <li>5.2. Testing the image</li> <li>5.3 Test if the server is running and listening:</li> <li>6. Container with arguments</li> </ul>"},{"location":"100-Hands-On/#1-verify-docker-installation","title":"1. Verify Docker installation","text":"<ul> <li>Run the following command in the shell to verify that Docker is installed and running</li> </ul> <pre><code>docker run node\n</code></pre>"},{"location":"100-Hands-On/#2-building-our-nodejs-server","title":"2. Building our NodeJs server","text":"<ul> <li>In this section we will create a simple NodeJs server and later on we will run it inside a docker container.</li> <li>First, we will create a folder for our server code and then we will create the server code file.</li> </ul> <pre><code># Create the desired folder\nmkdir hello-docker\n# Switch to the created directory\ncd hello-docker\n</code></pre> <ul> <li>Create our server code file (copy the code into this file) <code>server.js</code></li> </ul> <pre><code>// import the HTTP module\nvar http = require('http');\n\n// Define a port we want to listen to\nconst PORT=8080;\n\n// We need a function which handles requests and send response\nfunction handleRequest(request, response){\n  response.end('It Works!! Path Hit: ' + request.url);\n}\n\n// Create a server\nvar server = http.createServer(handleRequest);\n\n// Start our server\nserver.listen(PORT, function(){\n  //Callback triggered when server is successfully listening. Hurray!\n  console.log(\"Server listening on: http://localhost:%s\", PORT);\n});\n</code></pre>"},{"location":"100-Hands-On/#3-test-our-server-code","title":"3. Test our server code","text":"<ul> <li>You can run the server code using NodeJs by executing the following command.</li> <li>This will run the server and listen to port 8080</li> </ul> <pre><code>node server.js\n</code></pre>"},{"location":"100-Hands-On/#31-test-if-the-server-is-running","title":"3.1. Test if the server is running","text":"<ul> <li>If you are using Cloud Shell, you can test it by clicking on the most left icon (web preview) and open port 8080 which is our server port</li> <li>If you are running it locally, you can test it by opening your browser </li> <li>Use the following URL in your browser <code>http://localhost:8080</code></li> <li>You should see the following message in your browser or terminal   <pre><code>It Works!! Path Hit: /\n</code></pre></li> </ul>"},{"location":"100-Hands-On/#32-stop-the-server","title":"3.2. Stop the server","text":"<ul> <li>Type CTRL+C twice to stop the server</li> </ul>"},{"location":"100-Hands-On/#4-creating-docker-containers","title":"4. Creating Docker containers","text":""},{"location":"100-Hands-On/#41-create-dockerfile","title":"4.1. Create <code>Dockerfile</code>","text":"<ul> <li>In the same folder create the create a file called <code>Dockerfile</code> and add the following content:   <pre><code># Base image for Node.js\nFROM node:latest\n\n# Set the working directory in the container\nWORKDIR /usr/src/app\n\n# Copy the server file into the container \n# (Don`t forget the dot at the end)\nCOPY server.js .\n\n# Expose port 8080\n# This is the port our server will listen to\nEXPOSE 8080\n\n# Start the Node.js server\nCMD [ \"node\", \"server.js\" ]\n</code></pre></li> <li>This Dockerfile does the following:</li> <li>Uses the latest Node.js image as the base image</li> <li>Sets the working directory inside the container to <code>/usr/src/app</code></li> <li>Copies the <code>server.js</code> file from the current directory into the container\u2019s working directory</li> <li>Exposes port 8080 so that it can be accessed from outside the container</li> <li>Specifies the command to run when the container starts, which is <code>node server.js</code></li> </ul> <p>[!NOTE]  Make sure that the <code>Dockerfile</code> is in the same directory as the <code>server.js</code> file.</p> <p>[!IMPORTANT]  The <code>COPY</code> command in the Dockerfile is used to copy files from the host machine into the container. The first argument is the source file (in this case, <code>server.js</code>), and the second argument is the destination path inside the container (<code>.</code> means the current working directory in the container).</p>"},{"location":"100-Hands-On/#5-build-the-docker-image","title":"5. Build the Docker image","text":"<ul> <li> <p>In this step we will build the Docker image using the <code>Dockerfile</code> we created in the previous step.</p> <p>[!IMPORTANT]  Make sure you are in the same directory where the <code>Dockerfile</code> and <code>server.js</code> files are located.</p> </li> <li> <p>Build the image using docker build with the following parameters   <pre><code>--t = Tag name which will be attached to the container\n.   = The Context to the Dockerfile (current folder in our case)\n\n# Sample command to build the image\ndocker build -t hello-node:v1 .\n</code></pre></p> </li> </ul>"},{"location":"100-Hands-On/#51-verifying-that-the-container-was-created","title":"5.1. Verifying that the container was created","text":"<ul> <li>Let\u2019s verify that the image was created successfully by listing all the images on our system</li> <li>Run the following command to list all Docker images:   <pre><code>docker images\n</code></pre></li> <li>You should see an output similar to this:   <pre><code>REPOSITORY          TAG       IMAGE ID       CREATED          SIZE\nhello-node          v1        123456789abc   10 seconds ago   200MB\nnode                latest    abcdef123456   2 days ago       150MB\n</code></pre></li> </ul>"},{"location":"100-Hands-On/#52-testing-the-image","title":"5.2. Testing the image","text":"<ul> <li>Now we can run the container using the image we just created</li> <li>Execute <code>docker run</code> command with the following flags:</li> </ul> Option Description <code>-d</code> Run container in background (daemon mode) <code>-p</code> Map internal port 8080 to external port 8080 <code>hello-node:v1</code> The name of the image we just created <pre><code># Run the container in the background\ndocker run -d -p 8080:8080 hello-node:v1\n</code></pre>"},{"location":"100-Hands-On/#53-test-if-the-server-is-running-and-listening","title":"5.3 Test if the server is running and listening:","text":"<ul> <li>If you are using Cloud Shell, you can test it by clicking on the most left icon (web preview) and open port <code>8080</code> which is our server port</li> <li>If you are running it locally, you can test it by opening your browser </li> <li>Use the following URL in your browser <code>http://localhost:8080</code></li> <li>You should see the following message in your browser or terminal   <pre><code>It Works!! Path Hit: /\n</code></pre></li> </ul>"},{"location":"100-Hands-On/#6-container-with-arguments","title":"6. Container with arguments","text":"<ul> <li>Let\u2019s test another container which will print content to screen</li> </ul> <p>[!WARNING] You might get the following error:</p> <p>docker: [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release.</p> <ul> <li>Run the following command to run a container with arguments   <pre><code>docker run docker/whalesay cowsay boo\n</code></pre></li> <li>You should see the following output in your terminal   <pre><code>________\n&lt; boo &gt;\n--------\n        \\  ^__^\n        \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre></li> </ul>"},{"location":"Tasks/","title":"DockerLabs Tasks","text":"<ul> <li>Welcome to the DockerLabs Tasks section.</li> <li>Each folder below contains a hands-on task/exercise that you can complete independently to practice specific Docker skills.</li> <li>Follow the README file in each task for detailed instructions and solutions.</li> </ul>"},{"location":"Tasks/#task-index","title":"Task Index","text":""},{"location":"Tasks/#complete-docker-tasks","title":"Complete Docker Tasks","text":"Task Description Complete Docker Tasks Collection of 18 comprehensive Docker exercises covering CLI commands, container debugging, Dockerfile best practices, volumes, networking, multi-stage builds, Docker Compose, health checks, environment variables, security, and advanced debugging techniques."},{"location":"Tasks/#dockerfile-tasks","title":"Dockerfile Tasks","text":"Task Description Dockerfile Tasks Collection of 2 Dockerfile exercises covering build arguments, parameterized ports, and multi-stage builds."},{"location":"Tasks/#docker-network-tasks","title":"Docker Network Tasks","text":"Task Description Docker Network Tasks Comprehensive guide to creating and testing Docker networks, including custom networks and container connectivity. <p>Happy learning and hacking with Docker!</p>"},{"location":"Tasks/Docker-CLI-Tasks/","title":"Docker CLI","text":""},{"location":"Tasks/Docker-CLI-Tasks/#docker-cli-tasks","title":"Docker CLI Tasks","text":"<ul> <li>Hands-on Docker exercises covering essential CLI commands, debugging techniques, and advanced containerization concepts.</li> <li>Each task includes a description and a detailed solution with step-by-step instructions.</li> <li>Practice these tasks to master Docker from basic operations to advanced deployment scenarios.</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#table-of-contents","title":"Table of Contents","text":"<ul> <li>01. Docker Commit Workflow</li> <li>02. Docker Container Debugging Challenge</li> <li>03. Docker Logs and Container Management</li> <li>04. Run a docker container, write file to the container, export the container as a new image</li> <li>05. Run nginx container, map the ports, redirect the logs to a file on the host</li> <li>06. Serve a local folder as a website using Nginx</li> <li>07. Extract the default configuration file of an Apache server to use as a template</li> <li>08. Run nginx and monitor the logs in real time</li> <li>09. Delete all stopped containers at once</li> <li>10. Create a multi-stage build Dockerfile for a Go application</li> <li>11. Use Docker volumes to persist data between container restarts</li> <li>12. Set up a Docker network and connect multiple containers</li> <li>13. Build and push a custom image to Docker Hub</li> <li>14. Use docker-compose to run a multi-container application</li> <li>15. Implement health checks for a container</li> <li>16. Use environment variables in a Dockerfile</li> <li>17. Create a Dockerfile that runs as a non-root user</li> <li>18. Use docker exec to debug a running container</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#01-docker-commit-workflow","title":"01. Docker Commit Workflow","text":"<ul> <li> <p>Start an <code>alpine</code> container and keep it running for modifications, create a new file inside the running container, use <code>docker commit</code> to capture a new image with the file included, run a container from the committed image and verify the file exists.</p> </li> </ul> <p>Hint: <code>docker run</code>, <code>docker exec</code>, <code>docker commit</code>, and <code>docker run --rm</code></p> Solution <p>Solution:</p> <ul> <li>Run a modifiable alpine container</li> </ul> <pre><code>docker run -d --name alpine-commit alpine:latest sleep infinity\n</code></pre> <ul> <li>Write a file inside the running container</li> </ul> <pre><code>docker exec alpine-commit sh -c \"echo 'Persisted with docker commit' &gt; /opt/commit-note.txt\"\n</code></pre> <ul> <li>Validate the file inside the original container (optional check)</li> </ul> <pre><code>docker exec alpine-commit cat /opt/commit-note.txt\n</code></pre> <ul> <li>Create a new image from the modified container</li> </ul> <pre><code>docker commit alpine-commit alpine-with-note:latest\n</code></pre> <ul> <li>Run a container from the committed image and verify the file exists</li> </ul> <pre><code>docker run --rm alpine-with-note:latest cat /opt/commit-note.txt\n</code></pre> <p>Expected output:</p> <pre><code>Persisted with docker commit\n</code></pre> <ul> <li>Clean up resources</li> </ul> <pre><code>docker rm -f alpine-commit\ndocker image rm alpine-with-note:latest\n</code></pre> <p>Explanation:</p> <ul> <li>docker run -d \u2026 sleep infinity: Starts a container that stays alive for edits</li> <li>docker exec \u2026 echo \u2018\u2026\u2019 &gt; file: Writes a file into the running container</li> <li>docker commit: Captures the container\u2019s filesystem changes into a new image</li> <li>docker run \u2013rm new-image cat file: Launches the new image, verifies the persistent file, and removes the container when done</li> <li>Cleanup commands: Remove the temporary container and image to free resources</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#scenario","title":"Scenario:","text":"<ul> <li>As a developer, you need to quickly customize a base image for testing by adding configuration files or debugging tools without rebuilding the entire image from source code. </li> <li>This workflow allows you to make runtime modifications and save them as a new reusable image.</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#02-docker-container-debugging-challenge","title":"02. Docker Container Debugging Challenge","text":"<ul> <li> <p>Run a container that encounters an error, use debugging techniques to identify the issue, and fix the problem.</p> </li> </ul> <p>Hint: Use <code>docker logs</code>, <code>docker exec</code>, and <code>docker inspect</code> to troubleshoot container issues</p> Solution <p>Solution:</p> <ul> <li>Run a container that will fail</li> </ul> <pre><code>docker run --name failing-container alpine sh -c \"echo 'Starting...' &amp;&amp; nonexistent_command &amp;&amp; echo 'Success'\"\n</code></pre> <ul> <li>Check the container status</li> </ul> <pre><code>docker ps -a | grep failing-container\n</code></pre> <ul> <li>Examine the container logs</li> </ul> <pre><code>docker logs failing-container\n</code></pre> <p>Expected output will show the error:</p> <pre><code>Starting...\nsh: nonexistent_command: not found\n</code></pre> <ul> <li>Inspect the container for more details</li> </ul> <pre><code>docker inspect failing-container | grep -A 10 \"State\"\n</code></pre> <ul> <li>Debug by running a successful command in a similar container</li> </ul> <pre><code>docker run --rm alpine sh -c \"echo 'Debug: Container is working' &amp;&amp; ls -la /\"\n</code></pre> <ul> <li>Fix the original command and run a corrected version</li> </ul> <pre><code>docker run --rm alpine sh -c \"echo 'Starting...' &amp;&amp; echo 'This command exists' &amp;&amp; echo 'Success'\"\n</code></pre> <ul> <li>Clean up</li> </ul> <pre><code>docker rm failing-container\n</code></pre> <p>Explanation:</p> <ul> <li>docker logs: Shows stdout/stderr output from the container, crucial for debugging failures</li> <li>docker inspect: Provides detailed container information including exit codes and state</li> <li>docker exec: Allows running commands in running containers for interactive debugging</li> <li>Exit codes: Non-zero exit codes indicate failures that can be investigated with logs</li> </ul> <p>Bonus: Advanced Debugging Techniques</p> <pre><code># Check container exit code\ndocker inspect failing-container --format='{{.State.ExitCode}}'\n\n# Run a debug container with the same image\ndocker run -it --rm alpine sh\n\n# Inside the debug shell, test commands manually\n# echo \"Testing commands...\"\n# exit\n</code></pre>"},{"location":"Tasks/Docker-CLI-Tasks/#scenario_1","title":"Scenario:","text":"<ul> <li>Your production application container is failing to start, and you need to diagnose the root cause quickly. </li> <li>Using Docker\u2019s debugging tools, you can inspect logs, check exit codes, and test commands to identify and resolve the issue without affecting other services.</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#03-docker-logs-and-container-management","title":"03. Docker Logs and Container Management","text":"<ul> <li> <p>Run a <code>cowsay</code> Docker container with a custom message, send a message to the cowsay container (e.g., \u201cHello from Docker!\u201d), stop the container after execution, extract and save the container logs to the host machine for debugging purposes.</p> </li> </ul> <p>Hint: Use <code>docker run</code>, <code>docker logs</code>, and output redirection.</p> Solution <p>Solution:</p> <ul> <li>Run the cowsay container with a custom message</li> </ul> <pre><code>docker run --name my-cowsay docker/whalesay cowsay \"Hello from Docker!\"\n</code></pre> <ul> <li>Verify the container has stopped</li> </ul> <p>The container stops automatically after execution. You can verify with:</p> <pre><code>docker ps -a | grep my-cowsay\n</code></pre> <ul> <li>Grab the logs and save to host machine</li> </ul> <pre><code>docker logs my-cowsay &gt; cowsay-logs.txt\n</code></pre> <ul> <li>View the saved logs</li> </ul> <pre><code>cat cowsay-logs.txt\n</code></pre> <p>Expected output in <code>cowsay-logs.txt</code>:</p> <pre><code> ______________________\n&lt; Hello from Docker! &gt;\n ----------------------\n    \\\n     \\\n      \\\n                    ##        .\n              ## ## ##       ==\n           ## ## ## ##      ===\n       /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"___/ ===\n  ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ /  ===- ~~~\n       \\______ o          __/\n        \\    \\        __/\n          \\____\\______/\n</code></pre> <ul> <li>Clean up (optional)</li> </ul> <pre><code># Remove the container\ndocker rm my-cowsay\n\n# Remove the log file\nrm cowsay-logs.txt\n</code></pre> <p>Alternative: Using a Different Cowsay Image</p> <p>If <code>docker/whalesay</code> is not available, you can build your own:</p> <p>Dockerfile:</p> <pre><code>FROM alpine:latest\nRUN apk add --no-cache cowsay\nENTRYPOINT [\"/usr/bin/cowsay\"]\nCMD [\"Hello Docker!\"]\n</code></pre> <p>Build and run:</p> <pre><code>docker build -t my-cowsay .\ndocker run --name cowsay-test my-cowsay \"Hello from Docker!\"\ndocker logs cowsay-test &gt; cowsay-logs.txt\n</code></pre> <p>Explanation:</p> <ul> <li>docker run \u2013name: Assigns a name to the container for easy reference</li> <li>cowsay \u201cmessage\u201d: Passes the message to the cowsay command</li> <li>docker logs: Retrieves all stdout/stderr output from the container</li> <li>&gt; cowsay-logs.txt: Redirects the log output to a file on the host machine</li> <li>The container stops automatically after the command completes</li> </ul> <p>Bonus: Running in Detached Mode</p> <p>For containers that run longer:</p> <pre><code># Run in detached mode\ndocker run -d --name my-cowsay-bg docker/whalesay /bin/sh -c \"cowsay 'Background task' &amp;&amp; sleep 30\"\n\n# Get logs while running\ndocker logs my-cowsay-bg\n\n# Follow logs in real-time\ndocker logs -f my-cowsay-bg\n\n# Stop the container\ndocker stop my-cowsay-bg\n\n# Save logs after stopping\ndocker logs my-cowsay-bg &gt; cowsay-bg-logs.txt\n</code></pre>"},{"location":"Tasks/Docker-CLI-Tasks/#scenario_2","title":"Scenario:","text":"<ul> <li>You\u2019re running a batch processing job in a container and need to capture its output for analysis or compliance purposes. </li> <li>By redirecting container logs to host files, you can preserve important output data even after the container terminates.</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#04-run-a-docker-container-write-file-to-the-container-export-the-container-as-a-new-image","title":"04. Run a docker container, write file to the container, export the container as a new image","text":"<ul> <li> <p>Start an Alpine container, create a file inside it, and then commit the changes to create a new image.</p> </li> </ul> <p>Hint: Use <code>docker run -it</code>, <code>docker commit</code>, and <code>docker run --rm</code></p> Solution <p>Solution: <pre><code># Run an Alpine container interactively\ndocker run -it --name alpine-modify alpine sh\n\n# Inside the container, create a file\necho \"Hello from modified container\" &gt; /hello.txt\n\n# Exit the container (Ctrl+D or exit)\n\n# Commit the container to a new image\ndocker commit alpine-modify my-alpine-modified:v1\n\n# Verify the new image\ndocker images | grep my-alpine-modified\n\n# Test the new image\ndocker run --rm my-alpine-modified cat /hello.txt\n\n# Clean up\ndocker rm alpine-modify\n</code></pre></p> <p>Explanation:</p> <ul> <li>docker run -it: Runs a container interactively, allowing direct shell access</li> <li>docker commit: Creates a new image from a container\u2019s current state</li> <li>docker images: Lists all available images to verify the new image was created</li> <li>docker run \u2013rm: Runs a container and automatically removes it when it exits</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#scenario_3","title":"Scenario:","text":"<ul> <li>You have a legacy application that requires manual configuration steps that can\u2019t be easily automated in a Dockerfile. </li> <li>You need to perform these configurations interactively and then save the configured state as a new image for deployment.</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#05-run-nginx-container-map-the-ports-redirect-the-logs-to-a-file-on-the-host","title":"05. Run nginx container, map the ports, redirect the logs to a file on the host","text":"<ul> <li> <p>Run an Nginx container with port mapping and redirect its logs to a file on the host system.</p> </li> </ul> <p>Hint: Use <code>docker run -d -p</code> and volume mounting with <code>-v</code></p> Solution <p>Solution: <pre><code># Create a directory for logs\nmkdir -p ~/nginx-logs\n\n# Run Nginx container with port mapping and log redirection\ndocker run -d --name nginx-logged \\\n  -p 8080:80 \\\n  -v ~/nginx-logs:/var/log/nginx \\\n  nginx\n\n# Wait a moment for the container to start\nsleep 3\n\n# Test the container\ncurl http://localhost:8080\n\n# Check that logs are being written to the host\nls -la ~/nginx-logs/\ntail ~/nginx-logs/access.log\n\n# Clean up\ndocker stop nginx-logged\ndocker rm nginx-logged\nrm -rf ~/nginx-logs\n</code></pre></p> <p>Explanation:</p> <ul> <li>docker run -d -p: Runs container in detached mode and maps host port to container port</li> <li>-v flag: Mounts host directory as volume inside container for log persistence</li> <li>curl: Tests HTTP connectivity to verify the container is responding</li> <li>tail: Shows the last lines of log files to monitor access logs</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#scenario_4","title":"Scenario:","text":"<ul> <li>You\u2019re deploying a web application in production and need to centralize log collection for monitoring and troubleshooting. </li> <li>By mounting host directories as volumes, you can integrate container logs with your existing log aggregation system.</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#resources","title":"Resources:","text":"<ul> <li>Create a logs directory: <code>mkdir -p ~/nginx-logs</code></li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#06-serve-a-local-folder-as-a-website-using-nginx","title":"06. Serve a local folder as a website using Nginx","text":"<ul> <li> <p>Create a local HTML file and serve it using an Nginx container that mounts the local directory.</p> </li> </ul> <p>Hint: Use volume mounting to override Nginx\u2019s default web root directory</p> Solution <p>Solution: <pre><code># Create a local directory and HTML file\nmkdir -p ~/my-website\necho \"&lt;html&gt;&lt;body&gt;&lt;h1&gt;Hello from my local website!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;\" &gt; ~/my-website/index.html\n\n# Run Nginx container mounting the local directory\ndocker run -d --name nginx-website \\\n  -p 8080:80 \\\n  -v ~/my-website:/usr/share/nginx/html \\\n  nginx\n\n# Wait for the container to start\nsleep 3\n\n# Test the website\ncurl http://localhost:8080\n\n# Clean up\ndocker stop nginx-website\ndocker rm nginx-website\nrm -rf ~/my-website\n</code></pre></p> <p>Explanation:</p> <ul> <li>Volume mounting (-v): Maps host directory to container directory for file sharing</li> <li>Nginx web root: /usr/share/nginx/html is the default directory Nginx serves files from</li> <li>curl: Tests the web server to ensure content is being served correctly</li> <li>Directory creation: Creates local content that gets served by the container</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#scenario_5","title":"Scenario:","text":"<ul> <li>As a frontend developer, you want to quickly test your static website changes without setting up a full development server. </li> <li>Using Docker, you can instantly serve your local files through Nginx for testing across different devices on your network.</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#resources_1","title":"Resources:","text":"<ul> <li>Create a website directory: <code>mkdir -p ~/my-website</code></li> <li>Create an HTML file: <code>echo \"&lt;html&gt;&lt;body&gt;&lt;h1&gt;Hello from my local website!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;\" &gt; ~/my-website/index.html</code></li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#07-extract-the-default-configuration-file-of-an-apache-server-to-use-as-a-template","title":"07. Extract the default configuration file of an Apache server to use as a template","text":"<ul> <li> <p>Run an Apache container, copy its default configuration file to the host, then stop and remove the container.</p> </li> </ul> <p>Hint: Use <code>docker cp</code> to copy files from a running container to the host</p> Solution <p>Solution: <pre><code># Run Apache container\ndocker run -d --name apache-config httpd\n\n# Wait for it to start\nsleep 3\n\n# Copy the default configuration file\ndocker cp apache-config:/usr/local/apache2/conf/httpd.conf ~/apache-config-template.conf\n\n# Verify the file was copied\nls -la ~/apache-config-template.conf\nhead -20 ~/apache-config-template.conf\n\n# Clean up\ndocker stop apache-config\ndocker rm apache-config\n</code></pre></p> <p>Explanation:</p> <ul> <li>docker cp: Copies files between host and running containers</li> <li>Configuration extraction: Useful for getting default configs as templates for customization</li> <li>head command: Shows the first few lines of files to verify content</li> <li>File permissions check: ls -la shows detailed file information including permissions</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#scenario_6","title":"Scenario:","text":"<ul> <li>You\u2019re setting up a custom Apache configuration for your application and need a reference configuration file to start with. </li> <li>Rather than manually creating one, you can extract the default configuration from the official Apache image as a template.</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#08-run-nginx-and-monitor-the-logs-in-real-time","title":"08. Run nginx and monitor the logs in real time","text":"<ul> <li> <p>Start an Nginx container and monitor its access logs in real time using docker logs.</p> </li> </ul> <p>Hint: Use <code>docker logs -f</code> to follow logs in real time</p> Solution <p>Solution: <pre><code># Run Nginx container\ndocker run -d --name nginx-monitor -p 8080:80 nginx\n\n# In another terminal, monitor logs in real time\n# docker logs -f nginx-monitor\n\n# In the current terminal, generate some traffic\nfor i in {1..5}; do\n  curl http://localhost:8080\n  sleep 1\ndone\n\n# Stop monitoring (Ctrl+C in the other terminal)\n\n# Clean up\ndocker stop nginx-monitor\ndocker rm nginx-monitor\n</code></pre></p> <p>Explanation:</p> <ul> <li>docker logs -f: Follows log output in real-time (like tail -f)</li> <li>Background monitoring: Running logs command in separate terminal while generating traffic</li> <li>Traffic generation: Using curl in a loop to create access log entries</li> <li>Real-time debugging: Essential for monitoring live application behavior</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#scenario_7","title":"Scenario:","text":"<ul> <li>Your web application is experiencing intermittent issues, and you need to monitor incoming requests in real-time to identify patterns or problematic traffic. </li> <li>Following logs live helps you correlate user actions with system behavior.</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#09-delete-all-stopped-containers-at-once","title":"09. Delete all stopped containers at once","text":"<ul> <li> <p>Remove all containers that are not currently running.</p> </li> </ul> <p>Hint: Use <code>docker container prune</code> or filter containers by status</p> Solution <p>Solution: <pre><code># List all containers (running and stopped)\ndocker ps -a\n\n# Remove all stopped containers\ndocker container prune -f\n\n# Alternatively, using rm with ps\n# docker rm $(docker ps -aq --filter status=exited)\n\n# Verify cleanup\ndocker ps -a\n</code></pre></p> <p>Explanation:</p> <ul> <li>docker container prune: Removes all stopped containers at once</li> <li>docker ps -a: Shows all containers (running and stopped) for verification</li> <li>Container lifecycle management: Important for keeping Docker environment clean</li> <li>Bulk operations: More efficient than removing containers one by one</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#scenario_8","title":"Scenario:","text":"<ul> <li>After a development session with multiple container iterations, your Docker environment is cluttered with stopped containers consuming disk space. </li> <li>You need to clean up efficiently to free resources and maintain a tidy development environment.</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#10-create-a-multi-stage-build-dockerfile-for-a-go-application","title":"10. Create a multi-stage build Dockerfile for a Go application","text":"<ul> <li> <p>Create a Dockerfile that uses multi-stage builds to compile a Go application and produce a minimal runtime image.</p> </li> </ul> <p>Hint: Use <code>FROM ... AS</code> to define build stages and <code>COPY --from=</code> to copy artifacts between stages</p> Solution <p>Solution: Create a file named <code>Dockerfile.multi-stage</code>: <pre><code># Build stage\nFROM golang:1.21-alpine AS builder\n\nWORKDIR /app\n\n# Copy go mod files\nCOPY go.mod go.sum ./\nRUN go mod download\n\n# Copy source code\nCOPY . .\n\n# Build the application\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o main .\n\n# Runtime stage\nFROM alpine:latest\n\nRUN apk --no-cache add ca-certificates\nWORKDIR /root/\n\n# Copy the binary from builder stage\nCOPY --from=builder /app/main .\n\n# Expose port\nEXPOSE 8080\n\n# Run the binary\nCMD [\"./main\"]\n</code></pre></p> <p>Build and test: <pre><code># Assuming you have a simple Go app\necho 'package main\n\nimport (\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    http.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) {\n        fmt.Fprintf(w, \"Hello from Go!\")\n    })\n    http.ListenAndServe(\":8080\", nil)\n}' &gt; main.go\n\necho 'module app\ngo 1.21' &gt; go.mod\n\n# Build the multi-stage image\ndocker build -f Dockerfile.multi-stage -t go-multi-stage .\n\n# Run the container\ndocker run -d -p 8080:80 go-multi-stage\n\n# Test\ncurl http://localhost:8080\n\n# Clean up\ndocker stop $(docker ps -q --filter ancestor=go-multi-stage)\ndocker rm $(docker ps -aq --filter ancestor=go-multi-stage)\ndocker rmi go-multi-stage\n</code></pre></p> <p>Explanation:</p> <ul> <li>Multi-stage builds: Separate build and runtime stages for smaller final images</li> <li>FROM \u2026 AS: Defines named build stages that can be referenced later</li> <li>COPY \u2013from=: Copies artifacts from build stage to runtime stage</li> <li>CGO_ENABLED=0: Disables CGO for static binary compilation</li> <li>Minimal runtime images: Alpine Linux provides small, secure base images</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#scenario_9","title":"Scenario:","text":"<ul> <li>You\u2019re deploying a Go application to production and want to minimize the attack surface and image size. </li> <li>Multi-stage builds allow you to use heavy build tools in the first stage and copy only the compiled binary to a minimal runtime image.</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#resources_2","title":"Resources:","text":"<ul> <li>Create <code>main.go</code>:   <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"net/http\"\n)\n\nfunc main() {\n    http.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) {\n        fmt.Fprintf(w, \"Hello from Go!\")\n    })\n    http.ListenAndServe(\":8080\", nil)\n}\n</code></pre></li> <li>Create <code>go.mod</code>:   <pre><code>module app\ngo 1.21\n</code></pre></li> <li>Create <code>Dockerfile.multi-stage</code> (see solution for content)</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#11-use-docker-volumes-to-persist-data-between-container-restarts","title":"11. Use Docker volumes to persist data between container restarts","text":"<ul> <li> <p>Create a named volume, use it with a container to store data, restart the container, and verify data persistence.</p> </li> </ul> <p>Hint: Use <code>docker volume create</code> and mount volumes with <code>-v</code> flag</p> Solution <p>Solution: <pre><code># Create a named volume\ndocker volume create my-data-volume\n\n# Run a container that writes to the volume\ndocker run -d --name data-container \\\n  -v my-data-volume:/data \\\n  alpine sh -c \"echo 'Persistent data' &gt; /data/file.txt &amp;&amp; sleep 30\"\n\n# Wait a moment\nsleep 5\n\n# Check the data in the volume\ndocker run --rm -v my-data-volume:/data alpine cat /data/file.txt\n\n# Stop and remove the container\ndocker stop data-container\ndocker rm data-container\n\n# Run a new container with the same volume\ndocker run --rm -v my-data-volume:/data alpine cat /data/file.txt\n\n# Clean up\ndocker volume rm my-data-volume\n</code></pre></p> <p>Explanation:</p> <ul> <li>docker volume create: Creates named volumes for persistent data storage</li> <li>Named volumes: Managed by Docker and survive container deletion</li> <li>Data persistence: Volumes maintain data across container restarts and recreations</li> <li>Volume mounting: -v flag attaches volumes to containers at specific paths</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#scenario_10","title":"Scenario:","text":"<ul> <li>Your application stores user data or configuration that must survive container updates or crashes. </li> <li>Using Docker volumes ensures that important data persists independently of container lifecycle.</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#12-set-up-a-docker-network-and-connect-multiple-containers","title":"12. Set up a Docker network and connect multiple containers","text":"<ul> <li> <p>Create a custom Docker network, run two containers on it, and demonstrate inter-container communication.</p> </li> </ul> <p>Hint: Use <code>docker network create</code> and <code>--network</code> flag when running containers</p> Solution <p>Solution: <pre><code># Create a custom network\ndocker network create my-network\n\n# Run a container that provides a service (simple HTTP server)\ndocker run -d --name web-server \\\n  --network my-network \\\n  -p 8080:80 \\\n  nginx\n\n# Run another container that can communicate with the first\ndocker run --rm --network my-network \\\n  alpine wget -qO- http://web-server\n\n# Test from host (should work via port mapping)\ncurl http://localhost:8080\n\n# Inspect the network\ndocker network inspect my-network\n\n# Clean up\ndocker stop web-server\ndocker rm web-server\ndocker network rm my-network\n</code></pre></p> <p>Explanation:</p> <ul> <li>docker network create: Creates isolated networks for container communication</li> <li>\u2013network flag: Connects containers to specific networks</li> <li>Service discovery: Containers can communicate using container names as hostnames</li> <li>Network isolation: Provides security and organization for multi-container applications</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#scenario_11","title":"Scenario:","text":"<ul> <li>You\u2019re deploying a microservices architecture where multiple containers need to communicate securely. </li> <li>Custom networks provide isolation and service discovery, allowing containers to communicate using predictable hostnames.</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#13-build-and-push-a-custom-image-to-docker-hub","title":"13. Build and push a custom image to Docker Hub","text":"<ul> <li> <p>Create a simple custom image, tag it appropriately, and push it to Docker Hub (requires a Docker Hub account).</p> </li> </ul> <p>Hint: Use <code>docker build</code>, <code>docker tag</code>, <code>docker login</code>, and <code>docker push</code></p> Solution <p>Solution: <pre><code># Create a simple Dockerfile\necho 'FROM alpine:latest\nRUN echo \"Hello from my custom image\" &gt; /hello.txt\nCMD [\"cat\", \"/hello.txt\"]' &gt; Dockerfile\n\n# Build the image\ndocker build -t my-custom-image:v1 .\n\n# Tag for Docker Hub (replace 'yourusername' with your Docker Hub username)\n# docker tag my-custom-image:v1 yourusername/my-custom-image:v1\n\n# Login to Docker Hub\n# docker login\n\n# Push the image\n# docker push yourusername/my-custom-image:v1\n\n# Test pulling the image (after pushing)\n# docker rmi yourusername/my-custom-image:v1\n# docker pull yourusername/my-custom-image:v1\n\n# Clean up\ndocker rmi my-custom-image:v1\nrm Dockerfile\n</code></pre></p> <p>Explanation:</p> <ul> <li>docker build: Creates images from Dockerfiles</li> <li>docker tag: Assigns repository names and tags to images</li> <li>docker login: Authenticates with Docker Hub for pushing images</li> <li>docker push: Uploads images to remote registries like Docker Hub</li> <li>Image distribution: Enables sharing and deploying applications across environments</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#scenario_12","title":"Scenario:","text":"<ul> <li>You\u2019ve developed a custom application image and want to share it with your team or deploy it across multiple environments. </li> <li>Pushing to Docker Hub makes the image accessible from any Docker host with internet access.</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#14-use-docker-compose-to-run-a-multi-container-application","title":"14. Use docker-compose to run a multi-container application","text":"<ul> <li> <p>Create a docker-compose.yml file for a simple web application with a database, and run it.</p> </li> </ul> <p>Hint: Define services, ports, volumes, and environment variables in docker-compose.yml</p> Solution <p>Solution: Create a <code>docker-compose.yml</code> file: <pre><code>version: '3.8'\nservices:\n  web:\n    image: nginx:latest\n    ports:\n      - \"8080:80\"\n    volumes:\n      - ./html:/usr/share/nginx/html\n  db:\n    image: postgres:13\n    environment:\n      POSTGRES_PASSWORD: mypassword\n      POSTGRES_DB: mydb\n    volumes:\n      - db-data:/var/lib/postgresql/data\n\nvolumes:\n  db-data:\n</code></pre></p> <p>Run the application: <pre><code># Create a simple HTML file\nmkdir -p html\necho \"&lt;html&gt;&lt;body&gt;&lt;h1&gt;Web App with DB&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;\" &gt; html/index.html\n\n# Start the services\ndocker-compose up -d\n\n# Check running services\ndocker-compose ps\n\n# Test the web service\ncurl http://localhost:8080\n\n# Stop and clean up\ndocker-compose down -v\nrm -rf html docker-compose.yml\n</code></pre></p> <p>Explanation:</p> <ul> <li>docker-compose.yml: Defines multi-container applications with services, networks, and volumes</li> <li>docker-compose up -d: Starts all services defined in the compose file in detached mode</li> <li>Service orchestration: Manages complex applications with multiple interconnected containers</li> <li>Environment variables: Configure services through compose file declarations</li> <li>Named volumes: Persistent storage that survives container recreation</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#scenario_13","title":"Scenario:","text":"<ul> <li>You\u2019re developing a full-stack application with multiple components (web server, database, cache) that need to work together. </li> <li>Docker Compose allows you to define, configure, and run all services with a single command.</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#15-implement-health-checks-for-a-container","title":"15. Implement health checks for a container","text":"<ul> <li> <p>Create a Dockerfile with a health check and run a container to monitor its health status.</p> </li> </ul> <p>Hint: Use <code>HEALTHCHECK</code> instruction in Dockerfile and check status with <code>docker ps</code> or <code>docker inspect</code></p> Solution <p>Solution: Create a <code>Dockerfile.health</code>: <pre><code>FROM nginx:latest\n\n# Copy a custom health check script\nCOPY healthcheck.sh /healthcheck.sh\nRUN chmod +x /healthcheck.sh\n\n# Add health check\nHEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\n  CMD /healthcheck.sh\n\n# Expose port\nEXPOSE 80\n</code></pre></p> <p>Create the health check script: <pre><code>#!/bin/sh\n# Simple health check that verifies nginx is responding\ncurl -f http://localhost/ &gt; /dev/null 2&gt;&amp;1\nif [ $? -eq 0 ]; then\n    echo \"Health check passed\"\n    exit 0\nelse\n    echo \"Health check failed\"\n    exit 1\nfi\n</code></pre></p> <p>Build and run: <pre><code># Build the image\ndocker build -f Dockerfile.health -t nginx-healthy .\n\n# Run the container\ndocker run -d --name healthy-nginx -p 8080:80 nginx-healthy\n\n# Wait for health check to run\nsleep 10\n\n# Check container health\ndocker ps\ndocker inspect healthy-nginx | grep -A 5 \"Health\"\n\n# Clean up\ndocker stop healthy-nginx\ndocker rm healthy-nginx\ndocker rmi nginx-healthy\nrm Dockerfile.health healthcheck.sh\n</code></pre></p> <p>Explanation:</p> <ul> <li>HEALTHCHECK instruction: Defines how Docker determines if a container is healthy</li> <li>Health check intervals: \u2013interval, \u2013timeout, \u2013start-period, and \u2013retries parameters</li> <li>Custom health scripts: Executable scripts that perform application-specific health checks</li> <li>Container monitoring: Docker automatically restarts unhealthy containers when configured</li> <li>docker inspect: Shows health status information for running containers</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#scenario_14","title":"Scenario:","text":"<ul> <li>Your containerized application runs in production with auto-healing capabilities. </li> <li>Health checks allow the orchestrator to detect when a container becomes unresponsive and automatically restart it, ensuring high availability.</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#16-use-environment-variables-in-a-dockerfile","title":"16. Use environment variables in a Dockerfile","text":"<ul> <li> <p>Create a Dockerfile that uses environment variables for configuration and demonstrates their usage.</p> </li> </ul> <p>Hint: Use <code>ENV</code> instruction in Dockerfile and override with <code>docker run -e</code></p> Solution <p>Solution: Create a <code>Dockerfile.env</code>: <pre><code>FROM alpine:latest\n\n# Set environment variables\nENV GREETING=\"Hello\" \\\n    NAME=\"Docker User\"\n\n# Use environment variables in a script\nRUN echo '#!/bin/sh' &gt; /greet.sh &amp;&amp; \\\n    echo 'echo \"$GREETING, $NAME!\"' &gt;&gt; /greet.sh &amp;&amp; \\\n    chmod +x /greet.sh\n\n# Run the script\nCMD [\"/greet.sh\"]\n</code></pre></p> <p>Build and test: <pre><code># Build the image\ndocker build -f Dockerfile.env -t env-example .\n\n# Run with default environment variables\ndocker run --rm env-example\n\n# Run with overridden environment variables\ndocker run --rm -e GREETING=\"Hi\" -e NAME=\"Developer\" env-example\n\n# Clean up\ndocker rmi env-example\nrm Dockerfile.env\n</code></pre></p> <p>Explanation:</p> <ul> <li>ENV instruction: Sets environment variables in the Docker image</li> <li>docker run -e: Overrides environment variables at runtime</li> <li>Configuration flexibility: Environment variables allow runtime customization</li> <li>Build-time vs runtime: ENV sets defaults, -e overrides them when running containers</li> <li>Security considerations: Avoid hardcoding sensitive values in images</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#scenario_15","title":"Scenario:","text":"<ul> <li>Your application needs different configurations for development, staging, and production environments. </li> <li>Environment variables allow you to build one image that can be configured differently at runtime without code changes.</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#17-create-a-dockerfile-that-runs-as-a-non-root-user","title":"17. Create a Dockerfile that runs as a non-root user","text":"<ul> <li> <p>Create a Dockerfile that creates a non-root user and runs the container as that user for security.</p> </li> </ul> <p>Hint: Use <code>RUN</code> to create user/group, <code>USER</code> instruction, and proper file permissions</p> Solution <p>Solution: Create a <code>Dockerfile.nonroot</code>: <pre><code>FROM alpine:latest\n\n# Create a non-root user\nRUN addgroup -g 1001 -S appgroup &amp;&amp; \\\n    adduser -u 1001 -S appuser -G appgroup\n\n# Create a directory for the app\nRUN mkdir /app &amp;&amp; chown appuser:appgroup /app\n\n# Switch to non-root user\nUSER appuser\n\n# Set working directory\nWORKDIR /app\n\n# Create a simple script\nRUN echo '#!/bin/sh' &gt; /app/hello.sh &amp;&amp; \\\n    echo 'echo \"Running as $(whoami) with UID $(id -u)\"' &gt;&gt; /app/hello.sh &amp;&amp; \\\n    chmod +x /app/hello.sh\n\n# Run as non-root user\nCMD [\"/app/hello.sh\"]\n</code></pre></p> <p>Build and test: <pre><code># Build the image\ndocker build -f Dockerfile.nonroot -t nonroot-example .\n\n# Run the container\ndocker run --rm nonroot-example\n\n# Verify it's running as non-root (should show appuser, not root)\n# Clean up\ndocker rmi nonroot-example\nrm Dockerfile.nonroot\n</code></pre></p> <p>Explanation:</p> <ul> <li>USER instruction: Specifies the user to run subsequent commands and the final container</li> <li>Non-root security: Running as non-root user reduces attack surface and follows security best practices</li> <li>User creation: RUN commands to create users/groups before switching with USER</li> <li>File permissions: Proper ownership and permissions for non-root users</li> <li>whoami and id: Commands to verify which user the container is running as</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#scenario_16","title":"Scenario:","text":"<ul> <li>Security best practices require running containers as non-root users to minimize the impact of potential vulnerabilities. </li> <li>This is especially important in multi-tenant environments where container escapes could compromise the host system.</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#18-use-docker-exec-to-debug-a-running-container","title":"18. Use docker exec to debug a running container","text":"<ul> <li> <p>Start a container, use docker exec to enter it and inspect its state, then make changes.</p> </li> </ul> <p>Hint: Use <code>docker exec -it</code> for interactive shell access and <code>docker exec</code> for running commands</p> Solution <p>Solution: <pre><code># Run a container\ndocker run -d --name debug-container alpine sleep 1000\n\n# Check what's running\ndocker ps\n\n# Execute a command in the running container\ndocker exec debug-container echo \"Hello from inside the container\"\n\n# Start an interactive shell in the container\ndocker exec -it debug-container sh\n\n# Inside the shell, you can:\n# - Check processes: ps aux\n# - Check filesystem: ls -la /\n# - Check environment: env\n# - Create files: echo \"test\" &gt; /tmp/test.txt\n# - Exit with Ctrl+D\n\n# After exiting, check if changes persist\ndocker exec debug-container cat /tmp/test.txt\n\n# Check container logs\ndocker logs debug-container\n\n# Inspect container details\ndocker inspect debug-container | grep -A 10 \"State\"\n\n# Clean up\ndocker stop debug-container\ndocker rm debug-container\n</code></pre></p> <p>Explanation:</p> <ul> <li>docker exec: Runs commands inside running containers without stopping them</li> <li>docker exec -it: Interactive terminal access to running containers for debugging</li> <li>Container inspection: Examining processes, filesystem, and environment inside containers</li> <li>Live debugging: Essential for troubleshooting running applications</li> <li>Non-destructive testing: Debug without affecting the container\u2019s state</li> </ul>"},{"location":"Tasks/Docker-CLI-Tasks/#scenario_17","title":"Scenario:","text":"<ul> <li>Your production container is behaving unexpectedly, and you need to investigate without stopping the service. </li> <li>Using docker exec, you can attach to the running container, examine its state, and make temporary fixes while maintaining service availability.</li> </ul>"},{"location":"Tasks/DockerBasics/","title":"Docker Basics","text":""},{"location":"Tasks/DockerBasics/#lab-000-docker-basics","title":"Lab 000 - Docker Basics","text":"<ul> <li>This lab covers the fundamental concepts and commands of Docker.</li> <li>You will learn basic Docker operations including running containers, managing images, and creating simple Dockerfiles.</li> <li>Each task is designed to be simple and atomic, focusing on one specific Docker command or concept.</li> <li>By the end of this lab, you will have hands-on experience with core Docker functionality.</li> <li>The lab includes 50 tasks divided into Docker CLI commands, Dockerfile creation, and Docker Compose orchestration.</li> </ul>"},{"location":"Tasks/DockerBasics/#tasks-overview","title":"Tasks Overview","text":""},{"location":"Tasks/DockerBasics/#docker-cli-tasks","title":"Docker CLI Tasks","text":"<ul> <li>Task 01: Run hello-world container</li> <li>Task 02: List running containers</li> <li>Task 03: List all containers</li> <li>Task 04: Pull an image</li> <li>Task 05: List images</li> <li>Task 06: Run container interactively</li> <li>Task 07: Run container in background</li> <li>Task 08: List running containers again</li> <li>Task 09: Stop a container</li> <li>Task 10: Remove a container</li> <li>Task 11: Inspect a container</li> <li>Task 12: View container logs</li> <li>Task 13: Execute command in container</li> <li>Task 14: Copy file to container</li> <li>Task 15: Copy file from container</li> <li>Task 16: Run container with port mapping</li> <li>Task 17: Run container with volume</li> <li>Task 18: Create container without starting</li> <li>Task 19: Start created container</li> <li>Task 20: Kill a container</li> </ul>"},{"location":"Tasks/DockerBasics/#dockerfile-tasks","title":"Dockerfile Tasks","text":"<ul> <li>Task 21: Create basic Dockerfile</li> <li>Task 22: Build image from Dockerfile</li> <li>Task 23: Run built image</li> <li>Task 24: Create Dockerfile with COPY</li> <li>Task 25: Create Dockerfile with RUN</li> <li>Task 26: Docker commit - create image from container</li> <li>Task 27: Advanced docker cp - copy directories</li> <li>Task 28: Dockerfile with multiple RUN commands</li> <li>Task 29: Dockerfile with WORKDIR</li> <li>Task 30: Dockerfile with ENV variables</li> <li>Task 31: Dockerfile with EXPOSE</li> <li>Task 32: Custom ENTRYPOINT in Dockerfile</li> <li>Task 33: Dockerfile with USER (non-root)</li> <li>Task 34: Copy multiple files with COPY</li> <li>Task 35: Build a simple web server image</li> </ul>"},{"location":"Tasks/DockerBasics/#docker-compose-tasks","title":"Docker Compose Tasks","text":"<ul> <li>Task 36: Basic docker-compose.yml structure</li> <li>Task 37: Docker Compose with multiple services</li> <li>Task 38: Docker Compose with networks</li> <li>Task 39: Docker Compose with volumes</li> <li>Task 40: Docker Compose with environment variables</li> <li>Task 41: Docker Compose with build context</li> <li>Task 42: Docker Compose with depends_on</li> <li>Task 43: Docker Compose with health checks</li> <li>Task 44: Docker Compose scaling services</li> <li>Task 45: Docker Compose with logging</li> <li>Task 46: Docker Compose with environment files</li> <li>Task 47: Docker Compose overrides</li> <li>Task 48: Docker Compose with profiles</li> <li>Task 49: Docker Compose with secrets</li> <li>Task 50: Docker Compose with extensions</li> </ul>"},{"location":"Tasks/DockerBasics/#docker-cli-tasks_1","title":"Docker CLI Tasks","text":""},{"location":"Tasks/DockerBasics/#task-01-run-hello-world-container","title":"Task 01: Run hello-world container","text":"<ul> <li>Run the official hello-world container to verify Docker installation.</li> <li>This container will print a message and exit.</li> </ul> Solution <pre><code>docker run hello-world\n</code></pre>"},{"location":"Tasks/DockerBasics/#task-02-list-running-containers","title":"Task 02: List running containers","text":"<ul> <li>Display all currently running containers.</li> </ul> Solution <pre><code>docker ps\n</code></pre>"},{"location":"Tasks/DockerBasics/#task-03-list-all-containers","title":"Task 03: List all containers","text":"<ul> <li>Display all containers, including stopped ones.</li> </ul> Solution <pre><code>docker ps -a\n</code></pre>"},{"location":"Tasks/DockerBasics/#task-04-pull-an-image","title":"Task 04: Pull an image","text":"<ul> <li>Download the alpine image from Docker Hub without running it.</li> </ul> Solution <pre><code>docker pull alpine\n</code></pre>"},{"location":"Tasks/DockerBasics/#task-05-list-images","title":"Task 05: List images","text":"<ul> <li>Display all Docker images available locally.</li> </ul> Solution <pre><code>docker images\n</code></pre>"},{"location":"Tasks/DockerBasics/#task-06-run-container-interactively","title":"Task 06: Run container interactively","text":"<ul> <li>Run an alpine container and open an interactive shell.</li> </ul> Solution <pre><code>docker run -it alpine sh\n</code></pre>"},{"location":"Tasks/DockerBasics/#task-07-run-container-in-background","title":"Task 07: Run container in background","text":"<ul> <li>Run an nginx container in the background.</li> </ul> Solution <pre><code>docker run -d --name web nginx\n</code></pre>"},{"location":"Tasks/DockerBasics/#task-08-list-running-containers-again","title":"Task 08: List running containers again","text":"<ul> <li>Verify the nginx container is running.</li> </ul> Solution <pre><code>docker ps\n</code></pre>"},{"location":"Tasks/DockerBasics/#task-09-stop-a-container","title":"Task 09: Stop a container","text":"<ul> <li>Stop the running nginx container.</li> </ul> Solution <pre><code>docker stop web\n</code></pre>"},{"location":"Tasks/DockerBasics/#task-10-remove-a-container","title":"Task 10: Remove a container","text":"<ul> <li>Remove the stopped nginx container.</li> </ul> Solution <pre><code>docker rm web\n</code></pre>"},{"location":"Tasks/DockerBasics/#task-11-inspect-a-container","title":"Task 11: Inspect a container","text":"<ul> <li>Run a container and inspect its details.</li> </ul> Solution <pre><code>docker run -d --name test alpine sleep 100\ndocker inspect test\n</code></pre>"},{"location":"Tasks/DockerBasics/#task-12-view-container-logs","title":"Task 12: View container logs","text":"<ul> <li>View the logs of a running container.</li> </ul> Solution <pre><code>docker logs test\n</code></pre>"},{"location":"Tasks/DockerBasics/#task-13-execute-command-in-container","title":"Task 13: Execute command in container","text":"<ul> <li>Execute a command inside a running container.</li> </ul> Solution <pre><code>docker exec test echo \"Hello from inside the container\"\n</code></pre>"},{"location":"Tasks/DockerBasics/#task-14-copy-file-to-container","title":"Task 14: Copy file to container","text":"<ul> <li>Copy a file from the host to a running container.</li> </ul> Solution <pre><code>echo \"test content\" &gt; testfile.txt\ndocker cp testfile.txt test:/tmp/\n</code></pre>"},{"location":"Tasks/DockerBasics/#task-15-copy-file-from-container","title":"Task 15: Copy file from container","text":"<ul> <li>Copy a file from a container back to the host.</li> </ul> Solution <pre><code>docker cp test:/tmp/testfile.txt copiedfile.txt\n</code></pre>"},{"location":"Tasks/DockerBasics/#task-16-run-container-with-port-mapping","title":"Task 16: Run container with port mapping","text":"<ul> <li>Run nginx with port mapping to access it from the host.</li> </ul> Solution <pre><code>docker run -d -p 8080:80 --name web2 nginx\ncurl localhost:8080\n</code></pre>"},{"location":"Tasks/DockerBasics/#task-17-run-container-with-volume","title":"Task 17: Run container with volume","text":"<ul> <li>Run a container with a volume mount.</li> </ul> Solution <pre><code>docker run -d -v $(pwd):/data --name vol-test alpine sleep 100\n</code></pre>"},{"location":"Tasks/DockerBasics/#task-18-create-container-without-starting","title":"Task 18: Create container without starting","text":"<ul> <li>Create a container but don\u2019t start it.</li> </ul> Solution <pre><code>docker create --name created-test alpine echo \"created\"\n</code></pre>"},{"location":"Tasks/DockerBasics/#task-19-start-created-container","title":"Task 19: Start created container","text":"<ul> <li>Start the previously created container.</li> </ul> Solution <pre><code>docker start -a created-test\n</code></pre>"},{"location":"Tasks/DockerBasics/#task-20-kill-a-container","title":"Task 20: Kill a container","text":"<ul> <li>Forcefully kill a running container.</li> </ul> Solution <pre><code>docker run -d --name kill-test alpine sleep 100\ndocker kill kill-test\n</code></pre>"},{"location":"Tasks/DockerBasics/#dockerfile-tasks_1","title":"Dockerfile Tasks","text":""},{"location":"Tasks/DockerBasics/#task-21-create-basic-dockerfile","title":"Task 21: Create basic Dockerfile","text":"<ul> <li>Create a simple Dockerfile that uses alpine as base and runs a basic command.</li> </ul> Solution <pre><code>FROM alpine\nCMD [\"echo\", \"Hello Docker\"]\n</code></pre>"},{"location":"Tasks/DockerBasics/#task-22-build-image-from-dockerfile","title":"Task 22: Build image from Dockerfile","text":"<ul> <li>Build an image from the Dockerfile created in Task 21.</li> </ul> Solution <pre><code>docker build -t basic-image .\n</code></pre>"},{"location":"Tasks/DockerBasics/#task-23-run-built-image","title":"Task 23: Run built image","text":"<ul> <li>Run the image built in Task 22.</li> </ul> Solution <pre><code>docker run basic-image\n</code></pre>"},{"location":"Tasks/DockerBasics/#task-24-create-dockerfile-with-copy","title":"Task 24: Create Dockerfile with COPY","text":"<ul> <li>Create a Dockerfile that copies a file and runs it.</li> </ul> Solution <pre><code>FROM alpine\nCOPY testfile.txt /tmp/\nCMD [\"cat\", \"/tmp/testfile.txt\"]\n</code></pre>"},{"location":"Tasks/DockerBasics/#task-25-create-dockerfile-with-run","title":"Task 25: Create Dockerfile with RUN","text":"<ul> <li>Create a Dockerfile that installs a package and runs a command.</li> </ul> Solution <pre><code>FROM alpine\nRUN apk add --no-cache curl\nCMD [\"curl\", \"--version\"]\n</code></pre>"},{"location":"Tasks/DockerBasics/#task-26-docker-commit-create-image-from-container","title":"Task 26: Docker commit - create image from container","text":"<ul> <li>Modify a running container and create a new image from it using docker commit.</li> </ul> Solution <pre><code># Run a container and modify it\ndocker run -d --name modify-me alpine sleep 100\ndocker exec modify-me sh -c \"echo 'Modified content' &gt; /modified.txt\"\n\n# Commit the changes to a new image\ndocker commit modify-me my-modified-image\n\n# Run a container from the new image to verify\ndocker run --rm my-modified-image cat /modified.txt\n\n# Clean up\ndocker stop modify-me\ndocker rm modify-me\ndocker rmi my-modified-image\n</code></pre>"},{"location":"Tasks/DockerBasics/#task-27-advanced-docker-cp-copy-directories","title":"Task 27: Advanced docker cp - copy directories","text":"<ul> <li>Copy entire directories between host and container using docker cp.</li> </ul> Solution <pre><code># Create a directory with files\nmkdir test-dir\necho \"file1 content\" &gt; test-dir/file1.txt\necho \"file2 content\" &gt; test-dir/file2.txt\n\n# Run a container\ndocker run -d --name copy-test alpine sleep 100\n\n# Copy directory to container\ndocker cp test-dir copy-test:/tmp/\n\n# Copy directory from container to host\ndocker cp copy-test:/tmp/test-dir copied-dir\n\n# Verify\nls -la copied-dir/\n\n# Clean up\ndocker stop copy-test\ndocker rm copy-test\nrm -rf test-dir copied-dir\n</code></pre>"},{"location":"Tasks/DockerBasics/#task-28-dockerfile-with-multiple-run-commands","title":"Task 28: Dockerfile with multiple RUN commands","text":"<ul> <li>Create a Dockerfile with multiple RUN instructions to install packages and configure the image.</li> </ul> Solution <pre><code>FROM alpine:latest\n\n# Update package index\nRUN apk update\n\n# Install multiple packages\nRUN apk add --no-cache \\\n    curl \\\n    wget \\\n    git\n\n# Create a directory\nRUN mkdir -p /app\n\n# Set permissions\nRUN chmod 755 /app\n\nCMD [\"echo\", \"Multi-RUN Dockerfile completed\"]\n</code></pre> <p>Build and run: <pre><code>docker build -t multi-run-image .\ndocker run --rm multi-run-image\ndocker rmi multi-run-image\n</code></pre></p>"},{"location":"Tasks/DockerBasics/#task-29-dockerfile-with-workdir","title":"Task 29: Dockerfile with WORKDIR","text":"<ul> <li>Use WORKDIR instruction to set the working directory for subsequent instructions.</li> </ul> Solution <pre><code>FROM alpine:latest\n\n# Set working directory\nWORKDIR /app\n\n# Create files in the working directory\nRUN echo \"Hello from /app\" &gt; hello.txt\n\n# Copy files to working directory\nCOPY testfile.txt .\n\n# Run commands in working directory\nCMD [\"cat\", \"hello.txt\"]\n</code></pre> <p>Build and run: <pre><code>echo \"test content\" &gt; testfile.txt\ndocker build -t workdir-image .\ndocker run --rm workdir-image\ndocker rmi workdir-image\nrm testfile.txt\n</code></pre></p>"},{"location":"Tasks/DockerBasics/#task-30-dockerfile-with-env-variables","title":"Task 30: Dockerfile with ENV variables","text":"<ul> <li>Use ENV instruction to set environment variables in the Dockerfile.</li> </ul> Solution <pre><code>FROM alpine:latest\n\n# Set environment variables\nENV APP_NAME=\"My Docker App\" \\\n    APP_VERSION=\"1.0.0\" \\\n    AUTHOR=\"Docker User\"\n\n# Use environment variables in RUN commands\nRUN echo \"Building $APP_NAME version $APP_VERSION by $AUTHOR\" &gt; /app/info.txt\n\n# Use in CMD\nCMD [\"sh\", \"-c\", \"echo 'App: '$APP_NAME', Version: '$APP_VERSION', Author: '$AUTHOR &amp;&amp; cat /app/info.txt\"]\n</code></pre> <p>Build and run: <pre><code>docker build -t env-image .\ndocker run --rm env-image\ndocker rmi env-image\n</code></pre></p>"},{"location":"Tasks/DockerBasics/#task-31-dockerfile-with-expose","title":"Task 31: Dockerfile with EXPOSE","text":"<ul> <li>Use EXPOSE instruction to document which ports the container listens on.</li> </ul> Solution <pre><code>FROM alpine:latest\n\n# Install a simple HTTP server\nRUN apk add --no-cache python3\n\n# Create a simple HTML file\nRUN echo \"&lt;html&gt;&lt;body&gt;&lt;h1&gt;Hello from Docker!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;\" &gt; /index.html\n\n# Expose port 8080\nEXPOSE 8080\n\n# Start a simple HTTP server\nCMD [\"python3\", \"-m\", \"http.server\", \"8080\"]\n</code></pre> <p>Build and run: <pre><code>docker build -t expose-image .\ndocker run -d -p 8080:8080 --name expose-test expose-image\nsleep 2\ncurl localhost:8080\ndocker stop expose-test\ndocker rm expose-test\ndocker rmi expose-image\n</code></pre></p>"},{"location":"Tasks/DockerBasics/#task-32-custom-entrypoint-in-dockerfile","title":"Task 32: Custom ENTRYPOINT in Dockerfile","text":"<ul> <li>Use ENTRYPOINT instruction to set the default executable for the container.</li> </ul> Solution <pre><code>FROM alpine:latest\n\n# Install curl\nRUN apk add --no-cache curl\n\n# Set entrypoint to curl\nENTRYPOINT [\"curl\"]\n\n# Default arguments for curl\nCMD [\"--version\"]\n</code></pre> <p>Build and run: <pre><code>docker build -t entrypoint-image .\n\n# Run with default CMD\ndocker run --rm entrypoint-image\n\n# Run with custom arguments\ndocker run --rm entrypoint-image -I https://httpbin.org/get\n\ndocker rmi entrypoint-image\n</code></pre></p>"},{"location":"Tasks/DockerBasics/#task-33-dockerfile-with-user-non-root","title":"Task 33: Dockerfile with USER (non-root)","text":"<ul> <li>Create a Dockerfile that runs as a non-root user for security.</li> </ul> Solution <pre><code>FROM alpine:latest\n\n# Create a non-root user\nRUN addgroup -g 1001 -S appgroup &amp;&amp; \\\n    adduser -u 1001 -S appuser -G appgroup\n\n# Create app directory and set ownership\nRUN mkdir /app &amp;&amp; chown appuser:appgroup /app\n\n# Switch to non-root user\nUSER appuser\n\n# Set working directory\nWORKDIR /app\n\n# Create a file as non-root user\nRUN echo \"Running as $(whoami)\" &gt; user-info.txt\n\nCMD [\"cat\", \"user-info.txt\"]\n</code></pre> <p>Build and run: <pre><code>docker build -t user-image .\ndocker run --rm user-image\ndocker rmi user-image\n</code></pre></p>"},{"location":"Tasks/DockerBasics/#task-34-copy-multiple-files-with-copy","title":"Task 34: Copy multiple files with COPY","text":"<ul> <li>Use COPY instruction to copy multiple files and directories into the container.</li> </ul> Solution <pre><code>FROM alpine:latest\n\n# Create source files\nRUN mkdir /source\nRUN echo \"config data\" &gt; /source/config.txt\nRUN echo \"script content\" &gt; /source/script.sh\n\n# Copy multiple files\nCOPY /source/* /app/\n\n# Copy entire directory\nCOPY /source /app/source/\n\n# List copied files\nCMD [\"ls\", \"-la\", \"/app/\"]\n</code></pre> <p>Build and run: <pre><code>docker build -t copy-multi-image .\ndocker run --rm copy-multi-image\ndocker rmi copy-multi-image\n</code></pre></p>"},{"location":"Tasks/DockerBasics/#task-35-build-a-simple-web-server-image","title":"Task 35: Build a simple web server image","text":"<ul> <li>Create a complete Dockerfile for a simple web server that serves static files.</li> </ul> Solution <pre><code>FROM alpine:latest\n\n# Install nginx\nRUN apk add --no-cache nginx\n\n# Create web directory\nRUN mkdir -p /var/www/html\n\n# Create a simple HTML page\nRUN echo '&lt;html&gt;&lt;head&gt;&lt;title&gt;Docker Web Server&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to my Docker Web Server!&lt;/h1&gt;&lt;p&gt;This page is served from a custom Docker image.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;' &gt; /var/www/html/index.html\n\n# Create nginx config\nRUN echo 'server { listen 80; root /var/www/html; index index.html; location / { try_files $uri $uri/ =404; } }' &gt; /etc/nginx/http.d/default.conf\n\n# Expose port\nEXPOSE 80\n\n# Start nginx\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n</code></pre> <p>Build and run: <pre><code>docker build -t web-server-image .\ndocker run -d -p 8080:80 --name web-server web-server-image\nsleep 3\ncurl localhost:8080\ndocker stop web-server\ndocker rm web-server\ndocker rmi web-server-image\n</code></pre></p>"},{"location":"Tasks/DockerBasics/#docker-compose-tasks_1","title":"Docker Compose Tasks","text":""},{"location":"Tasks/DockerBasics/#task-36-basic-docker-composeyml-structure","title":"Task 36: Basic docker-compose.yml structure","text":"<ul> <li>Create a basic docker-compose.yml file with a single service.</li> </ul> Solution <p>Create <code>docker-compose.yml</code>: <pre><code>version: '3.8'\nservices:\n  web:\n    image: nginx:latest\n    ports:\n      - \"8080:80\"\n</code></pre></p> <p>Run and test: <pre><code>docker-compose up -d\ndocker-compose ps\ncurl localhost:8080\ndocker-compose down\n</code></pre></p>"},{"location":"Tasks/DockerBasics/#task-37-docker-compose-with-multiple-services","title":"Task 37: Docker Compose with multiple services","text":"<ul> <li>Create a docker-compose.yml with multiple services that communicate with each other.</li> </ul> Solution <p>Create <code>docker-compose.yml</code>: <pre><code>version: '3.8'\nservices:\n  web:\n    image: nginx:latest\n    ports:\n      - \"8080:80\"\n  api:\n    image: alpine:latest\n    command: [\"sh\", \"-c\", \"while true; do echo 'API running'; sleep 30; done\"]\n</code></pre></p> <p>Run and test: <pre><code>docker-compose up -d\ndocker-compose ps\ndocker-compose logs\ndocker-compose down\n</code></pre></p>"},{"location":"Tasks/DockerBasics/#task-38-docker-compose-with-networks","title":"Task 38: Docker Compose with networks","text":"<ul> <li>Configure custom networks in docker-compose.yml for service isolation.</li> </ul> Solution <p>Create <code>docker-compose.yml</code>: <pre><code>version: '3.8'\nservices:\n  web:\n    image: nginx:latest\n    ports:\n      - \"8080:80\"\n    networks:\n      - frontend\n  api:\n    image: alpine:latest\n    command: [\"sh\", \"-c\", \"while true; do echo 'API running'; sleep 30; done\"]\n    networks:\n      - backend\n  proxy:\n    image: nginx:latest\n    ports:\n      - \"8081:80\"\n    networks:\n      - frontend\n      - backend\n\nnetworks:\n  frontend:\n    driver: bridge\n  backend:\n    driver: bridge\n</code></pre></p> <p>Run and test: <pre><code>docker-compose up -d\ndocker-compose ps\ndocker network ls\ndocker-compose down\n</code></pre></p>"},{"location":"Tasks/DockerBasics/#task-39-docker-compose-with-volumes","title":"Task 39: Docker Compose with volumes","text":"<ul> <li>Use named volumes and bind mounts in docker-compose.yml.</li> </ul> Solution <p>Create <code>docker-compose.yml</code>: <pre><code>version: '3.8'\nservices:\n  web:\n    image: nginx:latest\n    ports:\n      - \"8080:80\"\n    volumes:\n      - ./html:/usr/share/nginx/html:ro\n      - nginx-logs:/var/log/nginx\n  data:\n    image: alpine:latest\n    volumes:\n      - data-volume:/data\n    command: [\"sh\", \"-c\", \"echo 'Data stored' &gt; /data/file.txt &amp;&amp; sleep 3600\"]\n\nvolumes:\n  nginx-logs:\n  data-volume:\n</code></pre></p> <p>Run and test: <pre><code>mkdir html\necho \"&lt;h1&gt;Hello from Docker Compose!&lt;/h1&gt;\" &gt; html/index.html\ndocker-compose up -d\ncurl localhost:8080\ndocker-compose exec data cat /data/file.txt\ndocker-compose down -v\nrm -rf html\n</code></pre></p>"},{"location":"Tasks/DockerBasics/#task-40-docker-compose-with-environment-variables","title":"Task 40: Docker Compose with environment variables","text":"<ul> <li>Configure environment variables in docker-compose.yml.</li> </ul> Solution <p>Create <code>docker-compose.yml</code>: <pre><code>version: '3.8'\nservices:\n  web:\n    image: nginx:latest\n    ports:\n      - \"8080:80\"\n    environment:\n      - NGINX_PORT=80\n      - APP_ENV=production\n  app:\n    image: alpine:latest\n    environment:\n      - DATABASE_URL=postgres://user:pass@db:5432/mydb\n      - REDIS_URL=redis://cache:6379\n    command: [\"sh\", \"-c\", \"echo 'DB: '$DATABASE_URL &amp;&amp; echo 'Redis: '$REDIS_URL &amp;&amp; sleep 30\"]\n</code></pre></p> <p>Run and test: <pre><code>docker-compose up -d\ndocker-compose exec app env | grep -E \"(DATABASE|REDIS)\"\ndocker-compose down\n</code></pre></p>"},{"location":"Tasks/DockerBasics/#task-41-docker-compose-with-build-context","title":"Task 41: Docker Compose with build context","text":"<ul> <li>Build custom images using docker-compose.yml.</li> </ul> Solution <p>Create <code>Dockerfile</code>: <pre><code>FROM alpine:latest\nRUN echo \"Custom built image\" &gt; /message.txt\nCMD [\"cat\", \"/message.txt\"]\n</code></pre></p> <p>Create <code>docker-compose.yml</code>: <pre><code>version: '3.8'\nservices:\n  custom-app:\n    build: .\n    ports:\n      - \"8080:8080\"\n</code></pre></p> <p>Run and test: <pre><code>docker-compose up --build -d\ndocker-compose logs\ndocker-compose down --rmi local\n</code></pre></p>"},{"location":"Tasks/DockerBasics/#task-42-docker-compose-with-depends_on","title":"Task 42: Docker Compose with depends_on","text":"<ul> <li>Use depends_on to control service startup order.</li> </ul> Solution <p>Create <code>docker-compose.yml</code>: <pre><code>version: '3.8'\nservices:\n  db:\n    image: postgres:13\n    environment:\n      - POSTGRES_PASSWORD=mypassword\n      - POSTGRES_DB=testdb\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U postgres\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n  app:\n    image: alpine:latest\n    depends_on:\n      db:\n        condition: service_healthy\n    command: [\"sh\", \"-c\", \"echo 'App started after DB is healthy' &amp;&amp; sleep 30\"]\n</code></pre></p> <p>Run and test: <pre><code>docker-compose up -d\ndocker-compose ps\ndocker-compose logs app\ndocker-compose down\n</code></pre></p>"},{"location":"Tasks/DockerBasics/#task-43-docker-compose-with-health-checks","title":"Task 43: Docker Compose with health checks","text":"<ul> <li>Configure health checks for services in docker-compose.yml.</li> </ul> Solution <p>Create <code>docker-compose.yml</code>: <pre><code>version: '3.8'\nservices:\n  web:\n    image: nginx:latest\n    ports:\n      - \"8080:80\"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost/\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 40s\n  monitor:\n    image: alpine:latest\n    depends_on:\n      web:\n        condition: service_healthy\n    command: [\"sh\", \"-c\", \"echo 'Web service is healthy!' &amp;&amp; sleep 30\"]\n</code></pre></p> <p>Run and test: <pre><code>docker-compose up -d\ndocker-compose ps\ndocker-compose exec web curl -f http://localhost/\ndocker-compose down\n</code></pre></p>"},{"location":"Tasks/DockerBasics/#task-44-docker-compose-scaling-services","title":"Task 44: Docker Compose scaling services","text":"<ul> <li>Scale services up and down using docker-compose.</li> </ul> Solution <p>Create <code>docker-compose.yml</code>: <pre><code>version: '3.8'\nservices:\n  web:\n    image: nginx:latest\n    ports:\n      - \"8080-8085:80\"\n</code></pre></p> <p>Run and test: <pre><code>docker-compose up -d --scale web=3\ndocker-compose ps\ncurl localhost:8080\ncurl localhost:8081\ncurl localhost:8082\ndocker-compose up -d --scale web=1\ndocker-compose down\n</code></pre></p>"},{"location":"Tasks/DockerBasics/#task-45-docker-compose-with-logging","title":"Task 45: Docker Compose with logging","text":"<ul> <li>Configure logging options in docker-compose.yml.</li> </ul> Solution <p>Create <code>docker-compose.yml</code>: <pre><code>version: '3.8'\nservices:\n  app:\n    image: alpine:latest\n    command: [\"sh\", \"-c\", \"for i in $(seq 1 10); do echo 'Log message '$i; sleep 2; done\"]\n    logging:\n      driver: json-file\n      options:\n        max-size: \"10m\"\n        max-file: \"3\"\n  web:\n    image: nginx:latest\n    ports:\n      - \"8080:80\"\n    logging:\n      driver: syslog\n      options:\n        syslog-address: \"tcp://localhost:514\"\n</code></pre></p> <p>Run and test: <pre><code>docker-compose up -d\ndocker-compose logs app\ndocker-compose logs -f --tail=5 web\ndocker-compose down\n</code></pre></p>"},{"location":"Tasks/DockerBasics/#task-46-docker-compose-with-environment-files","title":"Task 46: Docker Compose with environment files","text":"<ul> <li>Use .env files to manage environment variables.</li> </ul> Solution <p>Create <code>.env</code> file: <pre><code>APP_NAME=MyApp\nAPP_VERSION=1.0.0\nDATABASE_URL=postgres://user:pass@localhost:5432/mydb\n</code></pre></p> <p>Create <code>docker-compose.yml</code>: <pre><code>version: '3.8'\nservices:\n  app:\n    image: alpine:latest\n    environment:\n      - APP_NAME=${APP_NAME}\n      - APP_VERSION=${APP_VERSION}\n      - DATABASE_URL=${DATABASE_URL}\n    command: [\"sh\", \"-c\", \"echo 'App: '$APP_NAME' v'$APP_VERSION &amp;&amp; echo 'DB: '$DATABASE_URL &amp;&amp; sleep 30\"]\n</code></pre></p> <p>Run and test: <pre><code>docker-compose up -d\ndocker-compose exec app env | grep -E \"(APP|DATABASE)\"\ndocker-compose down\nrm .env\n</code></pre></p>"},{"location":"Tasks/DockerBasics/#task-47-docker-compose-overrides","title":"Task 47: Docker Compose overrides","text":"<ul> <li>Use multiple compose files for different environments.</li> </ul> Solution <p>Create <code>docker-compose.yml</code> (base): <pre><code>version: '3.8'\nservices:\n  web:\n    image: nginx:latest\n    ports:\n      - \"8080:80\"\n</code></pre></p> <p>Create <code>docker-compose.override.yml</code> (development): <pre><code>version: '3.8'\nservices:\n  web:\n    environment:\n      - ENV=development\n    volumes:\n      - ./dev-html:/usr/share/nginx/html\n  debug:\n    image: alpine:latest\n    command: [\"sh\", \"-c\", \"while true; do echo 'Debug service running'; sleep 30; done\"]\n</code></pre></p> <p>Run and test: <pre><code>mkdir dev-html\necho \"&lt;h1&gt;Development Environment&lt;/h1&gt;\" &gt; dev-html/index.html\ndocker-compose up -d\ncurl localhost:8080\ndocker-compose ps\ndocker-compose down\nrm -rf dev-html\n</code></pre></p>"},{"location":"Tasks/DockerBasics/#task-48-docker-compose-with-profiles","title":"Task 48: Docker Compose with profiles","text":"<ul> <li>Use profiles to enable/disable services.</li> </ul> Solution <p>Create <code>docker-compose.yml</code>: <pre><code>version: '3.8'\nservices:\n  web:\n    image: nginx:latest\n    ports:\n      - \"8080:80\"\n  db:\n    image: postgres:13\n    environment:\n      - POSTGRES_PASSWORD=mypassword\n    profiles:\n      - database\n  cache:\n    image: redis:latest\n    profiles:\n      - cache\n  monitoring:\n    image: alpine:latest\n    command: [\"sh\", \"-c\", \"while true; do echo 'Monitoring...'; sleep 30; done\"]\n    profiles:\n      - monitoring\n</code></pre></p> <p>Run and test: <pre><code># Start only web service\ndocker-compose up -d\ncurl localhost:8080\n\n# Start with database\ndocker-compose --profile database up -d\ndocker-compose ps\n\n# Start with cache and monitoring\ndocker-compose --profile cache --profile monitoring up -d\ndocker-compose ps\n\ndocker-compose down\n</code></pre></p>"},{"location":"Tasks/DockerBasics/#task-49-docker-compose-with-secrets","title":"Task 49: Docker Compose with secrets","text":"<ul> <li>Manage sensitive data using Docker secrets.</li> </ul> Solution <p>Create secret files: <pre><code>echo \"mysecretpassword\" &gt; db_password.txt\necho \"myappsecretkey\" &gt; app_secret.txt\n</code></pre></p> <p>Create <code>docker-compose.yml</code>: <pre><code>version: '3.8'\nservices:\n  db:\n    image: postgres:13\n    environment:\n      - POSTGRES_PASSWORD_FILE=/run/secrets/db_password\n    secrets:\n      - db_password\n  app:\n    image: alpine:latest\n    secrets:\n      - app_secret\n    command: [\"sh\", \"-c\", \"cat /run/secrets/app_secret &amp;&amp; sleep 30\"]\n\nsecrets:\n  db_password:\n    file: ./db_password.txt\n  app_secret:\n    file: ./app_secret.txt\n</code></pre></p> <p>Run and test: <pre><code>docker-compose up -d\ndocker-compose exec app cat /run/secrets/app_secret\ndocker-compose down\nrm db_password.txt app_secret.txt\n</code></pre></p>"},{"location":"Tasks/DockerBasics/#task-50-docker-compose-with-extensions","title":"Task 50: Docker Compose with extensions","text":"<ul> <li>Use extensions (x-) for reusable configurations.</li> </ul> Solution <p>Create <code>docker-compose.yml</code>: <pre><code>version: '3.8'\n\nx-app-defaults: &amp;app-defaults\n  image: alpine:latest\n  environment:\n    - LOG_LEVEL=info\n  restart: unless-stopped\n\nx-db-defaults: &amp;db-defaults\n  restart: unless-stopped\n  environment:\n    - POSTGRES_USER=app\n    - POSTGRES_DB=myapp\n\nservices:\n  web:\n    &lt;&lt;: *app-defaults\n    ports:\n      - \"8080:80\"\n    command: [\"sh\", \"-c\", \"echo 'Web service with defaults' &amp;&amp; sleep 3600\"]\n\n  api:\n    &lt;&lt;: *app-defaults\n    ports:\n      - \"8081:8081\"\n    environment:\n      - LOG_LEVEL=debug\n    command: [\"sh\", \"-c\", \"echo 'API service with custom log level' &amp;&amp; sleep 3600\"]\n\n  db:\n    &lt;&lt;: *db-defaults\n    image: postgres:13\n    ports:\n      - \"5432:5432\"\n    environment:\n      - POSTGRES_PASSWORD=mypassword\n</code></pre></p> <p>Run and test: <pre><code>docker-compose up -d\ndocker-compose ps\ndocker-compose exec web env | grep LOG_LEVEL\ndocker-compose exec api env | grep LOG_LEVEL\ndocker-compose down\n</code></pre></p> <ul> <li>After completing all tasks, clean up containers and images.</li> </ul> Clean Up Commands <pre><code># Remove all containers\ndocker rm $(docker ps -aq)\n\n# Remove unused images\ndocker image prune -f\n</code></pre> <p></p>"},{"location":"Tasks/DockerCompose/","title":"Docker Compose","text":""},{"location":"Tasks/DockerCompose/#docker-compose-tasks","title":"Docker Compose Tasks","text":"<ul> <li>Hands-on Docker Compose exercises covering multi-container application orchestration, service configuration, networking, and deployment patterns.</li> <li>Each task includes a clear scenario description, helpful hints, and detailed solutions with explanations.</li> <li>Practice these tasks to master Docker Compose for complex application stacks and microservices architectures.</li> </ul>"},{"location":"Tasks/DockerCompose/#table-of-contents","title":"Table of Contents","text":"<ul> <li>01. Basic Docker Compose Setup</li> <li>02. Multi-Service Application</li> <li>03. Environment Variables and Configuration</li> <li>04. Volumes and Data Persistence</li> <li>05. Networking Between Services</li> <li>06. Health Checks and Dependencies</li> <li>07. Scaling Services</li> <li>08. Build Configuration</li> <li>09. Override Files</li> <li>10. Secrets Management</li> <li>11. Logging Configuration</li> <li>12. Resource Limits</li> <li>13. Profiles and Selective Services</li> <li>14. External Networks</li> <li>15. Custom Networks</li> <li>16. Load Balancing</li> <li>17. Multi-Stage Deployments</li> <li>18. Production-Ready Stack</li> <li>19. Basic YAML Anchors and References</li> <li>20. Merging Service Configurations</li> <li>21. Using Includes for Modular Compose Files</li> <li>22. Environment-Specific Fragments</li> <li>23. Complex Fragment Hierarchies</li> <li>24. Fragment-Based Service Templates</li> </ul>"},{"location":"Tasks/DockerCompose/#01-basic-docker-compose-setup","title":"01. Basic Docker Compose Setup","text":"<ul> <li> <p>Create a simple docker-compose.yml file to run a single web service with port mapping and volume mounting.</p> </li> </ul> <p>Hint: Use <code>version</code>, <code>services</code>, <code>image</code>, <code>ports</code>, and <code>volumes</code> keys</p> Solution <p>Solution:</p> <p>docker-compose.yml <pre><code>version: '3.8'\nservices:\n  web:\n    image: nginx:alpine\n    ports:\n      - \"8080:80\"\n    volumes:\n      - ./html:/usr/share/nginx/html\n</code></pre></p> <p>Commands: <pre><code># Start the services\ndocker-compose up -d\n\n# Check running services\ndocker-compose ps\n\n# View logs\ndocker-compose logs\n\n# Stop and remove services\ndocker-compose down\n</code></pre></p> <p>Explanation:</p> <ul> <li>version: Specifies the Compose file format version</li> <li>services: Defines the containers to run</li> <li>image: Specifies the Docker image to use</li> <li>ports: Maps host ports to container ports</li> <li>volumes: Mounts host directories into containers</li> <li>docker-compose up -d: Starts services in detached mode</li> <li>docker-compose down: Stops and removes containers and networks</li> </ul>"},{"location":"Tasks/DockerCompose/#scenario","title":"Scenario:","text":"<ul> <li>As a developer, you need to quickly set up a development environment for a web application that requires consistent configuration across team members.</li> <li>Docker Compose allows you to define and run multi-container applications with a single command, ensuring everyone uses the same setup.</li> </ul>"},{"location":"Tasks/DockerCompose/#resources","title":"Resources:","text":"<ul> <li><code>index.html</code> \u27a4 <code>/usr/share/nginx/html</code> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Docker Compose App&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Hello from Docker Compose!&lt;/h1&gt;\n    &lt;p&gt;This page is served by a container managed by Docker Compose.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></li> </ul>"},{"location":"Tasks/DockerCompose/#02-multi-service-application","title":"02. Multi-Service Application","text":"<ul> <li> <p>Create a docker-compose.yml file that runs a web application with a database backend, demonstrating service communication.</p> </li> </ul> <p>Hint: Use <code>depends_on</code> for service dependencies and named volumes for data persistence</p> Solution <p>Solution:</p> <p>docker-compose.yml <pre><code>version: '3.8'\nservices:\n  web:\n    image: nginx:alpine\n    ports:\n      - \"8080:80\"\n    volumes:\n      - ./web:/usr/share/nginx/html\n    depends_on:\n      - db\n\n  db:\n    image: postgres:13-alpine\n    environment:\n      POSTGRES_DB: myapp\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    volumes:\n      - db_data:/var/lib/postgresql/data\n\nvolumes:\n  db_data:\n</code></pre></p> <p>web/index.html <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;Multi-Service App&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Web App with Database&lt;/h1&gt;\n    &lt;p&gt;Connected to PostgreSQL database via Docker Compose networking.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Commands: <pre><code># Start all services\ndocker-compose up -d\n\n# Check service connectivity\ndocker-compose exec web ping -c 2 db\n\n# View database logs\ndocker-compose logs db\n\n# Stop services\ndocker-compose down\n</code></pre></p> <p>Explanation:</p> <ul> <li>depends_on: Ensures services start in the correct order</li> <li>environment: Sets environment variables for service configuration</li> <li>volumes: Persists database data between container restarts</li> <li>Service networking: Services can communicate using service names as hostnames</li> <li>docker-compose exec: Runs commands in running service containers</li> </ul>"},{"location":"Tasks/DockerCompose/#scenario_1","title":"Scenario:","text":"<ul> <li>You\u2019re developing a full-stack application that requires both a web server and a database to work together.</li> <li>Docker Compose enables you to define and manage multiple interconnected services that can communicate with each other securely.</li> </ul>"},{"location":"Tasks/DockerCompose/#resources_1","title":"Resources:","text":"<ul> <li><code>web/index.html</code> \u27a4 <code>/usr/share/nginx/html</code> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;Multi-Service App&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Web App with Database&lt;/h1&gt;\n    &lt;p&gt;Connected to PostgreSQL database via Docker Compose networking.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></li> </ul>"},{"location":"Tasks/DockerCompose/#03-environment-variables-and-configuration","title":"03. Environment Variables and Configuration","text":"<ul> <li> <p>Create a configurable application stack using environment variables and .env files for different deployment environments.</p> </li> </ul> <p>Hint: Use <code>${VARIABLE_NAME}</code> syntax and .env files for configuration management</p> Solution <p>Solution:</p> <p>docker-compose.yml <pre><code>version: '3.8'\nservices:\n  web:\n    image: nginx:alpine\n    ports:\n      - \"${WEB_PORT:-8080}:80\"\n    environment:\n      - ENV=${APP_ENV:-development}\n    volumes:\n      - ./web:/usr/share/nginx/html\n\n  db:\n    image: postgres:13-alpine\n    environment:\n      POSTGRES_DB: ${DB_NAME}\n      POSTGRES_USER: ${DB_USER}\n      POSTGRES_PASSWORD: ${DB_PASSWORD}\n    volumes:\n      - db_data:/var/lib/postgresql/data\n\nvolumes:\n  db_data:\n</code></pre></p> <p>.env <pre><code>APP_ENV=development\nWEB_PORT=8080\nDB_NAME=myapp\nDB_USER=user\nDB_PASSWORD=password\n</code></pre></p> <p>web/index.html <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;Configurable App&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Environment: ${ENV}&lt;/h1&gt;\n    &lt;p&gt;Configuration loaded from environment variables.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Commands: <pre><code># Start with default .env\ndocker-compose up -d\n\n# Override environment variables\nWEB_PORT=3000 docker-compose up -d\n\n# Use different .env file\ndocker-compose --env-file .env.production up -d\n\n# Check environment in container\ndocker-compose exec web env | grep ENV\n</code></pre></p> <p>Explanation:</p> <ul> <li>${VARIABLE}: Substitutes environment variable values</li> <li>${VARIABLE:-default}: Provides default values for missing variables</li> <li>.env file: Automatically loaded by docker-compose</li> <li>\u2013env-file: Specify custom environment file</li> <li>Runtime overrides: Command-line variables override file values</li> </ul>"},{"location":"Tasks/DockerCompose/#scenario_2","title":"Scenario:","text":"<ul> <li>Your application needs to run in multiple environments (development, staging, production) with different configurations like database credentials, ports, and feature flags.</li> <li>Docker Compose allows you to manage environment-specific configurations using environment variables and .env files.</li> </ul>"},{"location":"Tasks/DockerCompose/#resources_2","title":"Resources:","text":"<ul> <li><code>.env</code> \u27a4 Environment configuration file     <pre><code>APP_ENV=development\nWEB_PORT=8080\nDB_NAME=myapp\nDB_USER=user\nDB_PASSWORD=password\n</code></pre></li> <li><code>web/index.html</code> \u27a4 <code>/usr/share/nginx/html</code> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;Configurable App&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Environment: ${ENV}&lt;/h1&gt;\n    &lt;p&gt;Configuration loaded from environment variables.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></li> </ul>"},{"location":"Tasks/DockerCompose/#04-volumes-and-data-persistence","title":"04. Volumes and Data Persistence","text":"<ul> <li> <p>Implement different types of volumes (named, bind mounts, tmpfs) for data persistence and performance optimization.</p> </li> </ul> <p>Hint: Use named volumes, bind mounts, and tmpfs for different data persistence needs</p> Solution <p>Solution:</p> <p>docker-compose.yml <pre><code>version: '3.8'\nservices:\n  web:\n    image: nginx:alpine\n    ports:\n      - \"8080:80\"\n    volumes:\n      - ./web:/usr/share/nginx/html:ro\n      - logs:/var/log/nginx\n\n  db:\n    image: postgres:13-alpine\n    environment:\n      POSTGRES_DB: myapp\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    volumes:\n      - db_data:/var/lib/postgresql/data\n      - ./init.sql:/docker-entrypoint-initdb.d/init.sql\n\n  cache:\n    image: redis:alpine\n    volumes:\n      - cache_data:/data\n      - type: tmpfs\n        target: /tmp\n        tmpfs:\n          size: 100m\n\nvolumes:\n  db_data:\n  logs:\n  cache_data:\n</code></pre></p> <p>Commands: <pre><code># Start services\ndocker-compose up -d\n\n# Check volume usage\ndocker volume ls\n\n# Inspect a volume\ndocker volume inspect $(docker-compose ps -q db)_db_data\n\n# View logs volume content\ndocker-compose exec web ls -la /var/log/nginx/\n\n# Clean up volumes\ndocker-compose down -v\n</code></pre></p> <p>Explanation:</p> <ul> <li>Named volumes: Managed by Docker, persist across container lifecycle</li> <li>Bind mounts: Host directories mounted into containers</li> <li>:ro: Read-only mount for security</li> <li>tmpfs: Temporary filesystem in memory</li> <li>Volume lifecycle: Separate from container lifecycle</li> <li>docker-compose down -v: Removes containers and volumes</li> </ul>"},{"location":"Tasks/DockerCompose/#scenario_3","title":"Scenario:","text":"<ul> <li>Your application stack requires different data persistence strategies - some data needs to persist across container restarts, some needs high performance, and some should be temporary.</li> <li>Docker Compose provides flexible volume management to handle various data persistence requirements.</li> </ul>"},{"location":"Tasks/DockerCompose/#05-networking-between-services","title":"05. Networking Between Services","text":"<ul> <li> <p>Configure custom networks in Docker Compose to control service communication and isolation.</p> </li> </ul> <p>Hint: Use custom networks with <code>internal: true</code> for backend isolation and multiple network connections</p> Solution <p>Solution:</p> <p>docker-compose.yml <pre><code>version: '3.8'\nservices:\n  web:\n    image: nginx:alpine\n    ports:\n      - \"8080:80\"\n    networks:\n      - frontend\n      - backend\n\n  api:\n    image: alpine:latest\n    networks:\n      - backend\n    command: sh -c \"busybox httpd -f -p 8000 -h /tmp\"\n\n  db:\n    image: postgres:13-alpine\n    environment:\n      POSTGRES_DB: myapp\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    networks:\n      - backend\n\nnetworks:\n  frontend:\n    driver: bridge\n  backend:\n    driver: bridge\n    internal: true\n</code></pre></p> <p>Commands: <pre><code># Start services\ndocker-compose up -d\n\n# Check networks\ndocker network ls\n\n# Test connectivity\ndocker-compose exec web ping -c 2 api\ndocker-compose exec web ping -c 2 db\ndocker-compose exec api ping -c 2 db\n\n# Check network isolation (should fail)\ncurl http://localhost:8000\n</code></pre></p> <p>Explanation:</p> <ul> <li>Custom networks: Isolated communication channels</li> <li>internal: true: Prevents external access to backend network</li> <li>Multiple networks: Services can connect to multiple networks</li> <li>Network isolation: Frontend accessible, backend internal only</li> <li>Service discovery: Services communicate using service names</li> </ul>"},{"location":"Tasks/DockerCompose/#scenario_4","title":"Scenario:","text":"<ul> <li>Your application has multiple services that need controlled communication - some services should be publicly accessible, others should only communicate internally, and some need complete isolation.</li> <li>Docker Compose networking allows you to create custom networks with specific connectivity rules.</li> </ul>"},{"location":"Tasks/DockerCompose/#06-health-checks-and-dependencies","title":"06. Health Checks and Dependencies","text":"<ul> <li> <p>Implement health checks and service dependencies to ensure proper startup order and fault tolerance.</p> </li> </ul> <p>Hint: Use <code>healthcheck</code> with <code>depends_on.condition: service_healthy</code> for proper service startup sequencing</p> Solution <p>Solution:</p> <p>docker-compose.yml <pre><code>version: '3.8'\nservices:\n  db:\n    image: postgres:13-alpine\n    environment:\n      POSTGRES_DB: myapp\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U user -d myapp\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n    volumes:\n      - db_data:/var/lib/postgresql/data\n\n  api:\n    image: alpine:latest\n    depends_on:\n      db:\n        condition: service_healthy\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--no-verbose\", \"--tries=1\", \"--spider\", \"http://localhost:8000/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n    command: sh -c 'echo \"OK\" &gt; /tmp/index.html &amp;&amp; busybox httpd -f -p 8000 -h /tmp'\n\n  web:\n    image: nginx:alpine\n    ports:\n      - \"8080:80\"\n    depends_on:\n      api:\n        condition: service_healthy\n    volumes:\n      - ./web:/usr/share/nginx/html\n\nvolumes:\n  db_data:\n</code></pre></p> <p>Commands: <pre><code># Start services (will wait for health checks)\ndocker-compose up -d\n\n# Monitor startup process\ndocker-compose logs -f\n\n# Check service health\ndocker-compose ps\n\n# Test health endpoints\ndocker-compose exec api curl http://localhost:8000/health\n</code></pre></p> <p>Explanation:</p> <ul> <li>healthcheck: Defines how to check service health</li> <li>depends_on.condition: Waits for healthy dependencies</li> <li>Startup sequencing: Services start only when dependencies are ready</li> <li>Fault tolerance: Automatic restarts for unhealthy services</li> <li>Health monitoring: Continuous health checking during runtime</li> </ul>"},{"location":"Tasks/DockerCompose/#scenario_5","title":"Scenario:","text":"<ul> <li>Your application services have complex dependencies and need to verify they\u2019re healthy before other services start depending on them.</li> <li>Docker Compose health checks ensure services are ready before dependent services start, improving application reliability.</li> </ul>"},{"location":"Tasks/DockerCompose/#07-scaling-services","title":"07. Scaling Services","text":"<ul> <li> <p>Scale services horizontally to handle increased load and implement load balancing.</p> </li> </ul> <p>Hint: Use <code>deploy.replicas</code> to scale services and configure load balancing with nginx upstream</p> Solution <p>Solution:</p> <p>docker-compose.yml <pre><code>version: '3.8'\nservices:\n  web:\n    image: nginx:alpine\n    ports:\n      - \"8080:80\"\n    volumes:\n      - ./web:/usr/share/nginx/html\n    deploy:\n      replicas: 3\n\n  api:\n    image: alpine:latest\n    command: sh -c \"\nwhile true; do\n  hostname=\\$(hostname)\n  { echo -e \\\"HTTP/1.1 200 OK\\r\\nContent-Type: text/plain\\r\\n\\r\\nAPI Instance: \\$hostname\\\"; } | nc -l -p 8000 -q 1 &gt; /dev/null\ndone\"\n    deploy:\n      replicas: 2\n\n  loadbalancer:\n    image: nginx:alpine\n    ports:\n      - \"9090:80\"\n    volumes:\n      - ./lb/nginx.conf:/etc/nginx/nginx.conf\n    depends_on:\n      - api\n\nnetworks:\n  default:\n    name: app-network\n</code></pre></p> <p>lb/nginx.conf <pre><code>events {\n    worker_connections 1024;\n}\n\nhttp {\n    upstream api_backend {\n        server api:8000;\n    }\n\n    server {\n        listen 80;\n\n        location /api {\n            proxy_pass http://api_backend;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n        }\n\n        location / {\n            proxy_pass http://web;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n        }\n    }\n}\n</code></pre></p> <p>Commands: <pre><code># Start scaled services\ndocker-compose up -d\n\n# Check running instances\ndocker-compose ps\n\n# Scale services dynamically\ndocker-compose up -d --scale api=3\n\n# Test load balancing\nfor i in {1..5}; do curl http://localhost:9090/api; done\n\n# Scale down\ndocker-compose up -d --scale web=1\n</code></pre></p> <p>Explanation:</p> <ul> <li>deploy.replicas: Specifies number of service instances</li> <li>Load balancing: Nginx distributes requests across instances</li> <li>Service discovery: Automatic load balancing between replicas</li> <li>Dynamic scaling: Change replica count without recreation</li> <li>Instance identification: Each replica has unique hostname</li> </ul>"},{"location":"Tasks/DockerCompose/#scenario_6","title":"Scenario:","text":"<ul> <li>Your application is experiencing high traffic and needs to handle more concurrent requests by running multiple instances of services.</li> <li>Docker Compose scaling allows you to run multiple instances of services and distribute load across them.</li> </ul>"},{"location":"Tasks/DockerCompose/#resources_3","title":"Resources:","text":"<ul> <li><code>lb/nginx.conf</code> \u27a4 <code>/etc/nginx/nginx.conf</code> <pre><code>events {\n    worker_connections 1024;\n}\n\nhttp {\n    upstream api_backend {\n        server api:8000;\n    }\n\n    server {\n        listen 80;\n\n        location /api {\n            proxy_pass http://api_backend;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n        }\n\n        location / {\n            proxy_pass http://web;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n        }\n    }\n}\n</code></pre></li> </ul>"},{"location":"Tasks/DockerCompose/#08-build-configuration","title":"08. Build Configuration","text":"<ul> <li> <p>Build custom images within Docker Compose using build contexts and Dockerfiles.</p> </li> </ul> <p>Hint: Use <code>build.context</code> and <code>build.args</code> to build custom images within Compose</p> Solution <p>Solution:</p> <p>docker-compose.yml <pre><code>version: '3.8'\nservices:\n  web:\n    build:\n      context: ./web\n      dockerfile: Dockerfile\n    ports:\n      - \"8080:80\"\n    environment:\n      - NODE_ENV=production\n\n  api:\n    build:\n      context: ./api\n      dockerfile: Dockerfile\n      args:\n        APP_VERSION: 1.0.0\n    ports:\n      - \"3000:3000\"\n    environment:\n      - DATABASE_URL=postgresql://user:pass@db:5432/myapp\n    depends_on:\n      - db\n\n  db:\n    image: postgres:13-alpine\n    environment:\n      POSTGRES_DB: myapp\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    volumes:\n      - db_data:/var/lib/postgresql/data\n\nvolumes:\n  db_data:\n</code></pre></p> <p>web/Dockerfile <pre><code>FROM node:18-alpine\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\nCOPY . .\nEXPOSE 80\nCMD [\"npm\", \"start\"]\n</code></pre></p> <p>api/Dockerfile <pre><code>FROM python:3.9-slim\nARG APP_VERSION\nENV VERSION=$APP_VERSION\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY . .\nEXPOSE 3000\nCMD [\"python\", \"app.py\"]\n</code></pre></p> <p>Commands: <pre><code># Build and start services\ndocker-compose up --build -d\n\n# Rebuild specific service\ndocker-compose build api\n\n# Build without cache\ndocker-compose build --no-cache\n\n# View build logs\ndocker-compose build --progress plain\n</code></pre></p> <p>Explanation:</p> <ul> <li>build.context: Directory containing Dockerfile and build context</li> <li>build.dockerfile: Specify custom Dockerfile name/location</li> <li>build.args: Pass build arguments to Dockerfile</li> <li>\u2013build: Force rebuild of images</li> <li>Selective building: Rebuild only specific services</li> <li>Build caching: Leverages Docker layer caching</li> </ul>"},{"location":"Tasks/DockerCompose/#scenario_7","title":"Scenario:","text":"<ul> <li>Your application requires custom Docker images that aren\u2019t available publicly, or you need to build images with specific configurations for your stack.</li> <li>Docker Compose can build images from Dockerfiles as part of the service definition.</li> </ul>"},{"location":"Tasks/DockerCompose/#resources_4","title":"Resources:","text":"<ul> <li><code>web/Dockerfile</code> \u27a4 Web service Dockerfile     <pre><code>FROM node:18-alpine\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\nCOPY . .\nEXPOSE 80\nCMD [\"npm\", \"start\"]\n</code></pre></li> <li><code>api/Dockerfile</code> \u27a4 API service Dockerfile     <pre><code>FROM python:3.9-slim\nARG APP_VERSION\nENV VERSION=$APP_VERSION\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY . .\nEXPOSE 3000\nCMD [\"python\", \"app.py\"]\n</code></pre></li> </ul>"},{"location":"Tasks/DockerCompose/#09-override-files","title":"09. Override Files","text":"<ul> <li> <p>Use multiple docker-compose files to manage different environments and configurations.</p> </li> </ul> <p>Hint: Use <code>docker-compose.override.yml</code> for development and custom override files for production</p> Solution <p>Solution:</p> <p>docker-compose.yml (base) <pre><code>version: '3.8'\nservices:\n  web:\n    image: nginx:alpine\n    ports:\n      - \"8080:80\"\n    volumes:\n      - ./web:/usr/share/nginx/html\n\n  db:\n    image: postgres:13-alpine\n    environment:\n      POSTGRES_DB: myapp\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    volumes:\n      - db_data:/var/lib/postgresql/data\n\nvolumes:\n  db_data:\n</code></pre></p> <p>docker-compose.override.yml (development) <pre><code>version: '3.8'\nservices:\n  web:\n    environment:\n      - DEBUG=true\n    volumes:\n      - ./web:/usr/share/nginx/html\n      - /app/node_modules\n\n  db:\n    ports:\n      - \"5432:5432\"\n    environment:\n      POSTGRES_PASSWORD: devpassword\n</code></pre></p> <p>docker-compose.prod.yml (production) <pre><code>version: '3.8'\nservices:\n  web:\n    deploy:\n      replicas: 3\n      resources:\n        limits:\n          memory: 512M\n          cpus: '0.5'\n    environment:\n      - NODE_ENV=production\n\n  db:\n    environment:\n      POSTGRES_PASSWORD: ${DB_PASSWORD}\n    deploy:\n      resources:\n        limits:\n          memory: 1G\n          cpus: '1.0'\n</code></pre></p> <p>Commands: <pre><code># Development (uses override automatically)\ndocker-compose up -d\n\n# Production\ndocker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d\n\n# Staging with custom override\ndocker-compose -f docker-compose.yml -f docker-compose.staging.yml up -d\n\n# List all services from combined files\ndocker-compose -f docker-compose.yml -f docker-compose.prod.yml config\n</code></pre></p> <p>Explanation:</p> <ul> <li>Automatic override: docker-compose.override.yml loaded automatically</li> <li>Multiple files: Use -f to specify multiple compose files</li> <li>Merging rules: Override files extend and modify base configuration</li> <li>Environment separation: Different settings for dev/staging/prod</li> <li>docker-compose config: Validate and view merged configuration</li> </ul>"},{"location":"Tasks/DockerCompose/#scenario_8","title":"Scenario:","text":"<ul> <li>Your application needs different configurations for development, testing, and production environments with varying resource requirements, logging levels, and service configurations.</li> <li>Docker Compose override files allow you to maintain a base configuration while applying environment-specific customizations.</li> </ul>"},{"location":"Tasks/DockerCompose/#resources_5","title":"Resources:","text":"<ul> <li><code>docker-compose.override.yml</code> \u27a4 Development overrides     <pre><code>version: '3.8'\nservices:\n  web:\n    environment:\n      - DEBUG=true\n    volumes:\n      - ./web:/usr/share/nginx/html\n      - /app/node_modules\n\n  db:\n    ports:\n      - \"5432:5432\"\n    environment:\n      POSTGRES_PASSWORD: devpassword\n</code></pre></li> <li><code>docker-compose.prod.yml</code> \u27a4 Production overrides     <pre><code>version: '3.8'\nservices:\n  web:\n    deploy:\n      replicas: 3\n      resources:\n        limits:\n          memory: 512M\n          cpus: '0.5'\n    environment:\n      - NODE_ENV=production\n\n  db:\n    environment:\n      POSTGRES_PASSWORD: ${DB_PASSWORD}\n    deploy:\n      resources:\n        limits:\n          memory: 1G\n          cpus: '1.0'\n</code></pre></li> </ul>"},{"location":"Tasks/DockerCompose/#10-secrets-management","title":"10. Secrets Management","text":"<ul> <li> <p>Securely manage sensitive data like passwords and API keys using Docker Compose secrets.</p> </li> </ul> <p>Hint: Use <code>secrets</code> with <code>file</code> source to securely pass sensitive data to containers</p> Solution <p>Solution:</p> <p>docker-compose.yml <pre><code>version: '3.8'\nservices:\n  web:\n    image: nginx:alpine\n    ports:\n      - \"8080:80\"\n    secrets:\n      - source: nginx_cert\n        target: /etc/ssl/certs/nginx.crt\n      - source: nginx_key\n        target: /etc/ssl/private/nginx.key\n\n  db:\n    image: postgres:13-alpine\n    environment:\n      POSTGRES_DB: myapp\n      POSTGRES_USER: user\n    secrets:\n      - source: db_password\n        target: postgres_password\n    command: &gt;\n      sh -c \"\n      export POSTGRES_PASSWORD=$(cat /run/secrets/postgres_password) &amp;&amp;\n      docker-entrypoint.sh postgres\n      \"\n    volumes:\n      - db_data:/var/lib/postgresql/data\n\n  api:\n    image: alpine:latest\n    secrets:\n      - source: api_key\n        target: /run/secrets/api_key\n    environment:\n      - API_KEY_FILE=/run/secrets/api_key\n\nsecrets:\n  nginx_cert:\n    file: ./secrets/nginx.crt\n  nginx_key:\n    file: ./secrets/nginx.key\n  db_password:\n    file: ./secrets/db_password.txt\n  api_key:\n    file: ./secrets/api_key.txt\n\nvolumes:\n  db_data:\n</code></pre></p> <p>Commands: <pre><code># Create secrets directory\nmkdir -p secrets\n\n# Create secret files (never commit these)\necho \"mysecretpassword\" &gt; secrets/db_password.txt\necho \"sk-1234567890abcdef\" &gt; secrets/api_key.txt\n\n# Start services\ndocker-compose up -d\n\n# Check secrets in containers\ndocker-compose exec api cat /run/secrets/api_key\n\n# Clean up (remove secrets)\ndocker-compose down\nrm -rf secrets/\n</code></pre></p> <p>Explanation:</p> <ul> <li>secrets: Secure way to pass sensitive data to containers</li> <li>file source: Load secrets from external files</li> <li>target: Location where secret is mounted in container</li> <li>Runtime only: Secrets not stored in image layers</li> <li>Access control: Secrets only available to specified services</li> <li>External management: Secrets can be managed by external systems</li> </ul>"},{"location":"Tasks/DockerCompose/#scenario_9","title":"Scenario:","text":"<ul> <li>Your application requires sensitive information like database passwords, API keys, and certificates that shouldn\u2019t be stored in plain text in your compose files.</li> <li>Docker Compose secrets provide a secure way to manage sensitive data and make it available to services at runtime.</li> </ul>"},{"location":"Tasks/DockerCompose/#11-logging-configuration","title":"11. Logging Configuration","text":"<ul> <li> <p>Configure centralized logging for all services in a Docker Compose stack.</p> </li> </ul> <p>Hint: Use <code>logging.driver</code> and <code>logging.options</code> to configure log rotation and formatting</p> Solution <p>Solution:</p> <p>docker-compose.yml <pre><code>version: '3.8'\nservices:\n  web:\n    image: nginx:alpine\n    ports:\n      - \"8080:80\"\n    logging:\n      driver: json-file\n      options:\n        max-size: \"10m\"\n        max-file: \"3\"\n\n  api:\n    image: alpine:latest\n    logging:\n      driver: json-file\n      options:\n        max-size: \"20m\"\n        max-file: \"5\"\n        labels: \"service\"\n    command: sh -c \"\nwhile true; do\n  echo \\\"\\$(date '+%Y-%m-%d %H:%M:%S') - INFO - API request processed\\\"\n  sleep 5\ndone\"\n\n  db:\n    image: postgres:13-alpine\n    environment:\n      POSTGRES_DB: myapp\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    logging:\n      driver: json-file\n      options:\n        max-size: \"50m\"\n        max-file: \"2\"\n    volumes:\n      - db_data:/var/lib/postgresql/data\n\n  log-collector:\n    image: fluent/fluent-bit:latest\n    volumes:\n      - /var/lib/docker/containers:/var/lib/docker/containers:ro\n      - ./fluent-bit.conf:/fluent-bit/etc/fluent-bit.conf\n    logging:\n      driver: json-file\n\nvolumes:\n  db_data:\n</code></pre></p> <p>Commands: <pre><code># Start services\ndocker-compose up -d\n\n# View logs for specific service\ndocker-compose logs web\n\n# View logs with timestamps\ndocker-compose logs --timestamps api\n\n# Follow logs in real-time\ndocker-compose logs -f\n\n# View logs for all services\ndocker-compose logs\n\n# Export logs to file\ndocker-compose logs &gt; app_logs.txt\n</code></pre></p> <p>Explanation:</p> <ul> <li>logging.driver: Specifies the logging driver (json-file, syslog, etc.)</li> <li>max-size: Maximum size of log files before rotation</li> <li>max-file: Maximum number of log files to keep</li> <li>Log rotation: Automatic log file management</li> <li>Centralized logging: Collect logs from multiple services</li> <li>Log filtering: View logs by service or time range</li> </ul>"},{"location":"Tasks/DockerCompose/#scenario_10","title":"Scenario:","text":"<ul> <li>Your multi-service application generates logs from different components, and you need to centralize logging for monitoring, debugging, and compliance purposes.</li> <li>Docker Compose logging configuration allows you to define logging drivers and options for consistent log management across all services.</li> </ul>"},{"location":"Tasks/DockerCompose/#12-resource-limits","title":"12. Resource Limits","text":"<ul> <li> <p>Set CPU and memory limits for services to ensure fair resource allocation and prevent resource exhaustion.</p> </li> </ul> <p>Hint: Use <code>deploy.resources.limits</code> and <code>deploy.resources.reservations</code> for CPU and memory control</p> Solution <p>Solution:</p> <p>docker-compose.yml <pre><code>version: '3.8'\nservices:\n  web:\n    image: nginx:alpine\n    ports:\n      - \"8080:80\"\n    deploy:\n      resources:\n        limits:\n          memory: 256M\n          cpus: '0.5'\n        reservations:\n          memory: 128M\n          cpus: '0.25'\n\n  api:\n    image: alpine:latest\n    deploy:\n      resources:\n        limits:\n          memory: 512M\n          cpus: '1.0'\n        reservations:\n          memory: 256M\n          cpus: '0.5'\n    command: sh -c \"\nwhile true; do\n  mem=\\$(free | grep Mem | awk '{printf \\\"%.0f\\\", \\$3/\\$2 * 100.0}')\n  cpu=\\$(top -bn1 | grep 'Cpu(s)' | sed 's/.*, *\\([0-9.]*\\)%* id.*/\\1/' | awk '{print 100 - \\$1}')\n  echo \\\"Memory: \\${mem}%, CPU: \\${cpu}%\\\"\n  sleep 10\ndone\"\n\n  db:\n    image: postgres:13-alpine\n    environment:\n      POSTGRES_DB: myapp\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    deploy:\n      resources:\n        limits:\n          memory: 1G\n          cpus: '2.0'\n        reservations:\n          memory: 512M\n          cpus: '1.0'\n    volumes:\n      - db_data:/var/lib/postgresql/data\n\n  monitoring:\n    image: prom/prometheus:latest\n    ports:\n      - \"9090:9090\"\n    volumes:\n      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml\n    deploy:\n      resources:\n        limits:\n          memory: 256M\n          cpus: '0.25'\n\nvolumes:\n  db_data:\n</code></pre></p> <p>Commands: <pre><code># Start services with resource limits\ndocker-compose up -d\n\n# Check resource usage\ndocker stats\n\n# View resource limits\ndocker-compose ps\n\n# Monitor specific container\ndocker stats $(docker-compose ps -q api)\n\n# Check if limits are enforced (try to consume more memory)\ndocker-compose exec api sh -c \"\ndata=''\nwhile true; do\n  data=\\\"\\$data\\$data\\\"\n  sleep 0.1\ndone\"\n</code></pre></p> <p>Explanation:</p> <ul> <li>limits: Hard limits that containers cannot exceed</li> <li>reservations: Guaranteed minimum resources</li> <li>memory: RAM limits (supports m, g suffixes)</li> <li>cpus: CPU core limits (supports decimal values)</li> <li>Resource enforcement: Docker prevents exceeding limits</li> <li>Monitoring: Track resource usage with docker stats</li> </ul>"},{"location":"Tasks/DockerCompose/#scenario_11","title":"Scenario:","text":"<ul> <li>Your application runs multiple services on shared infrastructure, and you need to ensure that no single service can consume all available resources and impact other services.</li> <li>Docker Compose resource limits allow you to control CPU and memory usage for each service.</li> </ul>"},{"location":"Tasks/DockerCompose/#13-profiles-and-selective-services","title":"13. Profiles and Selective Services","text":"<ul> <li> <p>Use profiles to run different combinations of services for development, testing, and production.</p> </li> </ul> <p>Hint: Use <code>profiles</code> to group services and <code>--profile</code> flag to run specific combinations</p> Solution <p>Solution:</p> <p>docker-compose.yml <pre><code>version: '3.8'\nservices:\n  web:\n    image: nginx:alpine\n    ports:\n      - \"8080:80\"\n    volumes:\n      - ./web:/usr/share/nginx/html\n    profiles:\n      - web\n      - full\n\n  api:\n    image: alpine:latest\n    profiles:\n      - api\n      - full\n    command: sh -c 'echo \"API Response\" &gt; /tmp/index.html &amp;&amp; busybox httpd -f -p 8000 -h /tmp'\n\n  db:\n    image: postgres:13-alpine\n    environment:\n      POSTGRES_DB: myapp\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    profiles:\n      - database\n      - full\n    volumes:\n      - db_data:/var/lib/postgresql/data\n\n  debug:\n    image: alpine:latest\n    profiles:\n      - debug\n    command: tail -f /dev/null\n    volumes:\n      - .:/app\n\n  test:\n    image: python:3.9-alpine\n    profiles:\n      - test\n    command: python -m pytest /app/tests/\n    volumes:\n      - .:/app\n\n  monitoring:\n    image: prom/prometheus:latest\n    ports:\n      - \"9090:9090\"\n    profiles:\n      - monitoring\n      - production\n    volumes:\n      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml\n\nvolumes:\n  db_data:\n</code></pre></p> <p>Commands: <pre><code># Run only web service\ndocker-compose --profile web up -d\n\n# Run web with database\ndocker-compose --profile web --profile database up -d\n\n# Run full application stack\ndocker-compose --profile full up -d\n\n# Run tests\ndocker-compose --profile test up\n\n# Development with debug tools\ndocker-compose --profile full --profile debug up -d\n\n# Production with monitoring\ndocker-compose --profile full --profile production up -d\n</code></pre></p> <p>Explanation:</p> <ul> <li>profiles: Assign services to specific profiles</li> <li>\u2013profile: Activate specific profiles when running</li> <li>Service grouping: Logical grouping of related services</li> <li>Environment-specific: Different service combinations per environment</li> <li>Selective deployment: Run only needed services for each use case</li> </ul>"},{"location":"Tasks/DockerCompose/#scenario_12","title":"Scenario:","text":"<ul> <li>Your application has services that are only needed in certain environments - debugging tools for development, testing services for CI/CD, and monitoring services for production.</li> <li>Docker Compose profiles allow you to define which services run in different scenarios without modifying the compose file.</li> </ul>"},{"location":"Tasks/DockerCompose/#14-external-networks","title":"14. External Networks","text":"<ul> <li> <p>Connect Docker Compose services to existing external networks for integration with other applications.</p> </li> </ul> <p>Hint: Use <code>external: true</code> and <code>name</code> to connect to existing Docker networks</p> Solution <p>Solution:</p> <p>docker-compose.yml <pre><code>version: '3.8'\nservices:\n  web:\n    image: nginx:alpine\n    ports:\n      - \"8080:80\"\n    networks:\n        - app-network\n        - external-network\n\n  api:\n    image: alpine:latest\n    networks:\n      - app-network\n      - external-network\n    command: sh -c \"busybox httpd -f -p 8000 -h /tmp\"\n\n  legacy-connector:\n    image: alpine:latest\n    networks:\n      - external-network\n    command: ping -c 4 legacy-service\n\nnetworks:\n  app-network:\n    driver: bridge\n  external-network:\n    external: true\n    name: company-network\n</code></pre></p> <p>Setup Commands: <pre><code># Create external network (if not exists)\ndocker network create company-network\n\n# Start external service\ndocker run -d --name legacy-service --network company-network alpine sleep infinity\n\n# Start compose services\ndocker-compose up -d\n\n# Test connectivity\ndocker-compose exec legacy-connector ping -c 2 legacy-service\n\n# Check networks\ndocker network ls\ndocker network inspect company-network\n</code></pre></p> <p>Explanation:</p> <ul> <li>external: true: Connect to existing Docker network</li> <li>name: Specify the exact network name</li> <li>Network sharing: Services can communicate across compose files</li> <li>Legacy integration: Connect to existing infrastructure</li> <li>Network isolation: Control which services access external networks</li> </ul>"},{"location":"Tasks/DockerCompose/#scenario_13","title":"Scenario:","text":"<ul> <li>Your application needs to communicate with services running outside of Docker Compose, such as a company-wide database or legacy applications.</li> <li>Docker Compose external networks allow services to connect to pre-existing Docker networks.</li> </ul>"},{"location":"Tasks/DockerCompose/#15-custom-networks","title":"15. Custom Networks","text":"<ul> <li> <p>Create custom networks with specific configurations for advanced networking requirements.</p> </li> </ul> <p>Hint: Use <code>ipam.config</code> to define custom subnets and <code>ipv4_address</code> for static IP assignment</p> Solution <p>Solution:</p> <p>docker-compose.yml <pre><code>version: '3.8'\nservices:\n  web:\n    image: nginx:alpine\n    ports:\n      - \"8080:80\"\n    networks:\n      frontend:\n        ipv4_address: 172.20.0.10\n\n  api:\n    image: alpine:latest\n    networks:\n      frontend:\n        ipv4_address: 172.20.0.11\n      backend:\n        ipv4_address: 172.21.0.10\n    command: sh -c \"busybox httpd -f -p 8000 -h /tmp\"\n\n  db:\n    image: postgres:13-alpine\n    environment:\n      POSTGRES_DB: myapp\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    networks:\n      backend:\n        ipv4_address: 172.21.0.11\n    volumes:\n      - db_data:/var/lib/postgresql/data\n\n  monitoring:\n    image: alpine:latest\n    networks:\n      frontend:\n        ipv4_address: 172.20.0.12\n      backend:\n        ipv4_address: 172.21.0.12\n    command: watch -n 5 'echo \"Monitoring networks\"'\n\nnetworks:\n  frontend:\n    driver: bridge\n    ipam:\n      config:\n        - subnet: 172.20.0.0/16\n          gateway: 172.20.0.1\n  backend:\n    driver: bridge\n    internal: true\n    ipam:\n      config:\n        - subnet: 172.21.0.0/16\n          gateway: 172.21.0.1\n\nvolumes:\n  db_data:\n</code></pre></p> <p>Commands: <pre><code># Start services with custom networks\ndocker-compose up -d\n\n# Check network configurations\ndocker network ls\ndocker network inspect $(docker-compose ps -q web | xargs docker inspect | jq -r '.[0].NetworkSettings.Networks | keys[]' | head -1)\n\n# Test connectivity\ndocker-compose exec web ping -c 2 172.20.0.11\ndocker-compose exec api ping -c 2 172.21.0.11\n\n# Check IP assignments\ndocker-compose exec web ip addr show eth0\n</code></pre></p> <p>Explanation:</p> <ul> <li>ipam.config: Define custom IP address management</li> <li>subnet: IP address range for the network</li> <li>ipv4_address: Assign static IP to specific services</li> <li>internal: true: Prevent external access to backend network</li> <li>Network segmentation: Separate frontend and backend traffic</li> <li>IP predictability: Static IPs for consistent service communication</li> </ul>"},{"location":"Tasks/DockerCompose/#scenario_14","title":"Scenario:","text":"<ul> <li>Your application requires specific network configurations like custom subnets, IP ranges, or network drivers for security, performance, or compliance reasons.</li> <li>Docker Compose custom networks allow you to define network properties like IP ranges, subnets, and drivers.</li> </ul>"},{"location":"Tasks/DockerCompose/#16-load-balancing","title":"16. Load Balancing","text":"<ul> <li> <p>Implement load balancing across multiple service instances for high availability and scalability.</p> </li> </ul> <p>Hint: Use nginx as load balancer with upstream blocks and deploy.replicas for scaling</p> Solution <p>Solution:</p> <p>docker-compose.yml <pre><code>version: '3.8'\nservices:\n  loadbalancer:\n    image: nginx:alpine\n    ports:\n      - \"8080:80\"\n    volumes:\n      - ./nginx/loadbalancer.conf:/etc/nginx/nginx.conf\n    depends_on:\n      - web\n\n  web:\n    image: nginx:alpine\n    volumes:\n      - ./web:/usr/share/nginx/html\n    deploy:\n      replicas: 3\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost/\"]\n      interval: 10s\n      timeout: 5s\n      retries: 3\n\n  api:\n    image: alpine:latest\n    deploy:\n      replicas: 2\n    command: sh -c \"\nwhile true; do\n  hostname=\\$(hostname)\n  { echo -e \\\"HTTP/1.1 200 OK\\r\\nContent-Type: text/plain\\r\\n\\r\\nAPI Response from \\$hostname\\\"; } | nc -l -p 8000 -q 1 &gt; /dev/null\ndone\"\n    healthcheck:\n      test: [\"CMD\", \"echo\", \"OK\"]\n      interval: 15s\n      timeout: 5s\n      retries: 3\n\n  redis:\n    image: redis:alpine\n    volumes:\n      - redis_data:/data\n\nvolumes:\n  redis_data:\n</code></pre></p> <p>nginx/loadbalancer.conf <pre><code>events {\n    worker_connections 1024;\n}\n\nhttp {\n    upstream web_backend {\n        server web:80;\n    }\n\n    upstream api_backend {\n        server api:8000;\n    }\n\n    server {\n        listen 80;\n\n        location / {\n            proxy_pass http://web_backend;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n        }\n\n        location /api {\n            proxy_pass http://api_backend;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n        }\n    }\n}\n</code></pre></p> <p>Commands: <pre><code># Start load balanced services\ndocker-compose up -d\n\n# Test load balancing\nfor i in {1..10}; do curl -s http://localhost:8080/api | grep \"API Response\"; done\n\n# Check service instances\ndocker-compose ps\n\n# Scale services\ndocker-compose up -d --scale web=5\n\n# Test failover (stop one instance)\ndocker-compose exec web.1 nginx -s stop\n# Requests should still work\ncurl http://localhost:8080/\n</code></pre></p> <p>Explanation:</p> <ul> <li>upstream blocks: Define backend server groups for load balancing</li> <li>proxy_pass: Forward requests to backend services</li> <li>deploy.replicas: Create multiple instances for load distribution</li> <li>healthcheck: Ensure only healthy instances receive traffic</li> <li>Automatic failover: Traffic rerouted when instances become unhealthy</li> <li>Session persistence: Optional sticky sessions for stateful applications</li> </ul>"},{"location":"Tasks/DockerCompose/#scenario_15","title":"Scenario:","text":"<ul> <li>Your application needs to handle high traffic loads and provide fault tolerance by distributing requests across multiple service instances.</li> <li>Docker Compose load balancing works with service scaling to automatically distribute traffic across healthy instances.</li> </ul>"},{"location":"Tasks/DockerCompose/#resources_6","title":"Resources:","text":"<ul> <li><code>nginx/loadbalancer.conf</code> \u27a4 <code>/etc/nginx/nginx.conf</code> <pre><code>events {\n    worker_connections 1024;\n}\n\nhttp {\n    upstream web_backend {\n        server web:80;\n    }\n\n    upstream api_backend {\n        server api:8000;\n    }\n\n    server {\n        listen 80;\n\n        location / {\n            proxy_pass http://web_backend;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n        }\n\n        location /api {\n            proxy_pass http://api_backend;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n        }\n    }\n}\n</code></pre></li> </ul>"},{"location":"Tasks/DockerCompose/#17-multi-stage-deployments","title":"17. Multi-Stage Deployments","text":"<ul> <li> <p>Implement blue-green or canary deployment strategies using Docker Compose.</p> </li> </ul> <p>Hint: Use profiles to control which version runs and nginx upstream weights for traffic shifting</p> Solution <p>Solution:</p> <p>docker-compose.yml (base) <pre><code>version: '3.8'\nservices:\n  loadbalancer:\n    image: nginx:alpine\n    ports:\n      - \"8080:80\"\n    volumes:\n      - ./nginx/nginx.conf:/etc/nginx/nginx.conf\n\n  web-v1:\n    image: nginx:alpine\n    volumes:\n      - ./web/v1:/usr/share/nginx/html\n    environment:\n      - VERSION=v1\n    deploy:\n      replicas: 2\n\n  web-v2:\n    image: nginx:alpine\n    volumes:\n      - ./web/v2:/usr/share/nginx/html\n    environment:\n      - VERSION=v2\n    deploy:\n      replicas: 2\n    profiles:\n      - v2\n\n  api-v1:\n    image: alpine:latest\n    environment:\n      - VERSION=v1\n    command: sh -c 'echo \"API v1\" &gt; /tmp/index.html &amp;&amp; busybox httpd -f -p 8000 -h /tmp'\n    deploy:\n      replicas: 2\n\n  api-v2:\n    image: alpine:latest\n    environment:\n      - VERSION=v2\n    command: sh -c 'echo \"API v2 - New Feature!\" &gt; /tmp/index.html &amp;&amp; busybox httpd -f -p 8000 -h /tmp'\n    deploy:\n      replicas: 2\n    profiles:\n      - v2\n</code></pre></p> <p>nginx/nginx.conf (canary deployment - 90% v1, 10% v2) <pre><code>events {\n    worker_connections 1024;\n}\n\nhttp {\n    upstream web_backend {\n        server web-v1:80 weight=9;\n        server web-v2:80 weight=1;\n    }\n\n    upstream api_backend {\n        server api-v1:8000 weight=9;\n        server api-v2:8000 weight=1;\n    }\n\n    server {\n        listen 80;\n\n        location / {\n            proxy_pass http://web_backend;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n        }\n\n        location /api {\n            proxy_pass http://api_backend;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n        }\n    }\n}\n</code></pre></p> <p>Commands: <pre><code># Start with only v1\ndocker-compose up -d\n\n# Test current version\nfor i in {1..10}; do curl -s http://localhost:8080/api; done\n\n# Deploy v2 alongside v1 (canary)\ndocker-compose --profile v2 up -d\n\n# Update nginx config for canary deployment\n# (edit nginx.conf to add v2 with low weight)\n\n# Reload nginx config\ndocker-compose exec loadbalancer nginx -s reload\n\n# Test canary deployment\nfor i in {1..20}; do curl -s http://localhost:8080/api; done\n\n# Full rollout to v2 (blue-green)\n# Update nginx.conf to send all traffic to v2\ndocker-compose exec loadbalancer nginx -s reload\n\n# Remove v1 services\ndocker-compose rm -f web-v1 api-v1\n</code></pre></p> <p>Explanation:</p> <ul> <li>Canary deployment: Route small percentage of traffic to new version</li> <li>Blue-green deployment: Switch all traffic to new version at once</li> <li>Profiles: Control which version services are running</li> <li>Load balancer config: Dynamic traffic shifting without downtime</li> <li>Rollback capability: Quickly switch back to previous version</li> <li>Zero downtime: New version tested before full rollout</li> </ul>"},{"location":"Tasks/DockerCompose/#scenario_16","title":"Scenario:","text":"<ul> <li>You need to deploy application updates with zero downtime and the ability to quickly rollback if issues occur.</li> <li>Multi-stage deployments with Docker Compose allow you to run multiple versions of your application simultaneously and gradually shift traffic.</li> </ul>"},{"location":"Tasks/DockerCompose/#resources_7","title":"Resources:","text":"<ul> <li><code>nginx/nginx.conf</code> \u27a4 Load balancer configuration for canary deployment     <pre><code>events {\n    worker_connections 1024;\n}\n\nhttp {\n    upstream web_backend {\n        server web-v1:80 weight=9;\n        server web-v2:80 weight=1;\n    }\n\n    upstream api_backend {\n        server api-v1:8000 weight=9;\n        server api-v2:8000 weight=1;\n    }\n\n    server {\n        listen 80;\n\n        location / {\n            proxy_pass http://web_backend;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n        }\n\n        location /api {\n            proxy_pass http://api_backend;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n        }\n    }\n}\n</code></pre></li> </ul>"},{"location":"Tasks/DockerCompose/#18-production-ready-stack","title":"18. Production-Ready Stack","text":"<ul> <li> <p>Create a complete production-ready application stack with monitoring, logging, and security best practices.</p> </li> </ul> <p>Hint: Combine all production best practices: health checks, secrets, resource limits, logging, monitoring, and backups</p> Solution <p>Solution:</p> <p>docker-compose.yml <pre><code>version: '3.8'\nservices:\n  web:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - ./web:/usr/share/nginx/html:ro\n      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro\n      - ./ssl:/etc/ssl/certs:ro\n    depends_on:\n      - api\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n    deploy:\n      resources:\n        limits:\n          memory: 256M\n          cpus: '0.5'\n      restart_policy:\n        condition: on-failure\n        delay: 5s\n        max_attempts: 3\n        window: 120s\n\n  api:\n    image: alpine:latest\n    environment:\n      - DATABASE_URL=postgresql://user:pass@db:5432/myapp\n      - REDIS_URL=redis://redis:6379\n    secrets:\n      - source: api_key\n        target: /run/secrets/api_key\n    depends_on:\n      db:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--no-verbose\", \"--tries=1\", \"--spider\", \"http://localhost:8000/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n    deploy:\n      resources:\n        limits:\n          memory: 512M\n          cpus: '1.0'\n    logging:\n      driver: json-file\n      options:\n        max-size: \"10m\"\n        max-file: \"3\"\n\n  db:\n    image: postgres:13-alpine\n    environment:\n      POSTGRES_DB: myapp\n      POSTGRES_USER: user\n    secrets:\n      - source: db_password\n        target: postgres_password\n    volumes:\n      - db_data:/var/lib/postgresql/data\n      - ./backup:/backup\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U user -d myapp\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n    deploy:\n      resources:\n        limits:\n          memory: 1G\n          cpus: '2.0'\n    logging:\n      driver: json-file\n      options:\n        max-size: \"50m\"\n        max-file: \"5\"\n\n  redis:\n    image: redis:alpine\n    volumes:\n      - redis_data:/data\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 10s\n      timeout: 3s\n      retries: 3\n    deploy:\n      resources:\n        limits:\n          memory: 256M\n          cpus: '0.5'\n\n  backup:\n    image: postgres:13-alpine\n    volumes:\n      - db_data:/var/lib/postgresql/data:ro\n      - ./backup:/backup\n    command: &gt;\n      sh -c \"\n      while true; do\n        pg_dump -U user -h db myapp &gt; /backup/backup_$(date +%Y%m%d_%H%M%S).sql\n        sleep 3600\n      done\n      \"\n    depends_on:\n      - db\n    profiles:\n      - backup\n\n  monitoring:\n    image: prom/prometheus:latest\n    ports:\n      - \"9090:9090\"\n    volumes:\n      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro\n      - monitoring_data:/prometheus\n    command:\n      - '--config.file=/etc/prometheus/prometheus.yml'\n      - '--storage.tsdb.path=/prometheus'\n      - '--web.console.libraries=/etc/prometheus/console_libraries'\n      - '--web.console.templates=/etc/prometheus/consoles'\n      - '--storage.tsdb.retention.time=200h'\n      - '--web.enable-lifecycle'\n    profiles:\n      - monitoring\n\n  log-aggregator:\n    image: fluent/fluent-bit:latest\n    volumes:\n      - /var/lib/docker/containers:/var/lib/docker/containers:ro\n      - ./fluent-bit.conf:/fluent-bit/etc/fluent-bit.conf:ro\n    profiles:\n      - logging\n\nsecrets:\n  api_key:\n    file: ./secrets/api_key.txt\n  db_password:\n    file: ./secrets/db_password.txt\n\nvolumes:\n  db_data:\n    driver: local\n  redis_data:\n    driver: local\n  monitoring_data:\n    driver: local\n\nnetworks:\n  frontend:\n    driver: bridge\n  backend:\n    driver: bridge\n    internal: true\n</code></pre></p> <p>Commands: <pre><code># Create secrets\nmkdir -p secrets\necho \"my-secret-api-key\" &gt; secrets/api_key.txt\necho \"my-secret-db-password\" &gt; secrets/db_password.txt\n\n# Start production stack\ndocker-compose up -d\n\n# Start with monitoring\ndocker-compose --profile monitoring up -d\n\n# Start with logging\ndocker-compose --profile logging up -d\n\n# Start backup service\ndocker-compose --profile backup up -d\n\n# Check all services\ndocker-compose ps\n\n# View logs\ndocker-compose logs -f\n\n# Monitor resources\ndocker stats\n\n# Backup database\ndocker-compose exec backup ls /backup/\n</code></pre></p> <p>Explanation:</p> <ul> <li>Production hardening: Resource limits, health checks, secrets management</li> <li>Monitoring: Prometheus for metrics collection</li> <li>Logging: Centralized log aggregation with Fluent Bit</li> <li>Backup: Automated database backups</li> <li>Security: Secrets, read-only volumes, network isolation</li> <li>High availability: Health checks, restart policies, resource management</li> <li>Scalability: Profiles for optional services, resource limits</li> <li>Maintainability: Structured configuration, clear separation of concerns</li> </ul>"},{"location":"Tasks/DockerCompose/#scenario_17","title":"Scenario:","text":"<ul> <li>You\u2019re deploying a critical application to production that requires monitoring, centralized logging, security hardening, and automated backups.</li> <li>A production-ready Docker Compose stack includes all necessary components for running applications reliably in production environments.</li> </ul>"},{"location":"Tasks/DockerCompose/#19-basic-yaml-anchors-and-references","title":"19. Basic YAML Anchors and References","text":"<ul> <li> <p>Learn the fundamentals of YAML anchors and references to avoid repetition in Docker Compose files.</p> </li> </ul> <p>Hint: Use <code>&amp;anchor-name</code> to define reusable blocks and <code>*anchor-name</code> to reference them</p> Solution <p>Solution:</p> <p>docker-compose.yml <pre><code>version: '3.8'\n\nx-common-env: &amp;common-env\n  APP_ENV: production\n  LOG_LEVEL: info\n  TZ: UTC\n\nx-common-volumes: &amp;common-volumes\n  - ./logs:/app/logs\n  - ./config:/app/config:ro\n\nx-common-deploy: &amp;common-deploy\n  resources:\n    limits:\n      memory: 256M\n      cpus: '0.5'\n    restart_policy:\n      condition: on-failure\n\nservices:\n  web:\n    image: nginx:alpine\n    environment:\n      &lt;&lt;: *common-env\n      SERVICE_NAME: web\n    volumes: *common-volumes\n    ports:\n      - \"8080:80\"\n    deploy: *common-deploy\n\n  api:\n    image: node:18-alpine\n    environment:\n      &lt;&lt;: *common-env\n      SERVICE_NAME: api\n      DATABASE_URL: postgresql://db:5432/myapp\n    volumes: *common-volumes\n    ports:\n      - \"3000:3000\"\n    deploy: *common-deploy\n    depends_on:\n      - db\n\n  db:\n    image: postgres:13-alpine\n    environment:\n      POSTGRES_DB: myapp\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    volumes:\n      - db_data:/var/lib/postgresql/data\n    deploy: *common-deploy\n\nvolumes:\n  db_data:\n</code></pre></p> <p>Commands: <pre><code># Start services\ndocker-compose up -d\n\n# Check environment variables\ndocker-compose exec web env | grep APP_ENV\ndocker-compose exec api env | grep SERVICE_NAME\n\n# View configuration\ndocker-compose config\n\n# Stop services\ndocker-compose down\n</code></pre></p> <p>Explanation:</p> <ul> <li>x- prefix: Extension fields for reusable configurations</li> <li>&amp;anchor: Defines a named anchor for later reference</li> <li>***reference**: References the anchored configuration</li> <li>&lt;&lt;: *anchor: Merges the referenced configuration</li> <li>DRY principle: Avoids repetition in compose files</li> <li>Maintainability: Changes to common config affect all services</li> </ul>"},{"location":"Tasks/DockerCompose/#scenario_18","title":"Scenario:","text":"<ul> <li>You have multiple services that share common configuration elements like environment variables, volumes, or network settings.</li> <li>YAML anchors (&amp;) and references (*) allow you to define reusable configuration blocks and reference them throughout your compose file.</li> </ul>"},{"location":"Tasks/DockerCompose/#resources_8","title":"Resources:","text":"<ul> <li><code>docker-compose.yml</code> \u27a4 Main compose file with anchors and references     <pre><code>version: '3.8'\n\nx-common-env: &amp;common-env\n  APP_ENV: production\n  LOG_LEVEL: info\n\nx-common-volumes: &amp;common-volumes\n  - ./logs:/app/logs\n  - ./config:/app/config:ro\n\nservices:\n  web:\n    image: nginx:alpine\n    environment:\n      &lt;&lt;: *common-env\n      SERVICE_NAME: web\n    volumes: *common-volumes\n    ports:\n      - \"8080:80\"\n\n  api:\n    image: node:18-alpine\n    environment:\n      &lt;&lt;: *common-env\n      SERVICE_NAME: api\n    volumes: *common-volumes\n    ports:\n      - \"3000:3000\"\n</code></pre></li> </ul>"},{"location":"Tasks/DockerCompose/#20-merging-service-configurations","title":"20. Merging Service Configurations","text":"<ul> <li> <p>Use YAML merge keys to combine base configurations with service-specific overrides.</p> </li> </ul> <p>Hint: Use <code>&lt;&lt;: *anchor</code> to merge configurations and override specific values</p> Solution <p>Solution:</p> <p>docker-compose.yml <pre><code>version: '3.8'\n\nx-base-service: &amp;base-service\n  image: alpine:latest\n  environment:\n    APP_ENV: production\n    LOG_LEVEL: info\n    TZ: UTC\n  deploy:\n    resources:\n      limits:\n        memory: 128M\n        cpus: '0.25'\n    restart_policy:\n      condition: on-failure\n  logging:\n    driver: json-file\n    options:\n      max-size: \"10m\"\n      max-file: \"3\"\n\nx-web-config: &amp;web-config\n  &lt;&lt;: *base-service\n  ports:\n    - \"8080:80\"\n  healthcheck:\n    test: [\"CMD\", \"wget\", \"--no-verbose\", \"--tries=1\", \"--spider\", \"http://localhost/\"]\n    interval: 30s\n    timeout: 10s\n    retries: 3\n\nx-api-config: &amp;api-config\n  &lt;&lt;: *base-service\n  ports:\n    - \"3000:3000\"\n  environment:\n    &lt;&lt;: *base-service.environment\n    SERVICE_TYPE: api\n    DATABASE_URL: postgresql://db:5432/myapp\n  depends_on:\n    - db\n\nservices:\n  web:\n    &lt;&lt;: *web-config\n    command: sh -c 'echo \"&lt;h1&gt;Web Service&lt;/h1&gt;\" &gt; /tmp/index.html &amp;&amp; busybox httpd -f -p 80 -h /tmp'\n\n  api:\n    &lt;&lt;: *api-config\n    command: sh -c 'while true; do echo \"API running on port 3000\"; sleep 10; done'\n\n  db:\n    image: postgres:13-alpine\n    environment:\n      POSTGRES_DB: myapp\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    volumes:\n      - db_data:/var/lib/postgresql/data\n    deploy: *base-service.deploy\n\nvolumes:\n  db_data:\n</code></pre></p> <p>Commands: <pre><code># Start services\ndocker-compose up -d\n\n# Check merged configurations\ndocker-compose config\n\n# Test services\ncurl http://localhost:8080\ndocker-compose logs api\n\n# Inspect service configurations\ndocker inspect $(docker-compose ps -q web) | jq '.[0].Config.Env'\n\n# Stop services\ndocker-compose down\n</code></pre></p> <p>Explanation:</p> <ul> <li>&lt;&lt;: *anchor: Merges the referenced configuration into the current block</li> <li>Override behavior: Later values override earlier ones with the same key</li> <li>Nested merges: Can merge multiple levels of configuration</li> <li>Base + specific: Common pattern of base config plus service-specific additions</li> <li>Configuration inheritance: Services inherit from base configurations</li> </ul>"},{"location":"Tasks/DockerCompose/#scenario_19","title":"Scenario:","text":"<ul> <li>Your services have a common base configuration but need individual customizations for ports, environment variables, or resource limits.</li> <li>The merge key (&lt;&lt;) allows you to combine multiple configurations, with later values overriding earlier ones.</li> </ul>"},{"location":"Tasks/DockerCompose/#resources_9","title":"Resources:","text":"<ul> <li><code>docker-compose.yml</code> \u27a4 Compose file demonstrating merge operations     <pre><code>version: '3.8'\n\nx-base-service: &amp;base-service\n  image: alpine:latest\n  environment:\n    APP_ENV: production\n  deploy:\n    resources:\n      limits:\n        memory: 128M\n        cpus: '0.25'\n\nservices:\n  web:\n    &lt;&lt;: *base-service\n    ports:\n      - \"8080:80\"\n    command: nginx -g 'daemon off;'\n\n  worker:\n    &lt;&lt;: *base-service\n    environment:\n      &lt;&lt;: *base-service.environment\n      WORKER_TYPE: background\n    command: sh -c 'while true; do echo \"Working...\"; sleep 30; done'\n</code></pre></li> </ul>"},{"location":"Tasks/DockerCompose/#21-using-includes-for-modular-compose-files","title":"21. Using Includes for Modular Compose Files","text":"<ul> <li> <p>Split large compose files into smaller, manageable modules using Docker Compose includes.</p> </li> </ul> <p>Hint: Use <code>include</code> directive to reference external compose files</p> Solution <p>Solution:</p> <p>docker-compose.yml (main) <pre><code>include:\n  - services.yml\n  - networks.yml\n  - volumes.yml\n\nversion: '3.8'\n\nservices:\n  proxy:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./proxy/nginx.conf:/etc/nginx/nginx.conf:ro\n    networks:\n      - frontend\n    depends_on:\n      - web\n      - api\n</code></pre></p> <p>services.yml <pre><code>services:\n  web:\n    image: nginx:alpine\n    volumes:\n      - web_data:/usr/share/nginx/html\n    networks:\n      - frontend\n    deploy:\n      replicas: 2\n\n  api:\n    image: node:18-alpine\n    environment:\n      DATABASE_URL: postgresql://db:5432/myapp\n    networks:\n      - frontend\n      - backend\n    depends_on:\n      - db\n\n  db:\n    image: postgres:13-alpine\n    environment:\n      POSTGRES_DB: myapp\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    volumes:\n      - db_data:/var/lib/postgresql/data\n    networks:\n      - backend\n</code></pre></p> <p>networks.yml <pre><code>networks:\n  frontend:\n    driver: bridge\n    ipam:\n      config:\n        - subnet: 172.20.0.0/16\n  backend:\n    driver: bridge\n    internal: true\n    ipam:\n      config:\n        - subnet: 172.21.0.0/16\n</code></pre></p> <p>volumes.yml <pre><code>volumes:\n  web_data:\n  db_data:\n</code></pre></p> <p>proxy/nginx.conf <pre><code>events {\n    worker_connections 1024;\n}\n\nhttp {\n    upstream web_backend {\n        server web:80;\n    }\n\n    upstream api_backend {\n        server api:3000;\n    }\n\n    server {\n        listen 80;\n\n        location / {\n            proxy_pass http://web_backend;\n        }\n\n        location /api {\n            proxy_pass http://api_backend;\n        }\n    }\n}\n</code></pre></p> <p>Commands: <pre><code># Start all services from modular files\ndocker-compose up -d\n\n# Check included configurations\ndocker-compose config\n\n# List all services from includes\ndocker-compose ps\n\n# View logs from specific service\ndocker-compose logs web\n\n# Stop all services\ndocker-compose down\n</code></pre></p> <p>Explanation:</p> <ul> <li>include: References external compose files to include their configurations</li> <li>Modular organization: Split large files into logical components</li> <li>Reusability: Include files can be shared across projects</li> <li>Maintainability: Easier to manage and version control smaller files</li> <li>Override capability: Main file can override included configurations</li> </ul>"},{"location":"Tasks/DockerCompose/#scenario_20","title":"Scenario:","text":"<ul> <li>Your application stack is complex with many services, networks, and volumes, making the compose file difficult to maintain.</li> <li>Docker Compose includes allow you to split your configuration across multiple files for better organization and reusability.</li> </ul>"},{"location":"Tasks/DockerCompose/#resources_10","title":"Resources:","text":"<ul> <li><code>docker-compose.yml</code> \u27a4 Main compose file with includes     <pre><code>include:\n  - services.yml\n  - networks.yml\n  - volumes.yml\n\nversion: '3.8'\n\nservices:\n  proxy:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n</code></pre></li> <li><code>services.yml</code> \u27a4 Service definitions     <pre><code>services:\n  web:\n    image: nginx:alpine\n    networks:\n      - frontend\n  api:\n    image: node:18-alpine\n    networks:\n      - frontend\n      - backend\n</code></pre></li> <li><code>networks.yml</code> \u27a4 Network definitions     <pre><code>networks:\n  frontend:\n    driver: bridge\n  backend:\n    driver: bridge\n    internal: true\n</code></pre></li> </ul>"},{"location":"Tasks/DockerCompose/#22-environment-specific-fragments","title":"22. Environment-Specific Fragments","text":"<ul> <li> <p>Create environment-specific configurations using fragments and conditional includes.</p> </li> </ul> <p>Hint: Use environment variables in include paths and fragments for environment-specific configurations</p> Solution <p>Solution:</p> <p>docker-compose.yml (main) <pre><code>include:\n  - common.yml\n  - path: environments/${ENVIRONMENT:-development}.yml\n\nversion: '3.8'\n\nx-env-overrides: &amp;env-overrides\n  environment:\n    ENVIRONMENT: ${ENVIRONMENT:-development}\n  logging:\n    driver: json-file\n    options:\n      max-size: ${LOG_MAX_SIZE:-10m}\n      max-file: ${LOG_MAX_FILE:-3}\n</code></pre></p> <p>common.yml <pre><code>x-common-service: &amp;common-service\n  image: alpine:latest\n  &lt;&lt;: *env-overrides\n  deploy:\n    resources:\n      limits:\n        memory: ${MEMORY_LIMIT:-128M}\n        cpus: ${CPU_LIMIT:-0.25}\n  healthcheck:\n    test: [\"CMD\", \"echo\", \"OK\"]\n    interval: 30s\n    timeout: 10s\n    retries: 3\n\nservices:\n  web:\n    &lt;&lt;: *common-service\n    command: sh -c 'echo \"&lt;h1&gt;Web Service - ${ENVIRONMENT}&lt;/h1&gt;\" &gt; /tmp/index.html &amp;&amp; busybox httpd -f -p 80 -h /tmp'\n\n  api:\n    &lt;&lt;: *common-service\n    environment:\n      &lt;&lt;: *common-service.environment\n      SERVICE_NAME: api\n    command: sh -c 'while true; do echo \"API in ${ENVIRONMENT}\"; sleep 10; done'\n\n  db:\n    image: postgres:13-alpine\n    environment:\n      POSTGRES_DB: myapp\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    volumes:\n      - db_data:/var/lib/postgresql/data\n    &lt;&lt;: *env-overrides\n\nvolumes:\n  db_data:\n</code></pre></p> <p>environments/development.yml <pre><code>services:\n  web:\n    ports:\n      - \"8080:80\"\n    environment:\n      DEBUG: true\n    volumes:\n      - ./dev-logs:/app/logs\n\n  api:\n    ports:\n      - \"3000:3000\"\n\n  db:\n    ports:\n      - \"5432:5432\"\n</code></pre></p> <p>environments/production.yml <pre><code>services:\n  web:\n    deploy:\n      replicas: 3\n      resources:\n        limits:\n          memory: 512M\n          cpus: '1.0'\n    environment:\n      NODE_ENV: production\n\n  api:\n    deploy:\n      replicas: 2\n      resources:\n        limits:\n          memory: 256M\n          cpus: '0.5'\n\n  db:\n    deploy:\n      resources:\n        limits:\n          memory: 1G\n          cpus: '2.0'\n</code></pre></p> <p>environments/staging.yml <pre><code>services:\n  web:\n    deploy:\n      replicas: 2\n      resources:\n        limits:\n          memory: 256M\n          cpus: '0.5'\n    environment:\n      NODE_ENV: staging\n\n  api:\n    deploy:\n      replicas: 1\n      resources:\n        limits:\n          memory: 128M\n          cpus: '0.25'\n\n  db:\n    deploy:\n      resources:\n        limits:\n          memory: 512M\n          cpus: '1.0'\n</code></pre></p> <p>Commands: <pre><code># Development environment\nENVIRONMENT=development docker-compose up -d\n\n# Production environment\nENVIRONMENT=production docker-compose up -d\n\n# Staging environment\nENVIRONMENT=staging docker-compose up -d\n\n# Check current configuration\ndocker-compose config\n\n# View environment-specific settings\ndocker-compose exec web env | grep ENVIRONMENT\n\n# Stop services\ndocker-compose down\n</code></pre></p> <p>Explanation:</p> <ul> <li>Environment variables in includes: Dynamic file inclusion based on environment</li> <li>Fragment overrides: Environment files override common configurations</li> <li>Conditional configuration: Different settings per environment</li> <li>Scalability: Easy to add new environments</li> <li>Maintainability: Separate concerns for common vs environment-specific config</li> </ul>"},{"location":"Tasks/DockerCompose/#scenario_21","title":"Scenario:","text":"<ul> <li>You need different service configurations for development, staging, and production environments with varying resource allocations, logging levels, and monitoring.</li> <li>Fragments combined with includes allow you to maintain environment-specific configurations while sharing common elements.</li> </ul>"},{"location":"Tasks/DockerCompose/#resources_11","title":"Resources:","text":"<ul> <li><code>docker-compose.yml</code> \u27a4 Main file with environment includes     <pre><code>include:\n  - common.yml\n  - path: environments/${ENVIRONMENT}.yml\n\nversion: '3.8'\n</code></pre></li> <li><code>environments/development.yml</code> \u27a4 Development-specific config     <pre><code>services:\n  web:\n    environment:\n      DEBUG: true\n    ports:\n      - \"8080:80\"\n</code></pre></li> <li><code>environments/production.yml</code> \u27a4 Production-specific config     <pre><code>services:\n  web:\n    deploy:\n      replicas: 3\n      resources:\n        limits:\n          memory: 512M\n    environment:\n      NODE_ENV: production\n</code></pre></li> </ul>"},{"location":"Tasks/DockerCompose/#23-complex-fragment-hierarchies","title":"23. Complex Fragment Hierarchies","text":"<ul> <li> <p>Build complex configuration hierarchies using nested anchors and multiple inheritance levels.</p> </li> </ul> <p>Hint: Create multi-level inheritance chains with anchors and merges</p> Solution <p>Solution:</p> <p>docker-compose.yml <pre><code>version: '3.8'\n\nx-global: &amp;global\n  environment:\n    APP_ENV: production\n    LOG_LEVEL: info\n    TZ: UTC\n  logging:\n    driver: json-file\n    options:\n      max-size: \"10m\"\n      max-file: \"3\"\n\nx-infrastructure: &amp;infrastructure\n  &lt;&lt;: *global\n  deploy:\n    restart_policy:\n      condition: on-failure\n      delay: 5s\n    resources:\n      limits:\n        memory: 128M\n        cpus: '0.25'\n\nx-web-infrastructure: &amp;web-infrastructure\n  &lt;&lt;: *infrastructure\n  healthcheck:\n    test: [\"CMD\", \"wget\", \"--no-verbose\", \"--tries=1\", \"--spider\", \"http://localhost/\"]\n    interval: 30s\n    timeout: 10s\n    retries: 3\n\nx-api-infrastructure: &amp;api-infrastructure\n  &lt;&lt;: *infrastructure\n  environment:\n    &lt;&lt;: *infrastructure.environment\n    SERVICE_TYPE: api\n  healthcheck:\n    test: [\"CMD\", \"echo\", \"API health check\"]\n    interval: 30s\n    timeout: 10s\n    retries: 3\n\nx-database-infrastructure: &amp;database-infrastructure\n  &lt;&lt;: *infrastructure\n  environment:\n    &lt;&lt;: *infrastructure.environment\n    SERVICE_TYPE: database\n  healthcheck:\n    test: [\"CMD-SHELL\", \"pg_isready -U user -d myapp\"]\n    interval: 10s\n    timeout: 5s\n    retries: 5\n\nx-web-primary: &amp;web-primary\n  &lt;&lt;: *web-infrastructure\n  environment:\n    &lt;&lt;: *web-infrastructure.environment\n    INSTANCE: primary\n    ROLE: frontend\n  ports:\n    - \"8080:80\"\n  deploy:\n    &lt;&lt;: *web-infrastructure.deploy\n    labels:\n      - \"service.role=primary\"\n\nx-web-secondary: &amp;web-secondary\n  &lt;&lt;: *web-infrastructure\n  environment:\n    &lt;&lt;: *web-infrastructure.environment\n    INSTANCE: secondary\n    ROLE: frontend\n  ports:\n    - \"8081:80\"\n  deploy:\n    &lt;&lt;: *web-infrastructure.deploy\n    labels:\n      - \"service.role=secondary\"\n\nservices:\n  web-primary:\n    &lt;&lt;: *web-primary\n    image: nginx:alpine\n    volumes:\n      - ./web/primary:/usr/share/nginx/html:ro\n\n  web-secondary:\n    &lt;&lt;: *web-secondary\n    image: nginx:alpine\n    volumes:\n      - ./web/secondary:/usr/share/nginx/html:ro\n\n  api-main:\n    &lt;&lt;: *api-infrastructure\n    image: node:18-alpine\n    ports:\n      - \"3000:3000\"\n    environment:\n      &lt;&lt;: *api-infrastructure.environment\n      DATABASE_URL: postgresql://db:5432/myapp\n    depends_on:\n      - db\n\n  api-worker:\n    &lt;&lt;: *api-infrastructure\n    image: node:18-alpine\n    environment:\n      &lt;&lt;: *api-infrastructure.environment\n      WORKER_TYPE: background\n      DATABASE_URL: postgresql://db:5432/myapp\n    deploy:\n      &lt;&lt;: *api-infrastructure.deploy\n      replicas: 2\n    depends_on:\n      - db\n\n  db:\n    &lt;&lt;: *database-infrastructure\n    image: postgres:13-alpine\n    environment:\n      &lt;&lt;: *database-infrastructure.environment\n      POSTGRES_DB: myapp\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    volumes:\n      - db_data:/var/lib/postgresql/data\n    ports:\n      - \"5432:5432\"\n    deploy:\n      &lt;&lt;: *database-infrastructure.deploy\n      resources:\n        limits:\n          memory: 512M\n          cpus: '1.0'\n\nvolumes:\n  db_data:\n</code></pre></p> <p>Commands: <pre><code># Start complex hierarchy services\ndocker-compose up -d\n\n# Check configuration hierarchy\ndocker-compose config | head -50\n\n# View service inheritance\ndocker-compose exec web-primary env | grep INSTANCE\ndocker-compose exec api-worker env | grep WORKER_TYPE\n\n# Check labels and metadata\ndocker inspect $(docker-compose ps -q web-primary) | jq '.[0].Config.Labels'\n\n# Stop services\ndocker-compose down\n</code></pre></p> <p>Explanation:</p> <ul> <li>Multi-level inheritance: Global \u2192 Infrastructure \u2192 Service-type \u2192 Instance-specific</li> <li>Complex merging: Multiple levels of configuration inheritance</li> <li>Override chains: Each level can override previous configurations</li> <li>Modular design: Easy to add new service types or instances</li> <li>Configuration clarity: Clear separation of concerns across inheritance levels</li> </ul>"},{"location":"Tasks/DockerCompose/#scenario_22","title":"Scenario:","text":"<ul> <li>Your application has services with multiple inheritance levels - base configurations, service-type configurations, and instance-specific overrides.</li> <li>Complex fragment hierarchies allow you to build sophisticated configuration inheritance chains.</li> </ul>"},{"location":"Tasks/DockerCompose/#resources_12","title":"Resources:","text":"<ul> <li><code>docker-compose.yml</code> \u27a4 Complex fragment hierarchy     <pre><code>version: '3.8'\n\nx-base: &amp;base\n  environment:\n    APP_ENV: production\n\nx-service-base: &amp;service-base\n  &lt;&lt;: *base\n  deploy:\n    resources:\n      limits:\n        memory: 128M\n\nx-web-service: &amp;web-service\n  &lt;&lt;: *service-base\n  ports:\n    - \"8080:80\"\n\nservices:\n  web-primary:\n    &lt;&lt;: *web-service\n    environment:\n      &lt;&lt;: *web-service.environment\n      INSTANCE: primary\n\n  web-secondary:\n    &lt;&lt;: *web-service\n    environment:\n      &lt;&lt;: *web-service.environment\n      INSTANCE: secondary\n    deploy:\n      &lt;&lt;: *web-service.deploy\n      replicas: 2\n</code></pre></li> </ul>"},{"location":"Tasks/DockerCompose/#24-fragment-based-service-templates","title":"24. Fragment-Based Service Templates","text":"<ul> <li> <p>Create reusable service templates using fragments for common service patterns.</p> </li> </ul> <p>Hint: Define service templates as anchors and instantiate them with specific overrides</p> Solution <p>Solution:</p> <p>docker-compose.yml <pre><code>version: '3.8'\n\nx-service-base: &amp;service-base\n  environment:\n    APP_ENV: production\n    LOG_LEVEL: info\n    TZ: UTC\n  logging:\n    driver: json-file\n    options:\n      max-size: \"10m\"\n      max-file: \"3\"\n  deploy:\n    restart_policy:\n      condition: on-failure\n      delay: 5s\n\nx-web-template: &amp;web-template\n  &lt;&lt;: *service-base\n  image: nginx:alpine\n  environment:\n    &lt;&lt;: *service-base.environment\n    SERVICE_TYPE: web\n  healthcheck:\n    test: [\"CMD\", \"wget\", \"--no-verbose\", \"--tries=1\", \"--spider\", \"http://localhost/\"]\n    interval: 30s\n    timeout: 10s\n    retries: 3\n  deploy:\n    &lt;&lt;: *service-base.deploy\n    resources:\n      limits:\n        memory: 256M\n        cpus: '0.5'\n\nx-api-template: &amp;api-template\n  &lt;&lt;: *service-base\n  image: node:18-alpine\n  environment:\n    &lt;&lt;: *service-base.environment\n    SERVICE_TYPE: api\n  healthcheck:\n    test: [\"CMD\", \"echo\", \"API OK\"]\n    interval: 30s\n    timeout: 10s\n    retries: 3\n  deploy:\n    &lt;&lt;: *service-base.deploy\n    resources:\n      limits:\n        memory: 512M\n        cpus: '1.0'\n\nx-worker-template: &amp;worker-template\n  &lt;&lt;: *service-base\n  image: python:3.9-alpine\n  environment:\n    &lt;&lt;: *service-base.environment\n    SERVICE_TYPE: worker\n  healthcheck:\n    test: [\"CMD\", \"echo\", \"Worker OK\"]\n    interval: 60s\n    timeout: 10s\n    retries: 3\n  deploy:\n    &lt;&lt;: *service-base.deploy\n    resources:\n      limits:\n        memory: 128M\n        cpus: '0.25'\n\nx-database-template: &amp;database-template\n  &lt;&lt;: *service-base\n  image: postgres:13-alpine\n  environment:\n    &lt;&lt;: *service-base.environment\n    SERVICE_TYPE: database\n  healthcheck:\n    test: [\"CMD-SHELL\", \"pg_isready -U ${POSTGRES_USER:-user} -d ${POSTGRES_DB:-myapp}\"]\n    interval: 10s\n    timeout: 5s\n    retries: 5\n  deploy:\n    &lt;&lt;: *service-base.deploy\n    resources:\n      limits:\n        memory: 1G\n        cpus: '2.0'\n\nservices:\n  # Web services\n  web-frontend:\n    &lt;&lt;: *web-template\n    ports:\n      - \"8080:80\"\n    volumes:\n      - ./web/frontend:/usr/share/nginx/html:ro\n    environment:\n      &lt;&lt;: *web-template.environment\n      SECTION: frontend\n\n  web-admin:\n    &lt;&lt;: *web-template\n    ports:\n      - \"8081:80\"\n    volumes:\n      - ./web/admin:/usr/share/nginx/html:ro\n    environment:\n      &lt;&lt;: *web-template.environment\n      SECTION: admin\n\n  web-api-docs:\n    &lt;&lt;: *web-template\n    ports:\n      - \"8082:80\"\n    volumes:\n      - ./web/docs:/usr/share/nginx/html:ro\n    environment:\n      &lt;&lt;: *web-template.environment\n      SECTION: api-docs\n    deploy:\n      &lt;&lt;: *web-template.deploy\n      resources:\n        limits:\n          memory: 128M\n          cpus: '0.25'\n\n  # API services\n  api-users:\n    &lt;&lt;: *api-template\n    ports:\n      - \"3000:3000\"\n    environment:\n      &lt;&lt;: *api-template.environment\n      MODULE: users\n      DATABASE_URL: postgresql://db:5432/myapp\n    depends_on:\n      - db\n\n  api-products:\n    &lt;&lt;: *api-template\n    ports:\n      - \"3001:3000\"\n    environment:\n      &lt;&lt;: *api-template.environment\n      MODULE: products\n      DATABASE_URL: postgresql://db:5432/myapp\n    depends_on:\n      - db\n\n  # Worker services\n  worker-email:\n    &lt;&lt;: *worker-template\n    environment:\n      &lt;&lt;: *worker-template.environment\n      WORKER_TYPE: email\n      QUEUE_NAME: email_queue\n    deploy:\n      &lt;&lt;: *worker-template.deploy\n      replicas: 2\n\n  worker-reports:\n    &lt;&lt;: *worker-template\n    environment:\n      &lt;&lt;: *worker-template.environment\n      WORKER_TYPE: reports\n      SCHEDULE: \"0 */6 * * *\"\n\n  # Database\n  db:\n    &lt;&lt;: *database-template\n    environment:\n      &lt;&lt;: *database-template.environment\n      POSTGRES_DB: myapp\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    volumes:\n      - db_data:/var/lib/postgresql/data\n    ports:\n      - \"5432:5432\"\n\nvolumes:\n  db_data:\n</code></pre></p> <p>Commands: <pre><code># Start templated services\ndocker-compose up -d\n\n# Check template instantiation\ndocker-compose ps\n\n# View service configurations\ndocker-compose config | grep -A 10 \"web-frontend:\"\n\n# Test different service types\ncurl http://localhost:8080\ncurl http://localhost:8081\ndocker-compose logs api-users\n\n# Scale templated services\ndocker-compose up -d --scale worker-email=3\n\n# Stop services\ndocker-compose down\n</code></pre></p> <p>Explanation:</p> <ul> <li>Service templates: Reusable patterns for common service types</li> <li>Template instantiation: Create multiple services from the same template</li> <li>Customization: Override template defaults for specific instances</li> <li>Consistency: Ensure similar services follow the same patterns</li> <li>Maintainability: Changes to templates affect all instances</li> <li>Scalability: Easy to add new services following established patterns</li> </ul>"},{"location":"Tasks/DockerCompose/#scenario_23","title":"Scenario:","text":"<ul> <li>Your application uses similar service patterns repeatedly (like web services, worker services, or API services) with slight variations.</li> <li>Service templates using fragments allow you to define reusable patterns and instantiate them with specific configurations.</li> </ul>"},{"location":"Tasks/DockerCompose/#resources_13","title":"Resources:","text":"<ul> <li><code>docker-compose.yml</code> \u27a4 Service templates with fragments     <pre><code>version: '3.8'\n\nx-web-template: &amp;web-template\n  image: nginx:alpine\n  environment:\n    SERVICE_TYPE: web\n  deploy:\n    resources:\n      limits:\n        memory: 256M\n\nx-api-template: &amp;api-template\n  image: node:18-alpine\n  environment:\n    SERVICE_TYPE: api\n  deploy:\n    resources:\n      limits:\n        memory: 512M\n\nservices:\n  web-frontend:\n    &lt;&lt;: *web-template\n    ports:\n      - \"8080:80\"\n\n  web-admin:\n    &lt;&lt;: *web-template\n    ports:\n      - \"8081:80\"\n    environment:\n      &lt;&lt;: *web-template.environment\n      SECTION: admin\n</code></pre></li> </ul>"},{"location":"Tasks/DockerFile/","title":"Docker File","text":""},{"location":"Tasks/DockerFile/#dockerfile-tasks","title":"Dockerfile Tasks","text":"<ul> <li>Hands-on Dockerfile exercises covering essential image building concepts, optimization techniques, and advanced containerization patterns.</li> <li>Each task includes a clear scenario description, helpful hints, and detailed solutions with explanations.</li> <li>Practice these tasks to master Dockerfile best practices, multi-stage builds, and image optimization.</li> </ul>"},{"location":"Tasks/DockerFile/#table-of-contents","title":"Table of Contents","text":"<ul> <li>01. Basic Dockerfile Structure</li> <li>02. Build Arguments and Environment Variables</li> <li>03. Multi-Stage Build Basics</li> <li>04. Working Directory and File Operations</li> <li>05. User Management and Security</li> <li>06. Port Exposure and Networking</li> <li>07. Health Checks Implementation</li> <li>08. Labels and Metadata</li> <li>09. Build Context Optimization</li> <li>10. Multi-Stage Build with Go Application</li> <li>11. Python Application Containerization</li> <li>12. Image Layer Caching Optimization</li> <li>13. Build Secrets Management</li> <li>14. Advanced Multi-Stage Build Patterns</li> <li>15. Dockerfile Security Best Practices</li> <li>16. BuildKit Advanced Features</li> <li>17. Image Size Optimization</li> <li>18. Complex Application Stack</li> </ul>"},{"location":"Tasks/DockerFile/#01-basic-dockerfile-structure","title":"01. Basic Dockerfile Structure","text":"<ul> <li> <p>Create a simple Dockerfile that builds a basic web server using Nginx to serve static HTML content.</p> </li> </ul> <p>Hint: Use <code>FROM</code>, <code>COPY</code>, <code>EXPOSE</code>, and <code>CMD</code> instructions</p> Solution <p>Solution:</p> <p>Create the following files:</p> <p>index.html <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;My First Docker App&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Hello from Docker!&lt;/h1&gt;\n    &lt;p&gt;This page is served from a container.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Dockerfile <pre><code>FROM nginx:alpine\n\n# Copy custom HTML file to nginx default location\nCOPY index.html /usr/share/nginx/html/\n\n# Expose port 80\nEXPOSE 80\n\n# Use default nginx command\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n</code></pre></p> <p>Build and run: <pre><code># Build the image\ndocker build -t basic-nginx .\n\n# Run the container\ndocker run -d -p 8080:80 --name basic-web basic-nginx\n\n# Test\ncurl http://localhost:8080\n</code></pre></p> <p>Explanation:</p> <ul> <li>FROM: Specifies the base image to build upon</li> <li>COPY: Copies files from build context to the image</li> <li>EXPOSE: Documents which ports the container listens on</li> <li>CMD: Specifies the command to run when the container starts</li> <li>nginx:alpine: Lightweight base image for web serving</li> </ul>"},{"location":"Tasks/DockerFile/#scenario","title":"Scenario:","text":"<ul> <li>As a web developer, you need to quickly containerize a static website for local development and testing before deploying to production. </li> <li>Using a basic Dockerfile allows you to package your HTML, CSS, and JavaScript files into a portable container that can run consistently across different environments.</li> </ul>"},{"location":"Tasks/DockerFile/#resources","title":"Resources:","text":"<ul> <li>Create <code>index.html</code>:   <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;My First Docker App&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Hello from Docker!&lt;/h1&gt;\n    &lt;p&gt;This page is served from a container.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></li> <li>Create <code>Dockerfile</code> (see solution for content)</li> </ul>"},{"location":"Tasks/DockerFile/#02-build-arguments-and-environment-variables","title":"02. Build Arguments and Environment Variables","text":"<ul> <li> <p>Build a configurable Node.js application that accepts build-time arguments for version and port configuration.</p> </li> </ul> <p>Hint: Use <code>ARG</code> for build-time variables and <code>ENV</code> for runtime environment variables</p> Solution <p>Solution:</p> <p>app.js <pre><code>const http = require('http');\nconst port = process.env.PORT || 3000;\nconst version = process.env.VERSION || '1.0.0';\n\nconst server = http.createServer((req, res) =&gt; {\n  res.writeHead(200, {'Content-Type': 'text/plain'});\n  res.end(`App Version: ${version}, Running on port: ${port}\\n`);\n});\n\nserver.listen(port, '0.0.0.0', () =&gt; {\n  console.log(`Server running on port ${port}`);\n});\n</code></pre></p> <p>package.json <pre><code>{\n  \"name\": \"configurable-app\",\n  \"version\": \"1.0.0\",\n  \"main\": \"app.js\",\n  \"scripts\": {\n    \"start\": \"node app.js\"\n  }\n}\n</code></pre></p> <p>Dockerfile <pre><code>FROM node:18-alpine\n\n# Build-time argument\nARG APP_VERSION=1.0.0\n\n# Set environment variable from build arg\nENV VERSION=${APP_VERSION}\nENV PORT=3000\n\nWORKDIR /app\n\n# Copy package files\nCOPY package*.json ./\n\n# Install dependencies\nRUN npm ci --only=production\n\n# Copy application code\nCOPY app.js ./\n\n# Expose the port\nEXPOSE ${PORT}\n\n# Run the application\nCMD [\"npm\", \"start\"]\n</code></pre></p> <p>Build and test: <pre><code># Build with custom version\ndocker build --build-arg APP_VERSION=2.1.0 -t configurable-app .\n\n# Run the container\ndocker run -d -p 3000:3000 --name config-app configurable-app\n\n# Test\ncurl http://localhost:3000\n</code></pre></p> <p>Explanation:</p> <ul> <li>ARG: Defines build-time variables that can be passed with \u2013build-arg</li> <li>ENV: Sets environment variables that persist in the final image</li> <li>Build-time vs runtime: ARG is only available during build, ENV persists in containers</li> <li>Default values: Both ARG and ENV can have fallback values</li> <li>EXPOSE with variables: Can use environment variables for port exposure</li> </ul>"},{"location":"Tasks/DockerFile/#scenario_1","title":"Scenario:","text":"<ul> <li>You\u2019re deploying the same application across multiple environments (development, staging, production) with different configurations. </li> <li>Build arguments allow you to customize the image at build time, while environment variables enable runtime configuration flexibility.</li> </ul>"},{"location":"Tasks/DockerFile/#resources_1","title":"Resources:","text":"<ul> <li>Create <code>app.js</code>:   <pre><code>const http = require('http');\nconst port = process.env.PORT || 3000;\nconst version = process.env.VERSION || '1.0.0';\n\nconst server = http.createServer((req, res) =&gt; {\n  res.writeHead(200, {'Content-Type': 'text/plain'});\n  res.end(`App Version: ${version}, Running on port: ${port}\\n`);\n});\n\nserver.listen(port, '0.0.0.0', () =&gt; {\n  console.log(`Server running on port ${port}`);\n});\n</code></pre></li> <li>Create <code>package.json</code>:   <pre><code>{\n  \"name\": \"configurable-app\",\n  \"version\": \"1.0.0\",\n  \"main\": \"app.js\",\n  \"scripts\": {\n    \"start\": \"node app.js\"\n  }\n}\n</code></pre></li> <li>Create <code>Dockerfile</code> (see solution for content)</li> </ul>"},{"location":"Tasks/DockerFile/#03-multi-stage-build-basics","title":"03. Multi-Stage Build Basics","text":"<ul> <li> <p>Create a multi-stage Dockerfile that compiles a C application in one stage and copies the binary to a minimal runtime image.</p> </li> </ul> <p>Hint: Use <code>FROM ... AS</code> to define stages and <code>COPY --from=</code> to transfer artifacts</p> Solution <p>Solution:</p> <p>hello.c <pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n    printf(\"Hello from multi-stage build!\\n\");\n    return 0;\n}\n</code></pre></p> <p>Dockerfile <pre><code># Build stage\nFROM gcc:9-alpine AS builder\n\nWORKDIR /src\n\n# Copy source code\nCOPY hello.c .\n\n# Compile the application\nRUN gcc -o hello hello.c\n\n# Runtime stage\nFROM alpine:latest\n\n# Copy binary from build stage\nCOPY --from=builder /src/hello /usr/local/bin/hello\n\n# Run the application\nCMD [\"hello\"]\n</code></pre></p> <p>Build and run: <pre><code># Build the multi-stage image\ndocker build -t multi-stage-hello .\n\n# Run the container\ndocker run --rm multi-stage-hello\n</code></pre></p> <p>Expected output: <pre><code>Hello from multi-stage build!\n</code></pre></p> <p>Explanation:</p> <ul> <li>Multi-stage builds: Separate build dependencies from runtime image</li> <li>AS builder: Names the build stage for reference</li> <li>COPY \u2013from=builder: Copies files from the named build stage</li> <li>Smaller final images: Only runtime dependencies in final image</li> <li>Build optimization: No need for GCC in the final running container</li> </ul>"},{"location":"Tasks/DockerFile/#scenario_2","title":"Scenario:","text":"<ul> <li>You\u2019re building a compiled application that requires heavy build tools and dependencies, but you want to minimize the production image size and attack surface. </li> <li>Multi-stage builds allow you to use a full development environment for compilation, then copy only the resulting binary to a minimal runtime image.</li> </ul>"},{"location":"Tasks/DockerFile/#resources_2","title":"Resources:","text":"<ul> <li>Create <code>hello.c</code>:   <pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n    printf(\"Hello from multi-stage build!\\n\");\n    return 0;\n}\n</code></pre></li> <li>Use this basic <code>Dockerfile</code> snippet to get started:   <pre><code># Build stage\nFROM gcc:9-alpine AS builder\n\nWORKDIR /src\n\n# Copy source code\nCOPY hello.c .\n\n# Compile the application\nRUN gcc -o hello hello.c\n</code></pre></li> </ul>"},{"location":"Tasks/DockerFile/#04-working-directory-and-file-operations","title":"04. Working Directory and File Operations","text":"<ul> <li> <p>Create a Dockerfile that demonstrates proper working directory management and file operations for a Python application.</p> </li> </ul> <p>Hint: Use <code>WORKDIR</code>, <code>COPY</code>, <code>ADD</code>, and proper file permissions</p> Solution <p>Solution:</p> <p>app.py <pre><code>#!/usr/bin/env python3\nprint(\"Hello from Python application!\")\nprint(\"Current working directory:\", __file__)\n</code></pre></p> <p>requirements.txt <pre><code># No dependencies for this simple example\n</code></pre></p> <p>Dockerfile <pre><code>FROM python:3.9-slim\n\n# Set working directory\nWORKDIR /app\n\n# Copy requirements first (for better caching)\nCOPY requirements.txt .\n\n# Install dependencies (if any)\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY app.py .\n\n# Make script executable\nRUN chmod +x app.py\n\n# Set working directory again (can be set multiple times)\nWORKDIR /app\n\n# Run the application\nCMD [\"python\", \"app.py\"]\n</code></pre></p> <p>Build and run: <pre><code>docker build -t workdir-demo .\ndocker run --rm workdir-demo\n</code></pre></p> <p>Explanation:</p> <ul> <li>WORKDIR: Sets the working directory for subsequent instructions</li> <li>Layer optimization: Copy requirements first for better Docker layer caching</li> <li>File permissions: Use RUN chmod to set executable permissions</li> <li>Working directory persistence: Affects COPY, RUN, and CMD instructions</li> <li>Absolute vs relative paths: WORKDIR helps avoid full path specifications</li> </ul>"},{"location":"Tasks/DockerFile/#scenario_3","title":"Scenario:","text":"<ul> <li>You\u2019re containerizing a Python application with multiple source files, configuration files, and dependencies that need to be organized properly within the container. </li> <li>Proper working directory management ensures that your application runs from the correct location and can access its files reliably.</li> </ul>"},{"location":"Tasks/DockerFile/#resources_3","title":"Resources:","text":"<ul> <li>Create <code>app.py</code>:   <pre><code>#!/usr/bin/env python3\nprint(\"Hello from Python application!\")\nprint(\"Current working directory:\", __file__)\n</code></pre></li> <li>Create <code>requirements.txt</code>:   <pre><code># No dependencies for this simple example\n</code></pre></li> <li>Create <code>Dockerfile</code> (see solution for content)</li> </ul>"},{"location":"Tasks/DockerFile/#05-user-management-and-security","title":"05. User Management and Security","text":"<ul> <li> <p>Create a Dockerfile that runs as a non-root user for security, demonstrating proper user creation and permission management.</p> </li> </ul> <p>Hint: Use <code>RUN</code> to create users, <code>USER</code> to switch, and proper file ownership</p> Solution <p>Solution:</p> <p>app.py <pre><code>#!/usr/bin/env python3\nimport os\nprint(f\"Running as user: {os.getuid()}\")\nprint(f\"Username: {os.getenv('USER', 'unknown')}\")\nprint(\"Application is running securely!\")\n</code></pre></p> <p>Dockerfile <pre><code>FROM python:3.9-slim\n\n# Create a non-root user\nRUN groupadd -r appuser &amp;&amp; useradd -r -g appuser appuser\n\n# Set working directory\nWORKDIR /app\n\n# Copy application files\nCOPY app.py .\n\n# Change ownership of the app directory\nRUN chown -R appuser:appuser /app\n\n# Switch to non-root user\nUSER appuser\n\n# Run the application\nCMD [\"python\", \"app.py\"]\n</code></pre></p> <p>Build and run: <pre><code>docker build -t secure-app .\ndocker run --rm secure-app\n</code></pre></p> <p>Expected output: <pre><code>Running as user: 1000\nUsername: appuser\nApplication is running securely!\n</code></pre></p> <p>Explanation:</p> <ul> <li>Non-root security: Running as non-root user reduces security risks</li> <li>groupadd/useradd: Creates system users and groups</li> <li>chown: Changes file ownership to the application user</li> <li>USER instruction: Switches the user context for subsequent commands</li> <li>Principle of least privilege: Application runs with minimal required permissions</li> </ul>"},{"location":"Tasks/DockerFile/#scenario_4","title":"Scenario:","text":"<ul> <li>Security auditors require that your production containers don\u2019t run as root user to minimize potential security vulnerabilities. </li> <li>Implementing non-root user execution ensures that even if an attacker compromises your application, they have limited system access.</li> </ul>"},{"location":"Tasks/DockerFile/#resources_4","title":"Resources:","text":"<ul> <li>Create <code>app.py</code>:   <pre><code>#!/usr/bin/env python3\nimport os\nprint(f\"Running as user: {os.getuid()}\")\nprint(f\"Username: {os.getenv('USER', 'unknown')}\")\nprint(\"Application is running securely!\")\n</code></pre></li> <li>Create <code>Dockerfile</code> (see solution for content)</li> </ul>"},{"location":"Tasks/DockerFile/#06-port-exposure-and-networking","title":"06. Port Exposure and Networking","text":"<ul> <li> <p>Create a Dockerfile for a web application that properly exposes ports and demonstrates networking concepts.</p> </li> </ul> <p>Hint: Use <code>EXPOSE</code> for documentation and port mapping, understand the difference between exposing and publishing ports</p> Solution <p>Solution:</p> <p>server.js <pre><code>const http = require('http');\n\nconst server = http.createServer((req, res) =&gt; {\n  res.writeHead(200, {'Content-Type': 'text/plain'});\n  res.end(`Hello from container!\\nRequest from: ${req.connection.remoteAddress}\\n`);\n});\n\nconst port = process.env.PORT || 8080;\nserver.listen(port, '0.0.0.0', () =&gt; {\n  console.log(`Server listening on port ${port}`);\n});\n</code></pre></p> <p>Dockerfile <pre><code>FROM node:18-alpine\n\n# Set working directory\nWORKDIR /app\n\n# Copy package files\nCOPY package*.json ./\n\n# Install dependencies\nRUN npm ci --only=production\n\n# Copy application\nCOPY server.js .\n\n# Expose port (documentation)\nEXPOSE 8080\n\n# Environment variable for port\nENV PORT=8080\n\n# Run the server\nCMD [\"node\", \"server.js\"]\n</code></pre></p> <p>Build and test: <pre><code>docker build -t networking-demo .\n\n# Run with port mapping\ndocker run -d -p 8080:8080 --name net-demo networking-demo\n\n# Test from host\ncurl http://localhost:8080\n\n# Test from another container\ndocker run --rm --network container:net-demo alpine wget -qO- http://localhost:8080\n</code></pre></p> <p>Explanation:</p> <ul> <li>EXPOSE: Documents which ports the container listens on (metadata only)</li> <li>Port mapping: <code>-p</code> flag maps host port to container port</li> <li>0.0.0.0 binding: Allows connections from outside the container</li> <li>Container networking: Containers can communicate via exposed ports</li> <li>Environment variables: Configure ports dynamically</li> </ul>"},{"location":"Tasks/DockerFile/#scenario_5","title":"Scenario:","text":"<ul> <li>You\u2019re deploying a web service that needs to accept HTTP requests from external clients while maintaining proper network isolation. </li> <li>Correct port exposure ensures your application is accessible to other services and clients while documenting the intended network interface.</li> </ul>"},{"location":"Tasks/DockerFile/#resources_5","title":"Resources:","text":"<ul> <li>Create <code>server.js</code>:   <pre><code>const http = require('http');\n\nconst server = http.createServer((req, res) =&gt; {\n  res.writeHead(200, {'Content-Type': 'text/plain'});\n  res.end(`Hello from container!\\nRequest from: ${req.connection.remoteAddress}\\n`);\n});\n\nconst port = process.env.PORT || 8080;\nserver.listen(port, '0.0.0.0', () =&gt; {\n  console.log(`Server listening on port ${port}`);\n});\n</code></pre></li> <li>Create <code>Dockerfile</code> (see solution for content)</li> </ul>"},{"location":"Tasks/DockerFile/#07-health-checks-implementation","title":"07. Health Checks Implementation","text":"<ul> <li> <p>Implement health checks in a Dockerfile to monitor container health and enable automatic restarts.</p> </li> </ul> <p>Hint: Use <code>HEALTHCHECK</code> instruction with appropriate intervals, timeouts, and retry logic</p> Solution <p>Solution:</p> <p>healthcheck.sh <pre><code>#!/bin/sh\n# Health check script\ncurl -f http://localhost:8080/health || exit 1\n</code></pre></p> <p>server.js <pre><code>const http = require('http');\n\nconst server = http.createServer((req, res) =&gt; {\n  if (req.url === '/health') {\n    res.writeHead(200, {'Content-Type': 'text/plain'});\n    res.end('OK');\n  } else {\n    res.writeHead(200, {'Content-Type': 'text/plain'});\n    res.end('Hello World!\\n');\n  }\n});\n\nserver.listen(8080, '0.0.0.0', () =&gt; {\n  console.log('Server running on port 8080');\n});\n</code></pre></p> <p>Dockerfile <pre><code>FROM node:18-alpine\n\n# Install curl for health checks\nRUN apk add --no-cache curl\n\nWORKDIR /app\n\nCOPY package*.json ./\nRUN npm ci --only=production\n\nCOPY server.js .\nCOPY healthcheck.sh .\n\n# Make health check script executable\nRUN chmod +x healthcheck.sh\n\n# Expose port\nEXPOSE 8080\n\n# Health check configuration\nHEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\n  CMD ./healthcheck.sh\n\nCMD [\"node\", \"server.js\"]\n</code></pre></p> <p>Build and test: <pre><code>docker build -t healthcheck-demo .\n\n# Run the container\ndocker run -d -p 8080:8080 --name health-demo healthcheck-demo\n\n# Check health status\ndocker ps\ndocker inspect health-demo | grep -A 10 \"Health\"\n</code></pre></p> <p>Explanation:</p> <ul> <li>HEALTHCHECK: Defines how Docker determines container health</li> <li>\u2013interval: How often to run the health check</li> <li>\u2013timeout: Maximum time for health check to complete</li> <li>\u2013start-period: Grace period before health checks begin</li> <li>\u2013retries: Number of consecutive failures before marking unhealthy</li> <li>Automatic restarts: Docker can restart unhealthy containers</li> </ul>"},{"location":"Tasks/DockerFile/#scenario_6","title":"Scenario:","text":"<ul> <li>Your production application needs to automatically recover from failures without manual intervention. </li> <li>Health checks allow the container orchestrator to detect when your application becomes unresponsive and automatically restart the container to maintain service availability.</li> </ul>"},{"location":"Tasks/DockerFile/#resources_6","title":"Resources:","text":"<ul> <li>Create <code>healthcheck.sh</code>:   <pre><code>#!/bin/sh\n# Health check script\ncurl -f http://localhost:8080/health || exit 1\n</code></pre></li> <li>Create <code>server.js</code>:   <pre><code>const http = require('http');\n\nconst server = http.createServer((req, res) =&gt; {\n  if (req.url === '/health') {\n    res.writeHead(200, {'Content-Type': 'text/plain'});\n    res.end('OK');\n  } else {\n    res.writeHead(200, {'Content-Type': 'text/plain'});\n    res.end('Hello World!\\n');\n  }\n});\n\nserver.listen(8080, '0.0.0.0', () =&gt; {\n  console.log('Server running on port 8080');\n});\n</code></pre></li> <li>Create <code>Dockerfile</code> (see solution for content)</li> </ul>"},{"location":"Tasks/DockerFile/#08-labels-and-metadata","title":"08. Labels and Metadata","text":"<ul> <li> <p>Add comprehensive labels to a Dockerfile to provide metadata about the image, maintainer, and build information.</p> </li> </ul> <p>Hint: Use <code>LABEL</code> instruction to add key-value metadata that can be inspected with <code>docker inspect</code></p> Solution <p>Solution:</p> <p>Dockerfile <pre><code>FROM nginx:alpine\n\n# Image metadata labels\nLABEL maintainer=\"DockerLabs Team &lt;team@dockerlabs.com&gt;\" \\\n      version=\"1.0.0\" \\\n      description=\"Nginx web server with custom configuration\" \\\n      build_date=\"2024-01-01\" \\\n      vcs_ref=\"abc123def\" \\\n      vcs_url=\"https://github.com/dockerlabs/web-server\" \\\n      vendor=\"DockerLabs\" \\\n      license=\"MIT\"\n\n# Copy custom configuration\nCOPY nginx.conf /etc/nginx/nginx.conf\nCOPY index.html /usr/share/nginx/html/\n\n# Expose port\nEXPOSE 80\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\n  CMD curl -f http://localhost/ || exit 1\n\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n</code></pre></p> <p>nginx.conf <pre><code>events {\n    worker_connections 1024;\n}\n\nhttp {\n    server {\n        listen 80;\n        server_name localhost;\n\n        location / {\n            root /usr/share/nginx/html;\n            index index.html;\n        }\n\n        location /health {\n            access_log off;\n            return 200 \"healthy\\n\";\n            add_header Content-Type text/plain;\n        }\n    }\n}\n</code></pre></p> <p>index.html <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;Labeled Image&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;&lt;h1&gt;This image has comprehensive labels!&lt;/h1&gt;&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Build and inspect: <pre><code>docker build -t labeled-nginx .\n\n# Inspect labels\ndocker inspect labeled-nginx | grep -A 20 \"Labels\"\n\n# Run the container\ndocker run -d -p 8080:80 labeled-nginx\n</code></pre></p> <p>Explanation:</p> <ul> <li>LABEL: Adds metadata to images as key-value pairs</li> <li>Maintainer info: Contact information for image support</li> <li>Version tracking: Build version and date information</li> <li>Source control: Git commit and repository information</li> <li>Licensing: Legal information about the image</li> <li>Inspection: Labels can be viewed with <code>docker inspect</code></li> </ul>"},{"location":"Tasks/DockerFile/#scenario_7","title":"Scenario:","text":"<ul> <li>Your organization needs to track image versions, maintainers, and build information for compliance and operational purposes. </li> <li>Labels provide structured metadata that can be inspected and used by automated tools for inventory management, security scanning, and deployment decisions.</li> </ul>"},{"location":"Tasks/DockerFile/#resources_7","title":"Resources:","text":"<ul> <li>Create <code>nginx.conf</code>:   <pre><code>events {\n    worker_connections 1024;\n}\n\nhttp {\n    server {\n        listen 80;\n        server_name localhost;\n\n        location / {\n            root /usr/share/nginx/html;\n            index index.html;\n        }\n\n        location /health {\n            access_log off;\n            return 200 \"healthy\\n\";\n            add_header Content-Type text/plain;\n        }\n    }\n}\n</code></pre></li> <li>Create <code>index.html</code>:   <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;Labeled Image&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;&lt;h1&gt;This image has comprehensive labels!&lt;/h1&gt;&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></li> <li>Create <code>Dockerfile</code> (see solution for content)</li> </ul>"},{"location":"Tasks/DockerFile/#09-build-context-optimization","title":"09. Build Context Optimization","text":"<ul> <li> <p>Optimize the build context by using .dockerignore to exclude unnecessary files and improve build performance.</p> </li> </ul> <p>Hint: Create a <code>.dockerignore</code> file to exclude files that aren\u2019t needed in the build context</p> Solution <p>Solution:</p> <p>Project structure: <pre><code>my-app/\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 .dockerignore\n\u251c\u2500\u2500 package.json\n\u251c\u2500\u2500 app.js\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 .git/\n\u251c\u2500\u2500 node_modules/\n\u251c\u2500\u2500 .env\n\u251c\u2500\u2500 logs/\n\u2514\u2500\u2500 temp/\n</code></pre></p> <p>.dockerignore <pre><code># Node modules (will be installed in container)\nnode_modules\nnpm-debug.log*\n\n# Git repository\n.git\n.gitignore\n\n# Environment files\n.env\n.env.local\n\n# Logs and temporary files\nlogs\n*.log\ntemp/\n*.tmp\n\n# IDE files\n.vscode\n.idea\n*.swp\n*.swo\n\n# OS files\n.DS_Store\nThumbs.db\n\n# Documentation (not needed for build)\nREADME.md\ndocs/\n\n# Test files (if not running tests in container)\ntest/\n*.test.js\n</code></pre></p> <p>app.js <pre><code>const http = require('http');\n\nconst server = http.createServer((req, res) =&gt; {\n  res.writeHead(200, {'Content-Type': 'text/plain'});\n  res.end('Optimized build context!\\n');\n});\n\nserver.listen(3000, '0.0.0.0', () =&gt; {\n  console.log('Server running on port 3000');\n});\n</code></pre></p> <p>Dockerfile <pre><code>FROM node:18-alpine\n\nWORKDIR /app\n\n# Copy package files first for better caching\nCOPY package*.json ./\n\n# Install dependencies\nRUN npm ci --only=production\n\n# Copy application code (excluding .dockerignore files)\nCOPY . .\n\nEXPOSE 3000\n\nCMD [\"node\", \"app.js\"]\n</code></pre></p> <p>Build comparison: <pre><code># Time the build with .dockerignore\ntime docker build -t optimized-app .\n\n# Compare build context size\ndocker build --no-cache --progress=plain -t optimized-app . 2&gt;&amp;1 | grep \"Sending build context\"\n</code></pre></p> <p>Explanation:</p> <ul> <li>.dockerignore: Excludes files from build context to improve performance</li> <li>Build context: All files in the directory are sent to the Docker daemon</li> <li>Performance: Smaller context means faster builds and less network transfer</li> <li>Security: Prevents sensitive files from being included in images</li> <li>Caching: Better layer caching when unnecessary files aren\u2019t included</li> </ul>"},{"location":"Tasks/DockerFile/#scenario_8","title":"Scenario:","text":"<ul> <li>Your application repository contains large files, dependencies, and temporary files that slow down Docker builds and increase build context size. </li> <li>Optimizing the build context with .dockerignore improves build performance, reduces network transfer, and prevents sensitive files from being included in images.</li> </ul>"},{"location":"Tasks/DockerFile/#resources_8","title":"Resources:","text":"<ul> <li>Create <code>.dockerignore</code>:   <pre><code># Node modules (will be installed in container)\nnode_modules\nnpm-debug.log*\n\n# Git repository\n.git\n.gitignore\n\n# Environment files\n.env\n.env.local\n\n# Logs and temporary files\nlogs\n*.log\ntemp/\n*.tmp\n\n# IDE files\n.vscode\n.idea\n*.swp\n*.swo\n\n# OS files\n.DS_Store\nThumbs.db\n\n# Documentation (not needed for build)\nREADME.md\ndocs/\n\n# Test files (if not running tests in container)\ntest/\n*.test.js\n</code></pre></li> <li>Create <code>app.js</code>:   <pre><code>const http = require('http');\n\nconst server = http.createServer((req, res) =&gt; {\n  res.writeHead(200, {'Content-Type': 'text/plain'});\n  res.end('Optimized build context!\\n');\n});\n\nserver.listen(3000, '0.0.0.0', () =&gt; {\n  console.log('Server running on port 3000');\n});\n</code></pre></li> <li>Create <code>Dockerfile</code> (see solution for content)</li> </ul>"},{"location":"Tasks/DockerFile/#10-multi-stage-build-with-go-application","title":"10. Multi-Stage Build with Go Application","text":"<ul> <li> <p>Create an optimized multi-stage Dockerfile for a Go web application that compiles in one stage and runs in a minimal distroless image.</p> </li> </ul> <p>Hint: Use multi-stage builds to separate compilation from runtime, and use distroless base images for security</p> Solution <p>Solution:</p> <p>main.go <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"log\"\n    \"net/http\"\n)\n\nfunc handler(w http.ResponseWriter, r *http.Request) {\n    fmt.Fprintf(w, \"Hello from Go multi-stage build!\\n\")\n}\n\nfunc main() {\n    http.HandleFunc(\"/\", handler)\n    log.Println(\"Server starting on :8080\")\n    log.Fatal(http.ListenAndServe(\":8080\", nil))\n}\n</code></pre></p> <p>go.mod <pre><code>module github.com/dockerlabs/go-app\n\ngo 1.21\n</code></pre></p> <p>Dockerfile <pre><code># Build stage\nFROM golang:1.21-alpine AS builder\n\nWORKDIR /app\n\n# Copy go mod files\nCOPY go.mod go.sum ./\n\n# Download dependencies\nRUN go mod download\n\n# Copy source code\nCOPY . .\n\n# Build the application\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o main .\n\n# Runtime stage\nFROM gcr.io/distroless/static-debian12\n\n# Copy binary from build stage\nCOPY --from=builder /app/main /\n\n# Expose port\nEXPOSE 8080\n\n# Run the binary\nCMD [\"/main\"]\n</code></pre></p> <p>Build and run: <pre><code>docker build -t go-multi-stage .\ndocker run -d -p 8080:8080 --name go-app go-multi-stage\ncurl http://localhost:8080\n</code></pre></p> <p>Explanation:</p> <ul> <li>Multi-stage optimization: Separate build and runtime environments</li> <li>Distroless images: Minimal images with no shell or package manager</li> <li>Static compilation: CGO_ENABLED=0 creates statically linked binaries</li> <li>Security: Smaller attack surface with minimal base images</li> <li>Go optimization: -a flag forces rebuild, -installsuffix cgo avoids caching issues</li> </ul>"},{"location":"Tasks/DockerFile/#scenario_9","title":"Scenario:","text":"<ul> <li>You\u2019re deploying a Go application to production where security and minimal image size are critical requirements. </li> <li>Using distroless base images with multi-stage builds eliminates unnecessary packages and shell access, significantly reducing the attack surface while maintaining functionality.</li> </ul>"},{"location":"Tasks/DockerFile/#resources_9","title":"Resources:","text":"<ul> <li>Create <code>main.go</code>:   <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"log\"\n    \"net/http\"\n)\n\nfunc handler(w http.ResponseWriter, r *http.Request) {\n    fmt.Fprintf(w, \"Hello from Go multi-stage build!\\n\")\n}\n\nfunc main() {\n    http.HandleFunc(\"/\", handler)\n    log.Println(\"Server starting on :8080\")\n    log.Fatal(http.ListenAndServe(\":8080\", nil))\n}\n</code></pre></li> <li>Create <code>go.mod</code>:   <pre><code>module github.com/dockerlabs/go-app\n\ngo 1.21\n</code></pre></li> <li>Create <code>Dockerfile</code> (see solution for content)</li> </ul>"},{"location":"Tasks/DockerFile/#11-python-application-containerization","title":"11. Python Application Containerization","text":"<ul> <li> <p>Containerize a Python Flask application with proper dependency management and optimization.</p> </li> </ul> <p>Hint: Use virtual environments, multi-stage builds, and proper Python packaging</p> Solution <p>Solution:</p> <p>app.py <pre><code>from flask import Flask\nimport os\n\napp = Flask(__name__)\n\n@app.route('/')\ndef hello():\n    return f'Hello from Python container! Version: {os.getenv(\"APP_VERSION\", \"1.0.0\")}\\n'\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=int(os.getenv('PORT', 5000)))\n</code></pre></p> <p>requirements.txt <pre><code>Flask==3.0.0\ngunicorn==21.2.0\n</code></pre></p> <p>Dockerfile <pre><code>FROM python:3.11-slim\n\n# Set environment variables\nENV PYTHONDONTWRITEBYTECODE=1 \\\n    PYTHONUNBUFFERED=1 \\\n    APP_VERSION=1.0.0 \\\n    PORT=5000\n\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    gcc \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Copy requirements and install Python dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY app.py .\n\n# Create non-root user\nRUN useradd --create-home --shell /bin/bash app \\\n    &amp;&amp; chown -R app:app /app\nUSER app\n\nEXPOSE 5000\n\nCMD [\"gunicorn\", \"--bind\", \"0.0.0.0:5000\", \"app:app\"]\n</code></pre></p> <p>Build and run: <pre><code>docker build -t python-flask .\ndocker run -d -p 5000:5000 python-flask\ncurl http://localhost:5000\n</code></pre></p> <p>Explanation:</p> <ul> <li>Python optimization: PYTHONDONTWRITEBYTECODE prevents .pyc files</li> <li>Dependency management: Separate requirements copying for better caching</li> <li>Gunicorn: Production WSGI server instead of development server</li> <li>System dependencies: Install build tools only when needed</li> <li>Non-root user: Security best practice for Python applications</li> </ul>"},{"location":"Tasks/DockerFile/#scenario_10","title":"Scenario:","text":"<ul> <li>You\u2019re deploying a Python web application that needs to run consistently across development, testing, and production environments. </li> <li>Proper containerization ensures that all dependencies are correctly managed and the application runs with the same configuration regardless of the host system.</li> </ul>"},{"location":"Tasks/DockerFile/#resources_10","title":"Resources:","text":"<ul> <li>Create <code>app.py</code>:   <pre><code>from flask import Flask\nimport os\n\napp = Flask(__name__)\n\n@app.route('/')\ndef hello():\n    return f'Hello from Python container! Version: {os.getenv(\"APP_VERSION\", \"1.0.0\")}\\n'\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=int(os.getenv('PORT', 5000)))\n</code></pre></li> <li>Create <code>requirements.txt</code>:   <pre><code>Flask==3.0.0\ngunicorn==21.2.0\n</code></pre></li> <li>Create <code>Dockerfile</code> (see solution for content)</li> </ul>"},{"location":"Tasks/DockerFile/#12-image-layer-caching-optimization","title":"12. Image Layer Caching Optimization","text":"<ul> <li> <p>Optimize Dockerfile layer caching by ordering instructions properly and combining RUN commands.</p> </li> </ul> <p>Hint: Order COPY commands from least to most frequently changing, combine RUN commands, and use multi-stage builds</p> Solution <p>Solution:</p> <p>Dockerfile (Optimized) <pre><code>FROM ubuntu:20.04\n\n# Update and install system packages in one layer\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    curl \\\n    wget \\\n    git \\\n    vim \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Copy dependency files first (rarely change)\nCOPY requirements.txt package.json ./\n\n# Install dependencies (changes less frequently)\nRUN pip install -r requirements.txt &amp;&amp; \\\n    npm install\n\n# Copy application code (changes most frequently)\nCOPY . .\n\n# Set permissions and create directories in one command\nRUN chmod +x scripts/* &amp;&amp; \\\n    mkdir -p /app/logs /app/data &amp;&amp; \\\n    chown -R www-data:www-data /app\n\nEXPOSE 8080\n\nCMD [\"./start.sh\"]\n</code></pre></p> <p>Dockerfile (Poor - for comparison) <pre><code>FROM ubuntu:20.04\n\n# Poor: Update in separate layer\nRUN apt-get update\n\n# Poor: Install packages one by one\nRUN apt-get install -y curl\nRUN apt-get install -y wget\nRUN apt-get install -y git\n\n# Poor: Copy application code before dependencies\nCOPY . .\n\n# Poor: Install dependencies after copying code\nRUN pip install -r requirements.txt\nRUN npm install\n\nEXPOSE 8080\nCMD [\"./start.sh\"]\n</code></pre></p> <p>Build comparison: <pre><code># Build optimized version\ndocker build -f Dockerfile.optimized -t optimized-image .\n\n# Make small change to app code\necho \"# comment\" &gt;&gt; app.py\n\n# Rebuild - notice how many layers are cached\ndocker build -f Dockerfile.optimized -t optimized-image .\n\n# Compare with poor version\ndocker build -f Dockerfile.poor -t poor-image .\necho \"# comment\" &gt;&gt; app.py\ndocker build -f Dockerfile.poor -t poor-image .\n</code></pre></p> <p>Explanation:</p> <ul> <li>Layer ordering: Least changing instructions first (system packages, dependencies)</li> <li>RUN command combining: Single RUN for related operations to reduce layers</li> <li>Dependency copying: Copy requirements before code for better caching</li> <li>Cache invalidation: Changing code doesn\u2019t invalidate dependency layers</li> <li>Cleanup: Remove package manager cache to reduce image size</li> </ul>"},{"location":"Tasks/DockerFile/#scenario_11","title":"Scenario:","text":"<ul> <li>Your development team frequently rebuilds Docker images during development, and slow build times are impacting productivity. </li> <li>Optimizing layer caching ensures that only changed parts of the application trigger rebuilds, significantly reducing build times and improving the development workflow.</li> </ul>"},{"location":"Tasks/DockerFile/#13-build-secrets-management","title":"13. Build Secrets Management","text":"<ul> <li> <p>Use BuildKit secrets to handle sensitive information during the build process without embedding them in the final image.</p> </li> </ul> <p>Hint: Use <code>--secret</code> flag with BuildKit and <code>RUN --mount=type=secret</code> to access secrets during build</p> Solution <p>Solution:</p> <p>Dockerfile <pre><code># syntax=docker/dockerfile:1\n\nFROM node:18-alpine AS builder\n\nWORKDIR /app\n\n# Copy package files\nCOPY package*.json ./\n\n# Install dependencies\nRUN npm ci --only=production\n\n# Copy source\nCOPY . .\n\n# Use secret during build (e.g., for API keys, tokens)\nRUN --mount=type=secret,id=npm_token \\\n    echo \"//registry.npmjs.org/:_authToken=$(cat /run/secrets/npm_token)\" &gt; ~/.npmrc &amp;&amp; \\\n    npm publish\n\nFROM node:18-alpine AS runtime\n\nWORKDIR /app\n\n# Copy from builder stage\nCOPY --from=builder /app/node_modules ./node_modules\nCOPY --from=builder /app/package*.json ./\nCOPY --from=builder /app/src ./src\n\nEXPOSE 3000\n\nCMD [\"npm\", \"start\"]\n</code></pre></p> <p>Build with secrets: <pre><code># Create a secret file (never commit this)\necho \"your-npm-token-here\" &gt; npm_token.txt\n\n# Build with secret\nDOCKER_BUILDKIT=1 docker build \\\n  --secret id=npm_token,src=npm_token.txt \\\n  -t secret-build .\n\n# Clean up\nrm npm_token.txt\n</code></pre></p> <p>Alternative approach with environment variables: <pre><code>FROM node:18-alpine\n\n# Use ARG for build-time secrets (less secure)\nARG NPM_TOKEN\n\nRUN echo \"//registry.npmjs.org/:_authToken=${NPM_TOKEN}\" &gt; ~/.npmrc\n\n# Rest of the build...\n</code></pre></p> <p>Explanation:</p> <ul> <li>BuildKit secrets: Secure way to pass sensitive data during build</li> <li>\u2013mount=type=secret: Mounts secret files into build container</li> <li>Runtime security: Secrets not embedded in final image layers</li> <li>Environment variables: Less secure alternative for build-time secrets</li> <li>Secret management: Proper handling of tokens, keys, and credentials</li> </ul>"},{"location":"Tasks/DockerFile/#scenario_12","title":"Scenario:","text":"<ul> <li>Your build process requires access to sensitive information like API keys, database credentials, or authentication tokens for private repositories. </li> <li>BuildKit secrets allow you to use these credentials during the build process without embedding them in the final image layers, maintaining security compliance.</li> </ul>"},{"location":"Tasks/DockerFile/#14-advanced-multi-stage-build-patterns","title":"14. Advanced Multi-Stage Build Patterns","text":"<ul> <li> <p>Implement advanced multi-stage build patterns including shared base stages and conditional builds.</p> </li> </ul> <p>Hint: Use shared base stages, target builds, and conditional logic with build arguments</p> Solution <p>Solution:</p> <p>Dockerfile <pre><code># syntax=docker/dockerfile:1\n\n# Shared base stage\nFROM node:18-alpine AS base\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci\n\n# Development stage\nFROM base AS development\nCOPY . .\nEXPOSE 3000\nCMD [\"npm\", \"run\", \"dev\"]\n\n# Build stage\nFROM base AS build\nCOPY . .\nRUN npm run build\n\n# Test stage\nFROM build AS test\nRUN npm run test\n\n# Production stage\nFROM nginx:alpine AS production\nARG BUILD_ENV=production\n\n# Copy built assets from build stage\nCOPY --from=build /app/dist /usr/share/nginx/html\n\n# Copy nginx config based on environment\nCOPY nginx.${BUILD_ENV}.conf /etc/nginx/nginx.conf\n\nEXPOSE 80\n\nHEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\n  CMD curl -f http://localhost/ || exit 1\n\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n</code></pre></p> <p>nginx.production.conf <pre><code>events { worker_connections 1024; }\nhttp {\n    server {\n        listen 80;\n        root /usr/share/nginx/html;\n        index index.html;\n        location / {\n            try_files $uri $uri/ /index.html;\n        }\n    }\n}\n</code></pre></p> <p>nginx.staging.conf <pre><code>events { worker_connections 1024; }\nhttp {\n    server {\n        listen 80;\n        root /usr/share/nginx/html;\n        index index.html;\n        add_header X-Environment staging;\n        location / {\n            try_files $uri $uri/ /index.html;\n        }\n    }\n}\n</code></pre></p> <p>Build different targets: <pre><code># Build for development\ndocker build --target development -t myapp:dev .\n\n# Build for production\ndocker build --target production -t myapp:prod .\n\n# Build for staging\ndocker build --build-arg BUILD_ENV=staging --target production -t myapp:staging .\n\n# Run tests\ndocker build --target test -t myapp:test .\n</code></pre></p> <p>Explanation:</p> <ul> <li>Shared base stages: Common setup shared across multiple targets</li> <li>Target builds: Build specific stages with \u2013target flag</li> <li>Conditional configuration: Build args to customize builds</li> <li>Development workflow: Separate dev, test, and production stages</li> <li>Multi-environment: Different configurations for staging/production</li> </ul>"},{"location":"Tasks/DockerFile/#scenario_13","title":"Scenario:","text":"<ul> <li>You\u2019re managing complex applications that need different configurations for development, testing, and production environments. </li> <li>Advanced multi-stage patterns allow you to create optimized images for each environment while sharing common build steps, improving both build efficiency and maintainability.</li> </ul>"},{"location":"Tasks/DockerFile/#resources_11","title":"Resources:","text":"<ul> <li>Create <code>nginx.production.conf</code>:   <pre><code>events { worker_connections 1024; }\nhttp {\n    server {\n        listen 80;\n        root /usr/share/nginx/html;\n        index index.html;\n        location / {\n            try_files $uri $uri/ /index.html;\n        }\n    }\n}\n</code></pre></li> <li>Create <code>nginx.staging.conf</code>:   <pre><code>events { worker_connections 1024; }\nhttp {\n    server {\n        listen 80;\n        root /usr/share/nginx/html;\n        index index.html;\n        add_header X-Environment staging;\n        location / {\n            try_files $uri $uri/ /index.html;\n        }\n    }\n}\n</code></pre></li> <li>Create <code>Dockerfile</code> (see solution for content)</li> </ul>"},{"location":"Tasks/DockerFile/#15-dockerfile-security-best-practices","title":"15. Dockerfile Security Best Practices","text":"<ul> <li> <p>Implement security best practices in a Dockerfile including non-root users, minimal attack surface, and proper secret handling.</p> </li> </ul> <p>Hint: Use non-root users, minimal base images, update packages, avoid secrets in images, and implement proper file permissions</p> Solution <p>Solution:</p> <p>Dockerfile <pre><code># syntax=docker/dockerfile:1\n\n# Use specific, minimal base image\nFROM python:3.11-slim\n\n# Update packages and install security updates\nRUN apt-get update &amp;&amp; apt-get upgrade -y &amp;&amp; \\\n    apt-get install -y --no-install-recommends \\\n        curl \\\n        &amp;&amp; rm -rf /var/lib/apt/lists/* \\\n        &amp;&amp; apt-get clean\n\n# Create non-root user early\nRUN groupadd -r appuser &amp;&amp; useradd -r -g appuser appuser\n\n# Set working directory with proper permissions\nWORKDIR /app\nRUN chown appuser:appuser /app\n\n# Copy only necessary files\nCOPY --chown=appuser:appuser requirements.txt .\n\n# Install dependencies as root, then switch user\nRUN pip install --no-cache-dir --upgrade pip &amp;&amp; \\\n    pip install --no-cache-dir -r requirements.txt\n\n# Copy application code with correct ownership\nCOPY --chown=appuser:appuser app.py .\n\n# Switch to non-root user\nUSER appuser\n\n# Don't run as root\n# Don't expose sensitive ports unnecessarily\nEXPOSE 8000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n    CMD curl -f http://localhost:8000/health || exit 1\n\n# Use exec form of CMD\nCMD [\"python\", \"app.py\"]\n</code></pre></p> <p>app.py <pre><code>from flask import Flask\nimport os\n\napp = Flask(__name__)\n\n@app.route('/')\ndef hello():\n    return f'Running as user: {os.getuid()}\\n'\n\n@app.route('/health')\ndef health():\n    return 'OK'\n\nif __name__ == '__main__':\n    port = int(os.getenv('PORT', 8000))\n    app.run(host='0.0.0.0', port=port)\n</code></pre></p> <p>requirements.txt <pre><code>Flask==3.0.0\n</code></pre></p> <p>Security scanning: <pre><code># Build the secure image\ndocker build -t secure-app .\n\n# Scan for vulnerabilities (requires security scanner)\n# docker scan secure-app\n\n# Run security checks\ndocker run --rm secure-app whoami\ndocker run --rm secure-app id\n</code></pre></p> <p>Explanation:</p> <ul> <li>Non-root user: Application runs with limited privileges</li> <li>Minimal base images: Smaller attack surface</li> <li>Package updates: Install security patches</li> <li>File permissions: Proper ownership and access controls</li> <li>No secrets in image: Sensitive data not embedded in layers</li> <li>Health checks: Monitor container health and security</li> <li>Exec form CMD: Proper signal handling</li> </ul>"},{"location":"Tasks/DockerFile/#scenario_14","title":"Scenario:","text":"<ul> <li>Your organization requires container images to meet strict security standards for production deployment. </li> <li>Implementing security best practices ensures that your containers minimize vulnerabilities, follow the principle of least privilege, and protect sensitive information throughout the build and runtime lifecycle.</li> </ul>"},{"location":"Tasks/DockerFile/#resources_12","title":"Resources:","text":"<ul> <li>Create <code>app.py</code>:   <pre><code>from flask import Flask\nimport os\n\napp = Flask(__name__)\n\n@app.route('/')\ndef hello():\n    return f'Running as user: {os.getuid()}\\n'\n\n@app.route('/health')\ndef health():\n    return 'OK'\n\nif __name__ == '__main__':\n    port = int(os.getenv('PORT', 8000))\n    app.run(host='0.0.0.0', port=port)\n</code></pre></li> <li>Create <code>requirements.txt</code>:   <pre><code>Flask==3.0.0\n</code></pre></li> <li>Create <code>Dockerfile</code> (see solution for content)</li> </ul>"},{"location":"Tasks/DockerFile/#16-buildkit-advanced-features","title":"16. BuildKit Advanced Features","text":"<ul> <li> <p>Utilize advanced BuildKit features including mounts, cache mounts, and SSH forwarding for improved build performance and capabilities.</p> </li> </ul> <p>Hint: Use <code>--mount=type=cache</code>, <code>--mount=type=ssh</code>, and other BuildKit mount types</p> Solution <p>Solution:</p> <p>Dockerfile <pre><code># syntax=docker/dockerfile:1\n\nFROM golang:1.21-alpine AS builder\n\n# Enable BuildKit\nENV DOCKER_BUILDKIT=1\n\nWORKDIR /app\n\n# Copy go mod files\nCOPY go.mod go.sum ./\n\n# Cache mount for Go modules\nRUN --mount=type=cache,target=/go/pkg/mod \\\n    go mod download\n\n# Copy source\nCOPY . .\n\n# Cache mount for Go build cache\nRUN --mount=type=cache,target=/root/.cache/go-build \\\n    CGO_ENABLED=0 GOOS=linux go build -o main .\n\nFROM alpine:latest\n\n# Install ca-certificates for HTTPS\nRUN apk --no-cache add ca-certificates\n\nWORKDIR /root/\n\n# Copy binary\nCOPY --from=builder /app/main .\n\nEXPOSE 8080\n\nCMD [\"./main\"]\n</code></pre></p> <p>Advanced Dockerfile with SSH: <pre><code>FROM node:18-alpine\n\nRUN apk add --no-cache openssh-client git\n\nWORKDIR /app\n\n# SSH mount for private repositories\nRUN --mount=type=ssh \\\n    git clone git@github.com:private/repo.git .\n\n# Cache mount for npm\nRUN --mount=type=cache,target=/root/.npm \\\n    npm install\n\nCOPY . .\n\nCMD [\"npm\", \"start\"]\n</code></pre></p> <p>Build with BuildKit: <pre><code># Enable BuildKit\nexport DOCKER_BUILDKIT=1\n\n# Build with cache mounts\ndocker build -t buildkit-demo .\n\n# Build with SSH access\ndocker build \\\n  --ssh default \\\n  -t ssh-build .\n\n# Use build secrets\necho \"secret-token\" | docker build \\\n  --secret id=mysecret \\\n  -t secret-build .\n</code></pre></p> <p>Explanation:</p> <ul> <li>Cache mounts: Persistent cache between builds for faster subsequent builds</li> <li>SSH mounts: Access to private repositories during build</li> <li>Secret mounts: Secure handling of sensitive build-time data</li> <li>BuildKit: Modern build system with advanced features</li> <li>Performance: Faster builds through intelligent caching</li> </ul>"},{"location":"Tasks/DockerFile/#scenario_15","title":"Scenario:","text":"<ul> <li>Your build process involves downloading large dependencies, accessing private repositories, and requires optimal caching for faster CI/CD pipelines. </li> <li>BuildKit advanced features provide sophisticated caching mechanisms and secure access methods that significantly improve build performance and reliability.</li> </ul>"},{"location":"Tasks/DockerFile/#17-image-size-optimization","title":"17. Image Size Optimization","text":"<ul> <li> <p>Optimize image size through various techniques including multi-stage builds, package cleanup, and efficient layer management.</p> </li> </ul> <p>Hint: Use multi-stage builds, remove unnecessary packages, combine RUN commands, and use smaller base images</p> Solution <p>Solution:</p> <p>Dockerfile (Optimized) <pre><code># Build stage\nFROM golang:1.21-alpine AS builder\n\nWORKDIR /app\n\n# Copy go mod\nCOPY go.mod go.sum ./\nRUN go mod download\n\n# Copy source\nCOPY . .\n\n# Build static binary\nRUN CGO_ENABLED=0 GOOS=linux go build \\\n    -a -installsuffix cgo \\\n    -ldflags '-w -s' \\\n    -o main .\n\n# Strip binary\nRUN strip main\n\n# Runtime stage - use scratch for minimal size\nFROM scratch\n\n# Copy CA certificates for HTTPS\nCOPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/\n\n# Copy binary\nCOPY --from=builder /app/main /main\n\nEXPOSE 8080\n\nCMD [\"/main\"]\n</code></pre></p> <p>Compare with unoptimized: <pre><code>FROM golang:1.21-alpine\n\nWORKDIR /app\n\nCOPY . .\n\nRUN go build -o main .\n\nCMD [\"./main\"]\n</code></pre></p> <p>Size comparison: <pre><code># Build optimized version\ndocker build -f Dockerfile.optimized -t optimized-app .\n\n# Build unoptimized version\ndocker build -f Dockerfile.unoptimized -t unoptimized-app .\n\n# Compare sizes\ndocker images | grep -E \"(optimized|unoptimized)\"\n\n# Expected result: optimized image much smaller\n</code></pre></p> <p>Additional optimization techniques: <pre><code># Use .dockerignore\n# Combine RUN commands\n# Remove package manager cache\n# Use smaller base images\n# Strip binaries\n# Use scratch base for static binaries\n</code></pre></p> <p>Explanation:</p> <ul> <li>Multi-stage builds: Separate build and runtime environments</li> <li>Scratch base: Minimal possible image size for static binaries</li> <li>Binary stripping: Remove debug symbols to reduce size</li> <li>Package cleanup: Remove build dependencies from final image</li> <li>Layer optimization: Combine commands to reduce layer count</li> </ul>"},{"location":"Tasks/DockerFile/#scenario_16","title":"Scenario:","text":"<ul> <li>Your production environment has limited storage and network bandwidth, and you need to minimize deployment times and storage costs. </li> <li>Image size optimization techniques reduce the attack surface, improve deployment speed, and lower infrastructure costs while maintaining full application functionality.</li> </ul>"},{"location":"Tasks/DockerFile/#18-complex-application-stack","title":"18. Complex Application Stack","text":"<ul> <li> <p>Create a multi-service application stack with a web frontend, API backend, and database using Docker Compose and optimized Dockerfiles.</p> </li> </ul> <p>Hint: Create separate Dockerfiles for each service, use multi-stage builds, and coordinate with docker-compose.yml</p> Solution <p>Solution:</p> <p>Project structure: <pre><code>complex-app/\n\u251c\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 frontend/\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 package.json\n\u2502   \u2514\u2500\u2500 src/\n\u251c\u2500\u2500 backend/\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2514\u2500\u2500 app.py\n\u2514\u2500\u2500 database/\n    \u2514\u2500\u2500 init.sql\n</code></pre></p> <p>frontend/Dockerfile <pre><code>FROM node:18-alpine AS builder\n\nWORKDIR /app\n\nCOPY package*.json ./\nRUN npm ci\n\nCOPY . .\nRUN npm run build\n\nFROM nginx:alpine\n\nCOPY --from=builder /app/dist /usr/share/nginx/html\n\nEXPOSE 80\n\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n</code></pre></p> <p>backend/Dockerfile <pre><code>FROM python:3.11-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nEXPOSE 8000\n\nCMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre></p> <p>docker-compose.yml <pre><code>version: '3.8'\n\nservices:\n  frontend:\n    build: ./frontend\n    ports:\n      - \"3000:80\"\n    depends_on:\n      - backend\n\n  backend:\n    build: ./backend\n    ports:\n      - \"8000:8000\"\n    environment:\n      - DATABASE_URL=postgresql://user:pass@db:5432/app\n    depends_on:\n      - db\n\n  db:\n    image: postgres:13\n    environment:\n      POSTGRES_DB: app\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: pass\n    volumes:\n      - db_data:/var/lib/postgresql/data\n      - ./database/init.sql:/docker-entrypoint-initdb.d/init.sql\n\nvolumes:\n  db_data:\n</code></pre></p> <p>Build and run: <pre><code># Build all services\ndocker-compose build\n\n# Start the stack\ndocker-compose up -d\n\n# Check services\ndocker-compose ps\n\n# Test the application\ncurl http://localhost:3000\ncurl http://localhost:8000\n</code></pre></p> <p>Explanation:</p> <ul> <li>Multi-service architecture: Separate concerns with microservices</li> <li>Service dependencies: Proper startup ordering with depends_on</li> <li>Optimized builds: Multi-stage for frontend, minimal for backend</li> <li>Environment configuration: Runtime configuration through environment variables</li> <li>Volume management: Persistent database storage</li> <li>Port mapping: Proper service exposure and communication</li> </ul>"},{"location":"Tasks/DockerFile/#scenario_17","title":"Scenario:","text":"<ul> <li>You\u2019re developing a full-stack web application with multiple interconnected services that need to work together seamlessly. </li> <li>Containerizing the entire stack ensures consistent deployment across development, testing, and production environments while maintaining service dependencies and network isolation.</li> </ul>"},{"location":"Tasks/DockerFile/#resources_13","title":"Resources:","text":"<ul> <li>Create project structure:   <pre><code>complex-app/\n\u251c\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 frontend/\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 package.json\n\u2502   \u2514\u2500\u2500 src/\n\u251c\u2500\u2500 backend/\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u2514\u2500\u2500 app.py\n\u2514\u2500\u2500 database/\n    \u2514\u2500\u2500 init.sql\n</code></pre></li> <li>Create <code>docker-compose.yml</code> (see solution for content)</li> <li>Create <code>frontend/Dockerfile</code> (see solution for content)</li> <li>Create <code>backend/Dockerfile</code> (see solution for content)</li> </ul>"},{"location":"Tasks/DockerNetwork/","title":"Task: Creating and Testing Docker Networks","text":"<p>This task provides a detailed guide to creating and testing Docker networks. Docker networks enable containers to communicate with each other securely and efficiently.</p>"},{"location":"Tasks/DockerNetwork/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed and running</li> <li>Basic understanding of Docker containers</li> <li>Terminal access</li> </ul>"},{"location":"Tasks/DockerNetwork/#task-overview","title":"Task Overview","text":"<p>In this task, you will:</p> <ul> <li>List existing Docker networks</li> <li>Create custom networks with different configurations</li> <li>Run containers connected to networks</li> <li>Test connectivity between containers</li> <li>Manage network connections</li> <li>Clean up resources</li> </ul> Solution"},{"location":"Tasks/DockerNetwork/#step-by-step-solution","title":"Step-by-Step Solution","text":""},{"location":"Tasks/DockerNetwork/#01-list-existing-networks","title":"01: List Existing Networks","text":"<p>First, let\u2019s see what networks are already available on your Docker host.</p> <pre><code>docker network ls\n</code></pre> <p>This command will show the default networks:</p> <ul> <li><code>bridge</code>: Default network for containers</li> <li><code>host</code>: Uses the host\u2019s network stack</li> <li><code>none</code>: No networking</li> </ul>"},{"location":"Tasks/DockerNetwork/#02-create-a-custom-network","title":"02: Create a Custom Network","text":"<p>Create a custom bridge network for better isolation and control.</p> <pre><code>docker network create --driver bridge my-custom-network\n</code></pre> <ul> <li><code>--driver bridge</code>: Specifies the bridge driver (default)</li> <li><code>my-custom-network</code>: Name of the network</li> </ul> <p>Verify the network was created:</p> <pre><code>docker network ls\n</code></pre> <p>You should see <code>my-custom-network</code> in the list.</p>"},{"location":"Tasks/DockerNetwork/#03-inspect-the-network","title":"03: Inspect the Network","text":"<p>Get detailed information about the network.</p> <pre><code>docker network inspect my-custom-network\n</code></pre> <p>This will show:</p> <ul> <li>Network ID</li> <li>Driver</li> <li>Subnet and gateway</li> <li>Connected containers (initially empty)</li> </ul>"},{"location":"Tasks/DockerNetwork/#04-run-containers-on-the-network","title":"04: Run Containers on the Network","text":"<p>Launch two containers connected to the custom network.</p> <pre><code># Run a web server container\ndocker run -d --name web-server --network my-custom-network -p 8080:80 nginx\n\n# Run a client container for testing\ndocker run -d --name test-client --network my-custom-network alpine sleep 3600\n</code></pre> <ul> <li><code>-d</code>: Run in detached mode</li> <li><code>--name</code>: Assign container names</li> <li><code>--network</code>: Connect to the custom network</li> <li><code>-p 8080:80</code>: Port mapping for the web server</li> </ul>"},{"location":"Tasks/DockerNetwork/#05-verify-container-connectivity","title":"05: Verify Container Connectivity","text":"<p>Check that containers are connected to the network.</p> <pre><code>docker network inspect my-custom-network\n</code></pre> <p>You should now see the two containers listed under \u201cContainers\u201d.</p>"},{"location":"Tasks/DockerNetwork/#06-test-network-communication","title":"06: Test Network Communication","text":"<p>Test communication between containers on the same network.</p> <pre><code># Get the IP address of the web server\ndocker inspect web-server | grep -A 10 \"Networks\"\n\n# Or use container names for DNS resolution\ndocker exec test-client ping -c 4 web-server\n</code></pre> <p>Since containers on the same network can resolve each other by name, you can ping using the container name.</p>"},{"location":"Tasks/DockerNetwork/#07-test-external-access","title":"07: Test External Access","text":"<p>Test access to the web server from the host.</p> <pre><code>curl http://localhost:8080\n</code></pre> <p>You should see the default Nginx welcome page.</p>"},{"location":"Tasks/DockerNetwork/#08-connect-existing-container-to-network","title":"08: Connect Existing Container to Network","text":"<p>Demonstrate connecting a running container to the network.</p> <pre><code># Run another container without specifying network\ndocker run -d --name standalone-container alpine sleep 3600\n\n# Connect it to the custom network\ndocker network connect my-custom-network standalone-container\n\n# Verify connection\ndocker network inspect my-custom-network\n</code></pre>"},{"location":"Tasks/DockerNetwork/#09-disconnect-container-from-network","title":"09: Disconnect Container from Network","text":"<p>Remove a container from the network.</p> <pre><code>docker network disconnect my-custom-network standalone-container\n</code></pre> <p>Verify the container is no longer connected.</p>"},{"location":"Tasks/DockerNetwork/#10-create-network-with-custom-subnet","title":"10: Create Network with Custom Subnet","text":"<p>Create a network with a specific subnet.</p> <pre><code>docker network create --driver bridge --subnet 192.168.10.0/24 --gateway 192.168.10.1 custom-subnet-network\n</code></pre> <p>Inspect to verify the custom configuration.</p>"},{"location":"Tasks/DockerNetwork/#11-clean-up","title":"11: Clean Up","text":"<p>Remove containers and networks.</p> <pre><code># Stop and remove containers\ndocker stop web-server test-client standalone-container\ndocker rm web-server test-client standalone-container\n\n# Remove networks\ndocker network rm my-custom-network custom-subnet-network\n</code></pre>"},{"location":"Tasks/DockerNetwork/#advanced-testing","title":"Advanced Testing","text":""},{"location":"Tasks/DockerNetwork/#test-with-docker-compose","title":"Test with Docker Compose","text":"<p>Create a <code>docker-compose.yml</code> file:</p> <pre><code>version: '3.8'\nservices:\n  web:\n    image: nginx\n    networks:\n      - my-network\n  client:\n    image: alpine\n    command: sleep 3600\n    networks:\n      - my-network\nnetworks:\n  my-network:\n    driver: bridge\n</code></pre> <p>Run with:</p> <pre><code>docker-compose up -d\n</code></pre>"},{"location":"Tasks/DockerNetwork/#test-network-isolation","title":"Test Network Isolation","text":"<p>Create two separate networks and verify containers can\u2019t communicate across them.</p> <pre><code># Create two networks\ndocker network create network-a\ndocker network create network-b\n\n# Run containers on each\ndocker run -d --name container-a --network network-a alpine sleep 3600\ndocker run -d --name container-b --network network-b alpine sleep 3600\n\n# Try to ping across networks (should fail)\ndocker exec container-a ping -c 4 container-b  # This will fail\n</code></pre>"},{"location":"Tasks/DockerNetwork/#troubleshooting","title":"Troubleshooting","text":""},{"location":"Tasks/DockerNetwork/#common-issues","title":"Common Issues","text":"<ol> <li>Port already in use: Change the port mapping</li> <li>Network not found: Ensure the network name is correct</li> <li>Container can\u2019t resolve names: Check if both containers are on the same network</li> </ol>"},{"location":"Tasks/DockerNetwork/#useful-commands","title":"Useful Commands","text":"<ul> <li><code>docker network prune</code>: Remove unused networks</li> <li><code>docker network connect/disconnect</code>: Manage container network connections</li> <li><code>docker inspect &lt;container&gt;</code>: Get detailed container information including networks</li> </ul>"},{"location":"Tasks/DockerNetwork/#explanation","title":"Explanation","text":"<ul> <li>docker network ls: Lists all networks on the Docker host</li> <li>docker network create: Creates a new network with specified driver and options</li> <li>docker network inspect: Shows detailed network configuration and connected containers</li> <li>docker run \u2013network: Connects a container to a specific network at startup</li> <li>docker network connect/disconnect: Dynamically connects or disconnects running containers from networks</li> <li>Container name resolution: Docker provides built-in DNS for containers on the same network</li> <li>Network isolation: Containers on different networks cannot communicate unless explicitly connected</li> </ul>"}]}